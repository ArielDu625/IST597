{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z64sgoMYCbdA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid, KFold\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "UhakxzoHC4zQ",
        "outputId": "dc364b73-4420-4863-8d75-5068ecf1073e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPB4GFSyGDBO"
      },
      "source": [
        "### MLP module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rTDRaYiZDN7t"
      },
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "    def __init__(self, input_size, output_size, hidden_size=256, device=None, optimizer='SGD'):\n",
        "        \"\"\"\n",
        "        input_size: int, size of input layer\n",
        "        output_size: int, size of output layer\n",
        "        hidden_size: int, size of hidden layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Execution\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.optimizer = optimizer\n",
        " \n",
        "        # Initialize weights for input layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.input_size, self.hidden_size], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.random.normal([1, self.hidden_size]))\n",
        "\n",
        "        # Initialize weights for the first hidden layer\n",
        "        self.W12 = tf.Variable(tf.random.normal([self.hidden_size, self.hidden_size], stddev=0.1))\n",
        "        self.b12 = tf.Variable(tf.random.normal([1, self.hidden_size]))\n",
        "\n",
        "        # Initialize weights between the first hidden layer and the second hidden layer\n",
        "        self.W22 = tf.Variable(tf.random.normal([self.hidden_size, self.hidden_size], stddev=0.1))\n",
        "        self.b22 = tf.Variable(tf.random.normal([1, self.hidden_size]))\n",
        "\n",
        "        # Initialize weights between the second hidden layer and the output layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.hidden_size, self.output_size], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.random.normal([1, self.output_size]))\n",
        "\n",
        "        # define variables to be updated during backpropagation\n",
        "        self.variables = [self.W1, self.W12, self.W22, self.W2, self.b1, self.b12, self.b22, self.b2]\n",
        "\n",
        "        ### define extra variables for optimizer\n",
        "        self.m = []\n",
        "        self.v = []\n",
        "        self.u = []\n",
        "        for var in self.variables:\n",
        "            self.m.append(tf.Variable(tf.zeros_like(var), trainable=False))\n",
        "            self.v.append(tf.Variable(tf.zeros_like(var), trainable=False))\n",
        "            self.u.append(tf.Variable(tf.zeros_like(var), trainable=False))\n",
        "        self.t = 0\n",
        "        self.alpha = 10e-3\n",
        "        self.beta_1 = 0.9\n",
        "        self.beta_2 = 0.999\n",
        "        self.beta_3 = 0.999987\n",
        "        self.epsilon = 10e-8\n",
        "\n",
        "    \n",
        "    def forward(self, X, dropout=0.0):\n",
        "        \"\"\" Forward pass\n",
        "        X: tensor, inputs\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X, dropout)\n",
        "        else:\n",
        "            self.y = self.compute_output(X, dropout)\n",
        "        \n",
        "        return self.y\n",
        "    \n",
        "    def loss(self, y_pred, y_true, beta=0.0):\n",
        "        \"\"\"\n",
        "        y_pred: tensor of shape (batch_size, output_size),\n",
        "        y_true: tensor of shape (batch_size), true labels are provided as integers\n",
        "        beta: weight of penalty term\n",
        "        \"\"\"\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.int32)\n",
        "        cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true_tf, logits=y_pred_tf))\n",
        "        l2_penalty = tf.nn.l2_loss(self.W1) + tf.nn.l2_loss(self.W12) + tf.nn.l2_loss(self.W22) +  tf.nn.l2_loss(self.W2)\n",
        "        total_cost = cost + beta * l2_penalty\n",
        "\n",
        "        return total_cost\n",
        "    \n",
        "    def backward(self, X_train, y_train, dropout=0.0, beta=0.0, lr=1e-6):\n",
        "        \"\"\"\n",
        "        backward pass\n",
        "        X_train: tensor of shape (batch_size, input_size)\n",
        "        y_train: tensor of shape (batch_size)\n",
        "        dropout:\n",
        "        beta:\n",
        "        lr:\n",
        "        \"\"\"\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train, dropout=dropout)\n",
        "            current_loss = self.loss(predicted, y_train, beta=beta)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "\n",
        "        if self.optimizer == 'SGD':\n",
        "            optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "            optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        elif self.optimizer == 'Adam':\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "            optimizer.apply_gradients(zip(grads, self.variables))\n",
        "        else:\n",
        "            self.t += 1\n",
        "            self.alpha = lr\n",
        "\n",
        "            for i in range(len(self.variables)):\n",
        "                mt = tf.math.add(tf.math.scalar_mul(self.beta_1, self.m[i]), tf.math.scalar_mul(1 - self.beta_1, grads[i]))\n",
        "                self.m[i].assign(mt)\n",
        "\n",
        "                vt = tf.math.add(tf.math.scalar_mul(self.beta_2, self.v[i]), tf.math.scalar_mul(1 - self.beta_2, tf.pow(grads[i], 2.0)))\n",
        "                self.v[i].assign(vt)\n",
        "\n",
        "                ut = tf.math.add(tf.math.scalar_mul(self.beta_3, self.u[i]), tf.math.scalar_mul(1 - self.beta_3, tf.pow(grads[i], 3.0)))\n",
        "                self.u[i].assign(ut)\n",
        "\n",
        "                beta_1_power = tf.pow(self.beta_1, self.t)\n",
        "                beta_2_power = tf.pow(self.beta_2, self.t)\n",
        "                beta_3_power = tf.pow(self.beta_3, self.t)\n",
        "\n",
        "                mt_hat = tf.math.divide(mt, 1 - beta_1_power)\n",
        "                vt_hat = tf.math.divide(vt, 1 - beta_2_power)\n",
        "                ut_hat = tf.math.divide(ut, 1 - beta_3_power)\n",
        "                \n",
        "                vt_power = tf.pow(vt_hat, 1.0/2.0)\n",
        "                ut_power = tf.math.multiply(tf.math.sign(ut_hat), tf.math.pow(tf.abs(ut_hat), 1.0/3.0))\n",
        "                _denominator = tf.math.add(vt_power, tf.math.scalar_mul(self.epsilon, ut_power))\n",
        "                # to avoid divide by zero\n",
        "                delta = tf.math.divide(mt_hat, tf.math.add(_denominator, 10e-8))\n",
        "                new_var = self.variables[i] - tf.math.scalar_mul(self.alpha, delta)\n",
        "                self.variables[i].assign(new_var)\n",
        "\n",
        "        return predicted, current_loss, grads\n",
        "    \n",
        "    def compute_output(self, X, dropout):\n",
        "        \"\"\"Custom method to obtain output tensor during forward pass\n",
        "        X: tensor, input\n",
        "        dropout: dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "        # compute values in the input layer\n",
        "        logits1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        input_layer = tf.nn.relu(logits1)\n",
        "\n",
        "        # compute values in the first hidden layer\n",
        "        logits12 = tf.matmul(input_layer, self.W12) + self.b12\n",
        "        hidden_layer1 = tf.nn.relu(logits12)\n",
        "        # hidden_layer1_dropout = tf.nn.dropout(hidden_layer1, rate=dropout_rate, seed=1)\n",
        "        hidden_layer1_dropout = tf.nn.experimental.stateless_dropout(hidden_layer1, rate=dropout, seed=[0,1])\n",
        "\n",
        "        # compute values in the second hidden layer\n",
        "        logits22 = tf.matmul(hidden_layer1_dropout, self.W22) + self.b22\n",
        "        hidden_layer2 = tf.nn.relu(logits22)\n",
        "        # hidden_layer2_dropout = tf.nn.dropout(hidden_layer2, rate = dropout_rate, seed = 2)\n",
        "        hidden_layer2_dropout = tf.nn.experimental.stateless_dropout(hidden_layer2, rate=dropout, seed=[1,2])\n",
        "\n",
        "        # compute values in the output layer\n",
        "        output = tf.matmul(hidden_layer2_dropout, self.W2) + self.b2\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZCBqJrotZLyZ"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset='fashion_mnist', valid_size=0.2):\n",
        "    if dataset == 'fashion_mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    elif dataset == 'mnist':\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    else:\n",
        "        print(\"invalid dataset\")\n",
        "        return\n",
        "   \n",
        "    # scale data to the range of [0,1]\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    x_test = x_test.astype('float32') / 255\n",
        "\n",
        "    x_train_tf = tf.reshape(x_train, (x_train.shape[0], -1))\n",
        "    x_test_tf = tf.reshape(x_test, (x_test.shape[0], -1))\n",
        "    \n",
        "    return x_train_tf, y_train, x_test_tf, y_test\n",
        "\n",
        "\n",
        "def accuracy(predictions, labels):\n",
        "    preds = tf.argmax(predictions, axis=1).numpy()\n",
        "    acc = np.sum(preds == labels) / predictions.shape[0]\n",
        "    return 100.0* acc\n",
        "\n",
        "\n",
        "def train(model, x_train_tf, y_train, x_validation_tf, y_validation, \n",
        "          batch_size=32, \n",
        "          dropout_rate=0.0, \n",
        "          beta=0.0, \n",
        "          learning_rate=1e-5, \n",
        "          epochs=10):\n",
        "    \n",
        "    epoch = 0\n",
        "    global_step = 0\n",
        "    eval_steps = 100\n",
        "    train_data_size, validation_data_size = x_train_tf.shape[0], x_validation_tf.shape[0]\n",
        "\n",
        "    train_losses = []\n",
        "    eval_losses = []\n",
        "    train_accuracy = []\n",
        "    eval_accuracy = []\n",
        "    epoch_times = []\n",
        "\n",
        "    while epoch < epochs:\n",
        "        time_start = time.time()\n",
        "\n",
        "        lt = 0\n",
        "        # Split dataset into batches\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((x_train_tf, y_train)).shuffle(train_data_size, \n",
        "                                                                                 seed=epoch*(1234)).batch(batch_size)\n",
        "        \n",
        "        for inputs, outputs in train_ds:\n",
        "            train_preds, train_loss, _ = model.backward(inputs, outputs, dropout=dropout_rate, lr=learning_rate, beta=beta)\n",
        "            lt = lt + train_loss\n",
        "\n",
        "        _train_preds = model.forward(x_train_tf, dropout=0.0)\n",
        "        _train_acc = accuracy(tf.nn.softmax(_train_preds), y_train)\n",
        "        train_losses.append(lt)\n",
        "        train_accuracy.append(_train_acc)\n",
        "\n",
        "        _eval_preds = model.forward(x_validation_tf, dropout=0.0)\n",
        "        _eval_loss = model.loss(_eval_preds, y_validation, beta=0.0)\n",
        "        _eval_acc = accuracy(tf.nn.softmax(_eval_preds), y_validation)\n",
        "        eval_losses.append(_eval_loss)\n",
        "        eval_accuracy.append(_eval_acc)\n",
        "\n",
        "        print('Epoch={} - Train Loss:={}, Evaluation Loss:={}, Train Acc:={}, Evaluation Acc:={}'.format(epoch, lt, _eval_loss, _train_acc, _eval_acc))\n",
        "        epoch += 1\n",
        "        time_end = time.time()\n",
        "        epoch_times.append(time_end - time_start)\n",
        "\n",
        "    return train_losses, eval_losses, train_accuracy, eval_accuracy, epoch_times\n",
        "\n",
        "def test(model, x_test_tf, y_test):\n",
        "    test_preds = model.forward(x_test_tf, dropout=0.0)\n",
        "    test_acc = accuracy(tf.nn.softmax(test_preds), y_test)\n",
        "    return test_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(train_losses, test_losses, train_acc, test_acc):\n",
        "    fig, axs = plt.subplots(1,2, figsize=(20, 10))\n",
        "    axs[0].plot(train_losses, label=\"train error\", color='blue')\n",
        "    axs[0].plot(test_losses, label=\"test error\", color='orange')\n",
        "    axs[0].set_xlabel(\"steps\")\n",
        "    axs[0].set_ylabel(\"loss\")\n",
        "    axs[0].legend(loc=\"upper right\")\n",
        "\n",
        "    axs[1].plot(train_acc, label=\"train acc\", color=\"blue\")\n",
        "    axs[1].plot(test_acc, label=\"test acc\", color='orange')\n",
        "    axs[1].set_xlabel(\"steps\")\n",
        "    axs[1].set_ylabel(\"accuracy\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qEfIEj5Uf4FI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YqNK7EwSC2x"
      },
      "source": [
        "## MINST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualize a selected sample.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y5OoSOkQwpi8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "EsvlLLNJSMPb",
        "outputId": "df603ff9-2305-4abb-ab17-c49f28ecb1f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "sample image shape: (28, 28)\n",
            "sample image label: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fecc05c2d50>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train_x, train_y, test_x, test_y = load_data(dataset=\"mnist\")\n",
        "sample = tf.reshape(train_x[0], (28,28))\n",
        "print(f\"sample image shape: {sample.shape}\")\n",
        "print(f\"sample image label: {train_y[0]}\")\n",
        "plt.imshow(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjK-BxnDRVOp"
      },
      "source": [
        "### Model1: without any regularization + SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv6Om3-HRcGp",
        "outputId": "260d87ec-0a49-4021-c16d-0394150c9e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=3842.14013671875, Evaluation Loss:=0.9717162251472473, Train Acc:=67.79333333333334, Evaluation Acc:=68.87\n",
            "Epoch=1 - Train Loss:=1566.403076171875, Evaluation Loss:=0.6843574047088623, Train Acc:=77.02166666666666, Evaluation Acc:=78.14999999999999\n",
            "Epoch=2 - Train Loss:=1230.8466796875, Evaluation Loss:=0.5752062797546387, Train Acc:=80.69666666666667, Evaluation Acc:=82.04\n",
            "Epoch=3 - Train Loss:=1069.9345703125, Evaluation Loss:=0.5124307870864868, Train Acc:=82.92, Evaluation Acc:=83.98\n",
            "Epoch=4 - Train Loss:=969.90869140625, Evaluation Loss:=0.47186920046806335, Train Acc:=84.50333333333333, Evaluation Acc:=85.69\n",
            "Epoch=5 - Train Loss:=899.9153442382812, Evaluation Loss:=0.44089004397392273, Train Acc:=85.54666666666667, Evaluation Acc:=86.66\n",
            "Epoch=6 - Train Loss:=846.3643188476562, Evaluation Loss:=0.4206176698207855, Train Acc:=86.26666666666667, Evaluation Acc:=86.92\n",
            "Epoch=7 - Train Loss:=803.5297241210938, Evaluation Loss:=0.3992197513580322, Train Acc:=87.095, Evaluation Acc:=87.74\n",
            "Epoch=8 - Train Loss:=768.0016479492188, Evaluation Loss:=0.38293755054473877, Train Acc:=87.67, Evaluation Acc:=88.14999999999999\n",
            "Epoch=9 - Train Loss:=738.4732055664062, Evaluation Loss:=0.37072911858558655, Train Acc:=88.09333333333333, Evaluation Acc:=88.64\n",
            "Epoch=0 - Train Loss:=3569.74609375, Evaluation Loss:=1.0077711343765259, Train Acc:=66.89833333333334, Evaluation Acc:=68.21000000000001\n",
            "Epoch=1 - Train Loss:=1609.3060302734375, Evaluation Loss:=0.722914457321167, Train Acc:=76.83333333333333, Evaluation Acc:=77.68\n",
            "Epoch=2 - Train Loss:=1263.0904541015625, Evaluation Loss:=0.6119109988212585, Train Acc:=80.66166666666666, Evaluation Acc:=81.28999999999999\n",
            "Epoch=3 - Train Loss:=1097.91650390625, Evaluation Loss:=0.5491429567337036, Train Acc:=82.88666666666667, Evaluation Acc:=83.09\n",
            "Epoch=4 - Train Loss:=995.047607421875, Evaluation Loss:=0.5067126750946045, Train Acc:=84.39833333333333, Evaluation Acc:=84.45\n",
            "Epoch=5 - Train Loss:=922.6098022460938, Evaluation Loss:=0.4743180572986603, Train Acc:=85.45666666666666, Evaluation Acc:=85.34\n",
            "Epoch=6 - Train Loss:=867.7799072265625, Evaluation Loss:=0.4501221776008606, Train Acc:=86.335, Evaluation Acc:=86.25\n",
            "Epoch=7 - Train Loss:=824.16015625, Evaluation Loss:=0.4304395616054535, Train Acc:=86.98833333333333, Evaluation Acc:=86.77\n",
            "Epoch=8 - Train Loss:=788.1141357421875, Evaluation Loss:=0.4142448306083679, Train Acc:=87.49666666666667, Evaluation Acc:=87.26\n",
            "Epoch=9 - Train Loss:=757.2814331054688, Evaluation Loss:=0.3983626365661621, Train Acc:=88.03, Evaluation Acc:=87.67\n",
            "Epoch=0 - Train Loss:=3475.554443359375, Evaluation Loss:=0.9759689569473267, Train Acc:=68.475, Evaluation Acc:=69.32000000000001\n",
            "Epoch=1 - Train Loss:=1582.1072998046875, Evaluation Loss:=0.7063111066818237, Train Acc:=77.20833333333333, Evaluation Acc:=78.13\n",
            "Epoch=2 - Train Loss:=1252.433837890625, Evaluation Loss:=0.5939673781394958, Train Acc:=80.91000000000001, Evaluation Acc:=82.07\n",
            "Epoch=3 - Train Loss:=1088.602783203125, Evaluation Loss:=0.5314067602157593, Train Acc:=83.06333333333333, Evaluation Acc:=83.96000000000001\n",
            "Epoch=4 - Train Loss:=985.3123168945312, Evaluation Loss:=0.4894478917121887, Train Acc:=84.57333333333334, Evaluation Acc:=85.37\n",
            "Epoch=5 - Train Loss:=912.1371459960938, Evaluation Loss:=0.45826098322868347, Train Acc:=85.52499999999999, Evaluation Acc:=86.32\n",
            "Epoch=6 - Train Loss:=857.310791015625, Evaluation Loss:=0.4326361417770386, Train Acc:=86.40666666666667, Evaluation Acc:=87.18\n",
            "Epoch=7 - Train Loss:=813.1632080078125, Evaluation Loss:=0.4150705933570862, Train Acc:=87.055, Evaluation Acc:=87.7\n",
            "Epoch=8 - Train Loss:=776.7010498046875, Evaluation Loss:=0.398298978805542, Train Acc:=87.68, Evaluation Acc:=88.27000000000001\n",
            "Epoch=9 - Train Loss:=746.1809692382812, Evaluation Loss:=0.38580965995788574, Train Acc:=88.14666666666666, Evaluation Acc:=88.6\n",
            "Epoch=0 - Train Loss:=3534.9443359375, Evaluation Loss:=0.9249154329299927, Train Acc:=69.02333333333334, Evaluation Acc:=70.07\n",
            "Epoch=1 - Train Loss:=1529.6744384765625, Evaluation Loss:=0.6657221913337708, Train Acc:=77.46833333333333, Evaluation Acc:=78.85\n",
            "Epoch=2 - Train Loss:=1215.8450927734375, Evaluation Loss:=0.5611788034439087, Train Acc:=81.20666666666668, Evaluation Acc:=82.41000000000001\n",
            "Epoch=3 - Train Loss:=1060.7763671875, Evaluation Loss:=0.5035151839256287, Train Acc:=83.205, Evaluation Acc:=84.32\n",
            "Epoch=4 - Train Loss:=962.6641845703125, Evaluation Loss:=0.46270477771759033, Train Acc:=84.63000000000001, Evaluation Acc:=85.6\n",
            "Epoch=5 - Train Loss:=892.2498779296875, Evaluation Loss:=0.4326002299785614, Train Acc:=85.72833333333332, Evaluation Acc:=86.72999999999999\n",
            "Epoch=6 - Train Loss:=838.3307495117188, Evaluation Loss:=0.40956780314445496, Train Acc:=86.68666666666667, Evaluation Acc:=87.62\n",
            "Epoch=7 - Train Loss:=794.4472045898438, Evaluation Loss:=0.39168402552604675, Train Acc:=87.29333333333334, Evaluation Acc:=88.09\n",
            "Epoch=8 - Train Loss:=758.9257202148438, Evaluation Loss:=0.37520018219947815, Train Acc:=87.94, Evaluation Acc:=88.55\n",
            "Epoch=9 - Train Loss:=729.194091796875, Evaluation Loss:=0.3622971773147583, Train Acc:=88.41666666666667, Evaluation Acc:=89.16\n",
            "Epoch=0 - Train Loss:=3235.087646484375, Evaluation Loss:=0.9796927571296692, Train Acc:=67.94833333333334, Evaluation Acc:=68.42\n",
            "Epoch=1 - Train Loss:=1558.5484619140625, Evaluation Loss:=0.705442488193512, Train Acc:=77.17500000000001, Evaluation Acc:=77.53999999999999\n",
            "Epoch=2 - Train Loss:=1231.119873046875, Evaluation Loss:=0.5951728820800781, Train Acc:=81.13, Evaluation Acc:=81.31\n",
            "Epoch=3 - Train Loss:=1071.9473876953125, Evaluation Loss:=0.5287400484085083, Train Acc:=83.36833333333334, Evaluation Acc:=83.78999999999999\n",
            "Epoch=4 - Train Loss:=971.8311157226562, Evaluation Loss:=0.486406534910202, Train Acc:=84.79166666666667, Evaluation Acc:=85.11\n",
            "Epoch=5 - Train Loss:=902.7960205078125, Evaluation Loss:=0.4551888704299927, Train Acc:=85.95, Evaluation Acc:=86.09\n",
            "Epoch=6 - Train Loss:=849.9962158203125, Evaluation Loss:=0.43189874291419983, Train Acc:=86.69, Evaluation Acc:=86.86\n",
            "Epoch=7 - Train Loss:=808.75048828125, Evaluation Loss:=0.41114169359207153, Train Acc:=87.33999999999999, Evaluation Acc:=87.47\n",
            "Epoch=8 - Train Loss:=774.0903930664062, Evaluation Loss:=0.39605796337127686, Train Acc:=87.91, Evaluation Acc:=88.01\n",
            "Epoch=9 - Train Loss:=745.0464477539062, Evaluation Loss:=0.3822072148323059, Train Acc:=88.34, Evaluation Acc:=88.56\n",
            "Epoch=0 - Train Loss:=3534.418212890625, Evaluation Loss:=1.0252130031585693, Train Acc:=65.5, Evaluation Acc:=66.74\n",
            "Epoch=1 - Train Loss:=1640.3275146484375, Evaluation Loss:=0.7157161235809326, Train Acc:=75.995, Evaluation Acc:=77.19\n",
            "Epoch=2 - Train Loss:=1273.452880859375, Evaluation Loss:=0.5967832803726196, Train Acc:=80.23833333333333, Evaluation Acc:=81.15\n",
            "Epoch=3 - Train Loss:=1099.63818359375, Evaluation Loss:=0.5308436155319214, Train Acc:=82.71833333333333, Evaluation Acc:=83.55\n",
            "Epoch=4 - Train Loss:=993.312744140625, Evaluation Loss:=0.48815399408340454, Train Acc:=84.30499999999999, Evaluation Acc:=84.8\n",
            "Epoch=5 - Train Loss:=919.0411376953125, Evaluation Loss:=0.45739614963531494, Train Acc:=85.33666666666667, Evaluation Acc:=85.95\n",
            "Epoch=6 - Train Loss:=863.149658203125, Evaluation Loss:=0.432540625333786, Train Acc:=86.28833333333333, Evaluation Acc:=86.94\n",
            "Epoch=7 - Train Loss:=818.8599853515625, Evaluation Loss:=0.41347959637641907, Train Acc:=86.97500000000001, Evaluation Acc:=87.74\n",
            "Epoch=8 - Train Loss:=782.9476928710938, Evaluation Loss:=0.39728012681007385, Train Acc:=87.55, Evaluation Acc:=88.19\n",
            "Epoch=9 - Train Loss:=752.5145874023438, Evaluation Loss:=0.3841437101364136, Train Acc:=87.97333333333334, Evaluation Acc:=88.63\n",
            "Epoch=0 - Train Loss:=3808.670166015625, Evaluation Loss:=1.0045524835586548, Train Acc:=67.60166666666667, Evaluation Acc:=68.47\n",
            "Epoch=1 - Train Loss:=1600.1767578125, Evaluation Loss:=0.6971765756607056, Train Acc:=77.25333333333333, Evaluation Acc:=78.11\n",
            "Epoch=2 - Train Loss:=1235.043212890625, Evaluation Loss:=0.5822699666023254, Train Acc:=81.31666666666668, Evaluation Acc:=81.81\n",
            "Epoch=3 - Train Loss:=1065.2435302734375, Evaluation Loss:=0.5197585821151733, Train Acc:=83.64666666666668, Evaluation Acc:=83.84\n",
            "Epoch=4 - Train Loss:=962.172119140625, Evaluation Loss:=0.4772006869316101, Train Acc:=85.06, Evaluation Acc:=85.07000000000001\n",
            "Epoch=5 - Train Loss:=890.2359008789062, Evaluation Loss:=0.44773316383361816, Train Acc:=86.06833333333334, Evaluation Acc:=85.98\n",
            "Epoch=6 - Train Loss:=836.2853393554688, Evaluation Loss:=0.42501044273376465, Train Acc:=86.84833333333334, Evaluation Acc:=86.64\n",
            "Epoch=7 - Train Loss:=794.1386108398438, Evaluation Loss:=0.40536585450172424, Train Acc:=87.56833333333334, Evaluation Acc:=87.22\n",
            "Epoch=8 - Train Loss:=758.9507446289062, Evaluation Loss:=0.3900030553340912, Train Acc:=88.05, Evaluation Acc:=87.78\n",
            "Epoch=9 - Train Loss:=729.4917602539062, Evaluation Loss:=0.37815961241722107, Train Acc:=88.41833333333334, Evaluation Acc:=88.17\n",
            "Epoch=0 - Train Loss:=3672.329345703125, Evaluation Loss:=1.034359335899353, Train Acc:=66.22833333333334, Evaluation Acc:=66.82000000000001\n",
            "Epoch=1 - Train Loss:=1646.424072265625, Evaluation Loss:=0.7368810176849365, Train Acc:=75.95833333333334, Evaluation Acc:=76.66\n",
            "Epoch=2 - Train Loss:=1296.49169921875, Evaluation Loss:=0.6182711720466614, Train Acc:=80.095, Evaluation Acc:=80.93\n",
            "Epoch=3 - Train Loss:=1125.150634765625, Evaluation Loss:=0.5521417856216431, Train Acc:=82.27, Evaluation Acc:=83.09\n",
            "Epoch=4 - Train Loss:=1018.8025512695312, Evaluation Loss:=0.5079426169395447, Train Acc:=83.95333333333333, Evaluation Acc:=84.78999999999999\n",
            "Epoch=5 - Train Loss:=944.2691040039062, Evaluation Loss:=0.47468289732933044, Train Acc:=85.10166666666666, Evaluation Acc:=85.77\n",
            "Epoch=6 - Train Loss:=887.9849853515625, Evaluation Loss:=0.45124688744544983, Train Acc:=85.92333333333333, Evaluation Acc:=86.33999999999999\n",
            "Epoch=7 - Train Loss:=842.7305908203125, Evaluation Loss:=0.4295508861541748, Train Acc:=86.53999999999999, Evaluation Acc:=87.22\n",
            "Epoch=8 - Train Loss:=806.0008544921875, Evaluation Loss:=0.41278108954429626, Train Acc:=87.22166666666666, Evaluation Acc:=87.7\n",
            "Epoch=9 - Train Loss:=774.3118896484375, Evaluation Loss:=0.40036460757255554, Train Acc:=87.64999999999999, Evaluation Acc:=88.18\n",
            "Epoch=0 - Train Loss:=3601.327392578125, Evaluation Loss:=0.9788132905960083, Train Acc:=67.98166666666667, Evaluation Acc:=68.99\n",
            "Epoch=1 - Train Loss:=1554.80859375, Evaluation Loss:=0.6866205930709839, Train Acc:=77.53833333333333, Evaluation Acc:=78.60000000000001\n",
            "Epoch=2 - Train Loss:=1215.1834716796875, Evaluation Loss:=0.5750308632850647, Train Acc:=81.54, Evaluation Acc:=82.14\n",
            "Epoch=3 - Train Loss:=1056.013916015625, Evaluation Loss:=0.5119484663009644, Train Acc:=83.61333333333333, Evaluation Acc:=84.22\n",
            "Epoch=4 - Train Loss:=957.4677734375, Evaluation Loss:=0.47105738520622253, Train Acc:=85.00999999999999, Evaluation Acc:=85.59\n",
            "Epoch=5 - Train Loss:=888.3524169921875, Evaluation Loss:=0.4399605095386505, Train Acc:=86.055, Evaluation Acc:=86.48\n",
            "Epoch=6 - Train Loss:=835.843017578125, Evaluation Loss:=0.41739198565483093, Train Acc:=86.85000000000001, Evaluation Acc:=87.25\n",
            "Epoch=7 - Train Loss:=794.3907470703125, Evaluation Loss:=0.39699459075927734, Train Acc:=87.51833333333333, Evaluation Acc:=87.8\n",
            "Epoch=8 - Train Loss:=759.4384155273438, Evaluation Loss:=0.381402850151062, Train Acc:=88.03833333333333, Evaluation Acc:=88.35\n",
            "Epoch=9 - Train Loss:=731.0810546875, Evaluation Loss:=0.3668718934059143, Train Acc:=88.58500000000001, Evaluation Acc:=88.99000000000001\n",
            "Epoch=0 - Train Loss:=3779.33251953125, Evaluation Loss:=1.0068174600601196, Train Acc:=67.11, Evaluation Acc:=68.38\n",
            "Epoch=1 - Train Loss:=1629.746826171875, Evaluation Loss:=0.7059100270271301, Train Acc:=76.665, Evaluation Acc:=77.83\n",
            "Epoch=2 - Train Loss:=1264.8443603515625, Evaluation Loss:=0.5842861533164978, Train Acc:=80.76333333333334, Evaluation Acc:=82.03\n",
            "Epoch=3 - Train Loss:=1092.7069091796875, Evaluation Loss:=0.5199095010757446, Train Acc:=82.98166666666667, Evaluation Acc:=84.23\n",
            "Epoch=4 - Train Loss:=987.7239379882812, Evaluation Loss:=0.47689101099967957, Train Acc:=84.47833333333334, Evaluation Acc:=85.54\n",
            "Epoch=5 - Train Loss:=913.9345703125, Evaluation Loss:=0.4470972716808319, Train Acc:=85.54333333333334, Evaluation Acc:=86.33\n",
            "Epoch=6 - Train Loss:=858.50341796875, Evaluation Loss:=0.422641783952713, Train Acc:=86.50166666666667, Evaluation Acc:=87.18\n",
            "Epoch=7 - Train Loss:=814.5863037109375, Evaluation Loss:=0.404148668050766, Train Acc:=87.21166666666666, Evaluation Acc:=87.88\n",
            "Epoch=8 - Train Loss:=778.1985473632812, Evaluation Loss:=0.38859981298446655, Train Acc:=87.68166666666667, Evaluation Acc:=88.39\n",
            "Epoch=9 - Train Loss:=747.9761352539062, Evaluation Loss:=0.37559032440185547, Train Acc:=88.25666666666667, Evaluation Acc:=88.66000000000001\n"
          ]
        }
      ],
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "hidden_size = 512\n",
        "learning_rate = 1e-4\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "optimizer = \"SGD\"\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i + 2)\n",
        "    \n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"mnist\")\n",
        "\n",
        "    model1 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer=optimizer)\n",
        "    train_losses, eval_losses, train_accuracy, eval_accuracy, _epoch_times = train(model1, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs)\n",
        "    acc = test(model1, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "nFYSjwtBW7QX",
        "outputId": "60622550-771e-41fd-bf7d-f9b2d4fd1e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 88.526, and the variance is 0.16448399999999969\n",
            "the average running time for one epoch is: 31.630993814468383\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fb3e07d33d0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fb3e07df410>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb3e07df950>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fb3e0766450>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fb3e07dfed0>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fb3e07d3950>,\n",
              "  <matplotlib.lines.Line2D at 0x7fb3e07d3e90>]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Ge-GyQZG7E"
      },
      "source": [
        "### Model2: with dropout regularization + SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dgktMs5azZXP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5440d2-e9ad-44f3-ed5f-71c0aa572580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=2290.04638671875, Evaluation Loss:=0.42360302805900574, Train Acc:=86.495, Evaluation Acc:=86.99\n",
            "Epoch=1 - Train Loss:=994.1759033203125, Evaluation Loss:=0.3398791551589966, Train Acc:=89.40833333333333, Evaluation Acc:=89.92\n",
            "Epoch=2 - Train Loss:=809.4541625976562, Evaluation Loss:=0.3048611581325531, Train Acc:=90.85833333333333, Evaluation Acc:=90.95\n",
            "Epoch=3 - Train Loss:=711.1637573242188, Evaluation Loss:=0.2712585926055908, Train Acc:=91.84833333333333, Evaluation Acc:=91.74\n",
            "Epoch=4 - Train Loss:=652.8272094726562, Evaluation Loss:=0.24812550842761993, Train Acc:=92.63166666666666, Evaluation Acc:=92.44\n",
            "Epoch=5 - Train Loss:=593.2278442382812, Evaluation Loss:=0.23679064214229584, Train Acc:=93.045, Evaluation Acc:=92.94\n",
            "Epoch=6 - Train Loss:=560.711669921875, Evaluation Loss:=0.22530443966388702, Train Acc:=93.585, Evaluation Acc:=93.33\n",
            "Epoch=7 - Train Loss:=528.3485107421875, Evaluation Loss:=0.2184758484363556, Train Acc:=93.74166666666667, Evaluation Acc:=93.26\n",
            "Epoch=8 - Train Loss:=497.5807800292969, Evaluation Loss:=0.20528315007686615, Train Acc:=94.16, Evaluation Acc:=93.74\n",
            "Epoch=9 - Train Loss:=475.4455261230469, Evaluation Loss:=0.19923779368400574, Train Acc:=94.43833333333333, Evaluation Acc:=93.92\n",
            "Epoch=0 - Train Loss:=2258.3828125, Evaluation Loss:=0.43291348218917847, Train Acc:=86.32666666666667, Evaluation Acc:=86.68\n",
            "Epoch=1 - Train Loss:=992.9779052734375, Evaluation Loss:=0.3514426648616791, Train Acc:=89.25333333333333, Evaluation Acc:=89.18\n",
            "Epoch=2 - Train Loss:=817.2813110351562, Evaluation Loss:=0.3013454079627991, Train Acc:=90.80333333333334, Evaluation Acc:=90.84\n",
            "Epoch=3 - Train Loss:=720.88037109375, Evaluation Loss:=0.2704399824142456, Train Acc:=91.80333333333334, Evaluation Acc:=91.86\n",
            "Epoch=4 - Train Loss:=653.9852294921875, Evaluation Loss:=0.2548530399799347, Train Acc:=92.44333333333333, Evaluation Acc:=92.31\n",
            "Epoch=5 - Train Loss:=603.9822387695312, Evaluation Loss:=0.24226894974708557, Train Acc:=92.66666666666666, Evaluation Acc:=92.75\n",
            "Epoch=6 - Train Loss:=563.434814453125, Evaluation Loss:=0.22391776740550995, Train Acc:=93.425, Evaluation Acc:=93.28999999999999\n",
            "Epoch=7 - Train Loss:=533.50634765625, Evaluation Loss:=0.21611836552619934, Train Acc:=93.70166666666667, Evaluation Acc:=93.53\n",
            "Epoch=8 - Train Loss:=504.4693908691406, Evaluation Loss:=0.20934650301933289, Train Acc:=93.89666666666666, Evaluation Acc:=93.7\n",
            "Epoch=9 - Train Loss:=477.88946533203125, Evaluation Loss:=0.19741898775100708, Train Acc:=94.30333333333333, Evaluation Acc:=94.04\n",
            "Epoch=0 - Train Loss:=2258.92431640625, Evaluation Loss:=0.43779346346855164, Train Acc:=86.31166666666667, Evaluation Acc:=86.8\n",
            "Epoch=1 - Train Loss:=1006.8350830078125, Evaluation Loss:=0.34885793924331665, Train Acc:=89.3, Evaluation Acc:=89.53999999999999\n",
            "Epoch=2 - Train Loss:=830.3094482421875, Evaluation Loss:=0.2924547493457794, Train Acc:=91.11166666666666, Evaluation Acc:=91.25\n",
            "Epoch=3 - Train Loss:=720.2965698242188, Evaluation Loss:=0.2686235010623932, Train Acc:=91.93666666666667, Evaluation Acc:=92.0\n",
            "Epoch=4 - Train Loss:=658.6967163085938, Evaluation Loss:=0.2579271197319031, Train Acc:=92.28833333333334, Evaluation Acc:=92.34\n",
            "Epoch=5 - Train Loss:=609.2782592773438, Evaluation Loss:=0.23935893177986145, Train Acc:=92.87166666666667, Evaluation Acc:=92.69\n",
            "Epoch=6 - Train Loss:=555.0494995117188, Evaluation Loss:=0.22882939875125885, Train Acc:=93.265, Evaluation Acc:=92.99\n",
            "Epoch=7 - Train Loss:=529.2268676757812, Evaluation Loss:=0.21434758603572845, Train Acc:=93.76833333333333, Evaluation Acc:=93.47999999999999\n",
            "Epoch=8 - Train Loss:=502.4416198730469, Evaluation Loss:=0.20531323552131653, Train Acc:=94.08666666666666, Evaluation Acc:=93.67999999999999\n",
            "Epoch=9 - Train Loss:=476.4453430175781, Evaluation Loss:=0.19956450164318085, Train Acc:=94.29833333333333, Evaluation Acc:=93.87\n",
            "Epoch=0 - Train Loss:=2133.23681640625, Evaluation Loss:=0.406215637922287, Train Acc:=86.80499999999999, Evaluation Acc:=87.66000000000001\n",
            "Epoch=1 - Train Loss:=981.0654296875, Evaluation Loss:=0.3392658829689026, Train Acc:=89.36166666666666, Evaluation Acc:=89.67\n",
            "Epoch=2 - Train Loss:=805.5372924804688, Evaluation Loss:=0.29520121216773987, Train Acc:=90.87333333333333, Evaluation Acc:=90.67\n",
            "Epoch=3 - Train Loss:=705.6759033203125, Evaluation Loss:=0.2640388607978821, Train Acc:=91.96, Evaluation Acc:=91.83\n",
            "Epoch=4 - Train Loss:=641.8771362304688, Evaluation Loss:=0.2477036565542221, Train Acc:=92.52666666666667, Evaluation Acc:=92.58999999999999\n",
            "Epoch=5 - Train Loss:=593.111572265625, Evaluation Loss:=0.22929248213768005, Train Acc:=93.28666666666666, Evaluation Acc:=93.13\n",
            "Epoch=6 - Train Loss:=547.56103515625, Evaluation Loss:=0.21915586292743683, Train Acc:=93.63333333333334, Evaluation Acc:=93.28999999999999\n",
            "Epoch=7 - Train Loss:=521.137451171875, Evaluation Loss:=0.21021440625190735, Train Acc:=94.00333333333334, Evaluation Acc:=93.8\n",
            "Epoch=8 - Train Loss:=496.5953063964844, Evaluation Loss:=0.20568257570266724, Train Acc:=94.055, Evaluation Acc:=93.77\n",
            "Epoch=9 - Train Loss:=469.2613525390625, Evaluation Loss:=0.20126119256019592, Train Acc:=94.255, Evaluation Acc:=93.77\n",
            "Epoch=0 - Train Loss:=2171.432373046875, Evaluation Loss:=0.43500351905822754, Train Acc:=86.83999999999999, Evaluation Acc:=87.07000000000001\n",
            "Epoch=1 - Train Loss:=987.7290649414062, Evaluation Loss:=0.37062782049179077, Train Acc:=88.73833333333333, Evaluation Acc:=88.7\n",
            "Epoch=2 - Train Loss:=814.8108520507812, Evaluation Loss:=0.3000415861606598, Train Acc:=91.08166666666668, Evaluation Acc:=91.03999999999999\n",
            "Epoch=3 - Train Loss:=715.981201171875, Evaluation Loss:=0.27563712000846863, Train Acc:=91.75333333333333, Evaluation Acc:=91.91\n",
            "Epoch=4 - Train Loss:=657.5452270507812, Evaluation Loss:=0.2650815546512604, Train Acc:=92.05333333333333, Evaluation Acc:=91.97\n",
            "Epoch=5 - Train Loss:=604.9371948242188, Evaluation Loss:=0.2426668405532837, Train Acc:=92.86, Evaluation Acc:=92.72\n",
            "Epoch=6 - Train Loss:=562.4429321289062, Evaluation Loss:=0.23150190711021423, Train Acc:=93.215, Evaluation Acc:=93.08\n",
            "Epoch=7 - Train Loss:=524.4346923828125, Evaluation Loss:=0.22564682364463806, Train Acc:=93.40833333333333, Evaluation Acc:=93.15\n",
            "Epoch=8 - Train Loss:=507.4137268066406, Evaluation Loss:=0.2094503939151764, Train Acc:=94.1, Evaluation Acc:=93.88\n",
            "Epoch=9 - Train Loss:=478.52764892578125, Evaluation Loss:=0.20345263183116913, Train Acc:=94.13, Evaluation Acc:=93.89\n",
            "Epoch=0 - Train Loss:=2155.047119140625, Evaluation Loss:=0.4207610487937927, Train Acc:=86.61166666666666, Evaluation Acc:=87.42999999999999\n",
            "Epoch=1 - Train Loss:=965.3455200195312, Evaluation Loss:=0.34563085436820984, Train Acc:=89.54833333333333, Evaluation Acc:=89.84\n",
            "Epoch=2 - Train Loss:=798.0143432617188, Evaluation Loss:=0.30561351776123047, Train Acc:=90.92833333333333, Evaluation Acc:=91.10000000000001\n",
            "Epoch=3 - Train Loss:=705.935302734375, Evaluation Loss:=0.2797895073890686, Train Acc:=91.77166666666666, Evaluation Acc:=91.73\n",
            "Epoch=4 - Train Loss:=637.49169921875, Evaluation Loss:=0.2564004361629486, Train Acc:=92.475, Evaluation Acc:=92.32000000000001\n",
            "Epoch=5 - Train Loss:=596.2252807617188, Evaluation Loss:=0.24678754806518555, Train Acc:=92.89500000000001, Evaluation Acc:=92.63\n",
            "Epoch=6 - Train Loss:=556.7726440429688, Evaluation Loss:=0.23453180491924286, Train Acc:=93.33333333333333, Evaluation Acc:=93.15\n",
            "Epoch=7 - Train Loss:=526.6935424804688, Evaluation Loss:=0.22270160913467407, Train Acc:=93.75333333333333, Evaluation Acc:=93.41000000000001\n",
            "Epoch=8 - Train Loss:=495.8409729003906, Evaluation Loss:=0.2171189934015274, Train Acc:=94.00833333333334, Evaluation Acc:=93.65\n",
            "Epoch=9 - Train Loss:=475.6594543457031, Evaluation Loss:=0.20473237335681915, Train Acc:=94.31666666666668, Evaluation Acc:=93.97999999999999\n",
            "Epoch=0 - Train Loss:=2224.251220703125, Evaluation Loss:=0.4015837013721466, Train Acc:=87.47833333333334, Evaluation Acc:=87.92\n",
            "Epoch=1 - Train Loss:=974.3084716796875, Evaluation Loss:=0.3267669081687927, Train Acc:=89.87333333333333, Evaluation Acc:=89.92999999999999\n",
            "Epoch=2 - Train Loss:=791.5919799804688, Evaluation Loss:=0.28362584114074707, Train Acc:=91.46666666666667, Evaluation Acc:=91.44\n",
            "Epoch=3 - Train Loss:=692.5350341796875, Evaluation Loss:=0.25788623094558716, Train Acc:=92.42, Evaluation Acc:=92.04\n",
            "Epoch=4 - Train Loss:=632.346923828125, Evaluation Loss:=0.24427002668380737, Train Acc:=92.78166666666667, Evaluation Acc:=92.47\n",
            "Epoch=5 - Train Loss:=576.8062133789062, Evaluation Loss:=0.23487690091133118, Train Acc:=93.05666666666667, Evaluation Acc:=92.78\n",
            "Epoch=6 - Train Loss:=536.5231323242188, Evaluation Loss:=0.22140228748321533, Train Acc:=93.475, Evaluation Acc:=93.33\n",
            "Epoch=7 - Train Loss:=503.7351379394531, Evaluation Loss:=0.2040279060602188, Train Acc:=94.13666666666667, Evaluation Acc:=93.97\n",
            "Epoch=8 - Train Loss:=480.0220031738281, Evaluation Loss:=0.1987820714712143, Train Acc:=94.24333333333334, Evaluation Acc:=93.97\n",
            "Epoch=9 - Train Loss:=455.8726806640625, Evaluation Loss:=0.19747786223888397, Train Acc:=94.36833333333333, Evaluation Acc:=94.05\n",
            "Epoch=0 - Train Loss:=2286.01123046875, Evaluation Loss:=0.4468596279621124, Train Acc:=85.94333333333334, Evaluation Acc:=86.56\n",
            "Epoch=1 - Train Loss:=1007.9467163085938, Evaluation Loss:=0.3522481620311737, Train Acc:=89.12833333333333, Evaluation Acc:=89.52\n",
            "Epoch=2 - Train Loss:=817.708251953125, Evaluation Loss:=0.3208581805229187, Train Acc:=90.35166666666666, Evaluation Acc:=90.36\n",
            "Epoch=3 - Train Loss:=722.6723022460938, Evaluation Loss:=0.2826974093914032, Train Acc:=91.65166666666667, Evaluation Acc:=91.57\n",
            "Epoch=4 - Train Loss:=659.1787719726562, Evaluation Loss:=0.2706041634082794, Train Acc:=92.065, Evaluation Acc:=91.89\n",
            "Epoch=5 - Train Loss:=605.7105712890625, Evaluation Loss:=0.24905551970005035, Train Acc:=92.75833333333333, Evaluation Acc:=92.46\n",
            "Epoch=6 - Train Loss:=568.7860717773438, Evaluation Loss:=0.24012956023216248, Train Acc:=93.10000000000001, Evaluation Acc:=92.72\n",
            "Epoch=7 - Train Loss:=541.2657470703125, Evaluation Loss:=0.2243446260690689, Train Acc:=93.645, Evaluation Acc:=93.34\n",
            "Epoch=8 - Train Loss:=513.6299438476562, Evaluation Loss:=0.21538850665092468, Train Acc:=93.86833333333333, Evaluation Acc:=93.55\n",
            "Epoch=9 - Train Loss:=484.887451171875, Evaluation Loss:=0.21115268766880035, Train Acc:=94.07, Evaluation Acc:=93.73\n",
            "Epoch=0 - Train Loss:=2183.858642578125, Evaluation Loss:=0.395578533411026, Train Acc:=87.64999999999999, Evaluation Acc:=88.18\n",
            "Epoch=1 - Train Loss:=979.0638427734375, Evaluation Loss:=0.3332899212837219, Train Acc:=89.77000000000001, Evaluation Acc:=89.94\n",
            "Epoch=2 - Train Loss:=803.8873901367188, Evaluation Loss:=0.2828098237514496, Train Acc:=91.35166666666666, Evaluation Acc:=91.5\n",
            "Epoch=3 - Train Loss:=707.2086181640625, Evaluation Loss:=0.25694188475608826, Train Acc:=92.27499999999999, Evaluation Acc:=92.42\n",
            "Epoch=4 - Train Loss:=640.90234375, Evaluation Loss:=0.2416657656431198, Train Acc:=92.82166666666667, Evaluation Acc:=92.86999999999999\n",
            "Epoch=5 - Train Loss:=585.5043334960938, Evaluation Loss:=0.22791047394275665, Train Acc:=93.25, Evaluation Acc:=93.06\n",
            "Epoch=6 - Train Loss:=555.2848510742188, Evaluation Loss:=0.21968793869018555, Train Acc:=93.595, Evaluation Acc:=93.47\n",
            "Epoch=7 - Train Loss:=520.1690673828125, Evaluation Loss:=0.20408186316490173, Train Acc:=93.97833333333332, Evaluation Acc:=93.92\n",
            "Epoch=8 - Train Loss:=494.6269226074219, Evaluation Loss:=0.20188359916210175, Train Acc:=94.175, Evaluation Acc:=94.01\n",
            "Epoch=9 - Train Loss:=472.1905212402344, Evaluation Loss:=0.1911105364561081, Train Acc:=94.475, Evaluation Acc:=94.35\n",
            "Epoch=0 - Train Loss:=2260.520751953125, Evaluation Loss:=0.41768398880958557, Train Acc:=86.59166666666667, Evaluation Acc:=87.11\n",
            "Epoch=1 - Train Loss:=978.7640380859375, Evaluation Loss:=0.3400411307811737, Train Acc:=89.38666666666667, Evaluation Acc:=89.67\n",
            "Epoch=2 - Train Loss:=793.7108764648438, Evaluation Loss:=0.2984953224658966, Train Acc:=90.67333333333333, Evaluation Acc:=90.97\n",
            "Epoch=3 - Train Loss:=698.4912109375, Evaluation Loss:=0.27148833870887756, Train Acc:=91.57833333333333, Evaluation Acc:=91.96\n",
            "Epoch=4 - Train Loss:=640.4219970703125, Evaluation Loss:=0.25493094325065613, Train Acc:=92.21000000000001, Evaluation Acc:=92.47\n",
            "Epoch=5 - Train Loss:=592.782470703125, Evaluation Loss:=0.24155470728874207, Train Acc:=92.64, Evaluation Acc:=92.75999999999999\n",
            "Epoch=6 - Train Loss:=545.68603515625, Evaluation Loss:=0.22989743947982788, Train Acc:=92.82666666666667, Evaluation Acc:=93.04\n",
            "Epoch=7 - Train Loss:=515.4273071289062, Evaluation Loss:=0.21503768861293793, Train Acc:=93.55166666666666, Evaluation Acc:=93.44\n",
            "Epoch=8 - Train Loss:=491.3415832519531, Evaluation Loss:=0.20629379153251648, Train Acc:=93.83333333333333, Evaluation Acc:=93.77\n",
            "Epoch=9 - Train Loss:=469.0205993652344, Evaluation Loss:=0.20293833315372467, Train Acc:=93.90166666666667, Evaluation Acc:=93.66\n"
          ]
        }
      ],
      "source": [
        "input_size = 28 * 28\n",
        "output_size = 10\n",
        "optimizer = \"SGD\"\n",
        "\n",
        "hidden_size = 512\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "dropout = 0.2\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i + 2)\n",
        "    x_train_tf, y_train_tf, x_test_tf, y_test_tf = load_data(dataset=\"mnist\")\n",
        "\n",
        "    model2 = MLP(input_size, output_size, hidden_size, optimizer=optimizer)\n",
        "    train_losses, eval_losses, train_accuracy, test_accuracy, _epoch_times = train(model2, x_train_tf, y_train_tf, x_test_tf, y_test_tf,\n",
        "            batch_size = batch_size, \n",
        "            learning_rate = learning_rate, \n",
        "            epochs = epochs,\n",
        "            dropout_rate = dropout)\n",
        "\n",
        "    acc = test(model2, x_test_tf, y_test_tf)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "dYVmykHTfh_3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "29a1ca96-6ba2-4a7f-e572-a7bed1186406"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 93.926, and the variance is 0.03490399999999957\n",
            "the average running time for one epoch is: 19.75828253030777\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fec522021d0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fec5220e250>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec5220e790>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fec52214290>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fec5220ed10>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fec52202790>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec52202cd0>]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model3: without regularization + Adam optimizer"
      ],
      "metadata": {
        "id": "G4Qak0QzWbuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "\n",
        "hidden_size = 512\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "optimizer = \"Adam\"\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i+3)\n",
        "\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"mnist\")\n",
        "    model3 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer=optimizer)\n",
        "    train_losses, eval_losses, train_accuracy, eval_accuracy, _epoch_times = train(model3, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs)\n",
        "    acc = test(model3, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "UDCEaIXAWiLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc47af7-2d69-4f9c-b9ae-db94f1184950"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=1132.806640625, Evaluation Loss:=0.35690948367118835, Train Acc:=96.24333333333334, Evaluation Acc:=95.93\n",
            "Epoch=1 - Train Loss:=628.921142578125, Evaluation Loss:=0.23519393801689148, Train Acc:=97.81166666666667, Evaluation Acc:=97.24000000000001\n",
            "Epoch=2 - Train Loss:=448.2666931152344, Evaluation Loss:=0.27761533856391907, Train Acc:=97.73666666666668, Evaluation Acc:=97.06\n",
            "Epoch=3 - Train Loss:=361.69970703125, Evaluation Loss:=0.2166418433189392, Train Acc:=98.555, Evaluation Acc:=97.58\n",
            "Epoch=4 - Train Loss:=289.9563903808594, Evaluation Loss:=0.23908691108226776, Train Acc:=98.735, Evaluation Acc:=97.67\n",
            "Epoch=5 - Train Loss:=227.55096435546875, Evaluation Loss:=0.27257248759269714, Train Acc:=98.73833333333333, Evaluation Acc:=97.49\n",
            "Epoch=6 - Train Loss:=196.1151885986328, Evaluation Loss:=0.2719242572784424, Train Acc:=99.03833333333333, Evaluation Acc:=97.44\n",
            "Epoch=7 - Train Loss:=151.28187561035156, Evaluation Loss:=0.27489936351776123, Train Acc:=99.28833333333333, Evaluation Acc:=97.85000000000001\n",
            "Epoch=8 - Train Loss:=127.34305572509766, Evaluation Loss:=0.23804017901420593, Train Acc:=99.43166666666666, Evaluation Acc:=97.87\n",
            "Epoch=9 - Train Loss:=108.72166442871094, Evaluation Loss:=0.2654460072517395, Train Acc:=99.495, Evaluation Acc:=97.74000000000001\n",
            "Epoch=0 - Train Loss:=1126.2259521484375, Evaluation Loss:=0.3377085328102112, Train Acc:=95.89333333333333, Evaluation Acc:=95.74000000000001\n",
            "Epoch=1 - Train Loss:=611.45654296875, Evaluation Loss:=0.2801382541656494, Train Acc:=97.45666666666666, Evaluation Acc:=96.93\n",
            "Epoch=2 - Train Loss:=442.52996826171875, Evaluation Loss:=0.2638978660106659, Train Acc:=98.05833333333334, Evaluation Acc:=97.08\n",
            "Epoch=3 - Train Loss:=355.2479553222656, Evaluation Loss:=0.27336058020591736, Train Acc:=98.44166666666668, Evaluation Acc:=97.38\n",
            "Epoch=4 - Train Loss:=286.7353515625, Evaluation Loss:=0.32755476236343384, Train Acc:=98.32166666666666, Evaluation Acc:=96.93\n",
            "Epoch=5 - Train Loss:=232.21026611328125, Evaluation Loss:=0.2658659815788269, Train Acc:=99.01666666666667, Evaluation Acc:=97.61\n",
            "Epoch=6 - Train Loss:=195.97406005859375, Evaluation Loss:=0.2772960364818573, Train Acc:=98.905, Evaluation Acc:=97.64\n",
            "Epoch=7 - Train Loss:=156.54031372070312, Evaluation Loss:=0.2500253915786743, Train Acc:=99.24833333333333, Evaluation Acc:=97.85000000000001\n",
            "Epoch=8 - Train Loss:=149.2738037109375, Evaluation Loss:=0.2637615501880646, Train Acc:=99.15166666666667, Evaluation Acc:=97.87\n",
            "Epoch=9 - Train Loss:=121.34776306152344, Evaluation Loss:=0.2506641149520874, Train Acc:=99.40333333333334, Evaluation Acc:=97.95\n",
            "Epoch=0 - Train Loss:=1107.265869140625, Evaluation Loss:=0.37118956446647644, Train Acc:=96.085, Evaluation Acc:=95.56\n",
            "Epoch=1 - Train Loss:=631.8806762695312, Evaluation Loss:=0.3302338421344757, Train Acc:=97.11999999999999, Evaluation Acc:=96.37\n",
            "Epoch=2 - Train Loss:=440.7329406738281, Evaluation Loss:=0.2771737575531006, Train Acc:=98.01833333333333, Evaluation Acc:=97.05\n",
            "Epoch=3 - Train Loss:=334.2350158691406, Evaluation Loss:=0.26381629705429077, Train Acc:=98.235, Evaluation Acc:=97.19\n",
            "Epoch=4 - Train Loss:=273.0962829589844, Evaluation Loss:=0.22432489693164825, Train Acc:=99.035, Evaluation Acc:=97.75\n",
            "Epoch=5 - Train Loss:=227.8747100830078, Evaluation Loss:=0.23518818616867065, Train Acc:=98.81333333333333, Evaluation Acc:=97.75\n",
            "Epoch=6 - Train Loss:=181.5873260498047, Evaluation Loss:=0.23366020619869232, Train Acc:=99.17833333333334, Evaluation Acc:=97.78999999999999\n",
            "Epoch=7 - Train Loss:=160.55856323242188, Evaluation Loss:=0.23011255264282227, Train Acc:=99.33833333333332, Evaluation Acc:=97.86\n",
            "Epoch=8 - Train Loss:=126.9908218383789, Evaluation Loss:=0.2213912308216095, Train Acc:=99.49666666666667, Evaluation Acc:=98.08\n",
            "Epoch=9 - Train Loss:=115.83940124511719, Evaluation Loss:=0.23003710806369781, Train Acc:=99.655, Evaluation Acc:=98.09\n",
            "Epoch=0 - Train Loss:=1094.4183349609375, Evaluation Loss:=0.38714638352394104, Train Acc:=95.83500000000001, Evaluation Acc:=95.34\n",
            "Epoch=1 - Train Loss:=602.4468383789062, Evaluation Loss:=0.36913731694221497, Train Acc:=96.86666666666667, Evaluation Acc:=96.14\n",
            "Epoch=2 - Train Loss:=452.45550537109375, Evaluation Loss:=0.30227890610694885, Train Acc:=97.86, Evaluation Acc:=96.67999999999999\n",
            "Epoch=3 - Train Loss:=333.0553283691406, Evaluation Loss:=0.2632027566432953, Train Acc:=98.29333333333334, Evaluation Acc:=97.33000000000001\n",
            "Epoch=4 - Train Loss:=275.01220703125, Evaluation Loss:=0.26652997732162476, Train Acc:=98.39166666666667, Evaluation Acc:=97.2\n",
            "Epoch=5 - Train Loss:=225.34523010253906, Evaluation Loss:=0.2620720863342285, Train Acc:=98.98, Evaluation Acc:=97.69\n",
            "Epoch=6 - Train Loss:=186.79685974121094, Evaluation Loss:=0.262661874294281, Train Acc:=99.27166666666668, Evaluation Acc:=97.77\n",
            "Epoch=7 - Train Loss:=161.36184692382812, Evaluation Loss:=0.3153219223022461, Train Acc:=98.94333333333334, Evaluation Acc:=97.50999999999999\n",
            "Epoch=8 - Train Loss:=131.49398803710938, Evaluation Loss:=0.28293195366859436, Train Acc:=99.53166666666667, Evaluation Acc:=97.92999999999999\n",
            "Epoch=9 - Train Loss:=111.56586456298828, Evaluation Loss:=0.23964235186576843, Train Acc:=99.59666666666666, Evaluation Acc:=98.00999999999999\n",
            "Epoch=0 - Train Loss:=1085.1927490234375, Evaluation Loss:=0.47301220893859863, Train Acc:=94.95, Evaluation Acc:=95.0\n",
            "Epoch=1 - Train Loss:=601.5382690429688, Evaluation Loss:=0.2725212275981903, Train Acc:=97.655, Evaluation Acc:=96.87\n",
            "Epoch=2 - Train Loss:=429.2225646972656, Evaluation Loss:=0.4449596703052521, Train Acc:=96.82666666666667, Evaluation Acc:=95.96000000000001\n",
            "Epoch=3 - Train Loss:=345.53802490234375, Evaluation Loss:=0.23833151161670685, Train Acc:=98.66166666666668, Evaluation Acc:=97.50999999999999\n",
            "Epoch=4 - Train Loss:=274.83807373046875, Evaluation Loss:=0.21599216759204865, Train Acc:=98.98166666666667, Evaluation Acc:=97.71\n",
            "Epoch=5 - Train Loss:=228.97232055664062, Evaluation Loss:=0.22196203470230103, Train Acc:=99.18833333333333, Evaluation Acc:=97.88\n",
            "Epoch=6 - Train Loss:=183.08306884765625, Evaluation Loss:=0.24228042364120483, Train Acc:=99.00333333333333, Evaluation Acc:=97.65\n",
            "Epoch=7 - Train Loss:=156.51971435546875, Evaluation Loss:=0.23299016058444977, Train Acc:=99.225, Evaluation Acc:=97.98\n",
            "Epoch=8 - Train Loss:=125.91265106201172, Evaluation Loss:=0.20914116501808167, Train Acc:=99.56666666666666, Evaluation Acc:=98.15\n",
            "Epoch=9 - Train Loss:=97.1530990600586, Evaluation Loss:=0.24047847092151642, Train Acc:=99.66333333333334, Evaluation Acc:=98.05\n",
            "Epoch=0 - Train Loss:=1105.4154052734375, Evaluation Loss:=0.3180064260959625, Train Acc:=96.44, Evaluation Acc:=96.13000000000001\n",
            "Epoch=1 - Train Loss:=601.2005004882812, Evaluation Loss:=0.30412623286247253, Train Acc:=97.33333333333334, Evaluation Acc:=96.55\n",
            "Epoch=2 - Train Loss:=437.7344055175781, Evaluation Loss:=0.24406731128692627, Train Acc:=98.17166666666667, Evaluation Acc:=97.26\n",
            "Epoch=3 - Train Loss:=341.4091796875, Evaluation Loss:=0.25477471947669983, Train Acc:=98.60166666666666, Evaluation Acc:=97.45\n",
            "Epoch=4 - Train Loss:=278.4156799316406, Evaluation Loss:=0.24306727945804596, Train Acc:=98.64666666666668, Evaluation Acc:=97.49\n",
            "Epoch=5 - Train Loss:=223.76925659179688, Evaluation Loss:=0.29673847556114197, Train Acc:=98.71166666666666, Evaluation Acc:=97.3\n",
            "Epoch=6 - Train Loss:=189.32028198242188, Evaluation Loss:=0.23616532981395721, Train Acc:=99.12166666666667, Evaluation Acc:=97.88\n",
            "Epoch=7 - Train Loss:=172.97926330566406, Evaluation Loss:=0.22303681075572968, Train Acc:=99.46666666666667, Evaluation Acc:=97.89999999999999\n",
            "Epoch=8 - Train Loss:=130.1262664794922, Evaluation Loss:=0.2820945978164673, Train Acc:=99.28666666666666, Evaluation Acc:=97.72999999999999\n",
            "Epoch=9 - Train Loss:=107.61971282958984, Evaluation Loss:=0.21293918788433075, Train Acc:=99.53666666666666, Evaluation Acc:=98.09\n",
            "Epoch=0 - Train Loss:=1120.17138671875, Evaluation Loss:=0.3060249388217926, Train Acc:=96.03333333333333, Evaluation Acc:=95.8\n",
            "Epoch=1 - Train Loss:=616.8206176757812, Evaluation Loss:=0.2550233006477356, Train Acc:=97.485, Evaluation Acc:=96.94\n",
            "Epoch=2 - Train Loss:=443.0208435058594, Evaluation Loss:=0.2399648129940033, Train Acc:=98.14666666666668, Evaluation Acc:=97.39\n",
            "Epoch=3 - Train Loss:=353.5891418457031, Evaluation Loss:=0.2309959977865219, Train Acc:=98.485, Evaluation Acc:=97.45\n",
            "Epoch=4 - Train Loss:=283.419189453125, Evaluation Loss:=0.2539573907852173, Train Acc:=98.47833333333334, Evaluation Acc:=97.47\n",
            "Epoch=5 - Train Loss:=229.24163818359375, Evaluation Loss:=0.25886204838752747, Train Acc:=98.91, Evaluation Acc:=97.46000000000001\n",
            "Epoch=6 - Train Loss:=189.3514862060547, Evaluation Loss:=0.2411656230688095, Train Acc:=99.16833333333334, Evaluation Acc:=97.78999999999999\n",
            "Epoch=7 - Train Loss:=167.3279571533203, Evaluation Loss:=0.2434731423854828, Train Acc:=99.215, Evaluation Acc:=97.68\n",
            "Epoch=8 - Train Loss:=129.32025146484375, Evaluation Loss:=0.30562588572502136, Train Acc:=99.18666666666667, Evaluation Acc:=97.6\n",
            "Epoch=9 - Train Loss:=109.87391662597656, Evaluation Loss:=0.2410430908203125, Train Acc:=99.45666666666668, Evaluation Acc:=98.06\n",
            "Epoch=0 - Train Loss:=1123.679443359375, Evaluation Loss:=0.42305028438568115, Train Acc:=95.565, Evaluation Acc:=95.19999999999999\n",
            "Epoch=1 - Train Loss:=636.423828125, Evaluation Loss:=0.2971455454826355, Train Acc:=97.15, Evaluation Acc:=96.46000000000001\n",
            "Epoch=2 - Train Loss:=493.47320556640625, Evaluation Loss:=0.25317656993865967, Train Acc:=97.97, Evaluation Acc:=97.09\n",
            "Epoch=3 - Train Loss:=368.2033386230469, Evaluation Loss:=0.25207680463790894, Train Acc:=98.55000000000001, Evaluation Acc:=97.39\n",
            "Epoch=4 - Train Loss:=283.0637512207031, Evaluation Loss:=0.24013742804527283, Train Acc:=98.62166666666667, Evaluation Acc:=97.6\n",
            "Epoch=5 - Train Loss:=225.9412078857422, Evaluation Loss:=0.23713339865207672, Train Acc:=98.94500000000001, Evaluation Acc:=97.85000000000001\n",
            "Epoch=6 - Train Loss:=189.7760772705078, Evaluation Loss:=0.25474682450294495, Train Acc:=99.03666666666666, Evaluation Acc:=97.84\n",
            "Epoch=7 - Train Loss:=154.2704620361328, Evaluation Loss:=0.22549644112586975, Train Acc:=99.395, Evaluation Acc:=98.03\n",
            "Epoch=8 - Train Loss:=125.35218811035156, Evaluation Loss:=0.26716434955596924, Train Acc:=99.43166666666666, Evaluation Acc:=97.86\n",
            "Epoch=9 - Train Loss:=104.28457641601562, Evaluation Loss:=0.26178139448165894, Train Acc:=99.52666666666666, Evaluation Acc:=98.0\n",
            "Epoch=0 - Train Loss:=1117.161376953125, Evaluation Loss:=0.4400906264781952, Train Acc:=95.31666666666668, Evaluation Acc:=94.67999999999999\n",
            "Epoch=1 - Train Loss:=612.5478515625, Evaluation Loss:=0.3003377914428711, Train Acc:=97.355, Evaluation Acc:=96.71\n",
            "Epoch=2 - Train Loss:=467.1455078125, Evaluation Loss:=0.28590288758277893, Train Acc:=97.96000000000001, Evaluation Acc:=97.19\n",
            "Epoch=3 - Train Loss:=362.1997985839844, Evaluation Loss:=0.25412505865097046, Train Acc:=98.36333333333333, Evaluation Acc:=97.32\n",
            "Epoch=4 - Train Loss:=298.5729675292969, Evaluation Loss:=0.2230895757675171, Train Acc:=98.80333333333333, Evaluation Acc:=97.76\n",
            "Epoch=5 - Train Loss:=247.7244110107422, Evaluation Loss:=0.22555266320705414, Train Acc:=98.79833333333333, Evaluation Acc:=97.76\n",
            "Epoch=6 - Train Loss:=204.67605590820312, Evaluation Loss:=0.25642696022987366, Train Acc:=99.15666666666667, Evaluation Acc:=97.81\n",
            "Epoch=7 - Train Loss:=166.73643493652344, Evaluation Loss:=0.21677598357200623, Train Acc:=99.39333333333333, Evaluation Acc:=98.04\n",
            "Epoch=8 - Train Loss:=143.08193969726562, Evaluation Loss:=0.28811582922935486, Train Acc:=99.26333333333334, Evaluation Acc:=97.78\n",
            "Epoch=9 - Train Loss:=113.11380004882812, Evaluation Loss:=0.2862585186958313, Train Acc:=99.27333333333334, Evaluation Acc:=97.76\n",
            "Epoch=0 - Train Loss:=1115.244140625, Evaluation Loss:=0.37141117453575134, Train Acc:=96.16666666666667, Evaluation Acc:=95.81\n",
            "Epoch=1 - Train Loss:=627.3920288085938, Evaluation Loss:=0.32565784454345703, Train Acc:=96.86, Evaluation Acc:=96.17\n",
            "Epoch=2 - Train Loss:=446.5097351074219, Evaluation Loss:=0.23789151012897491, Train Acc:=98.18833333333333, Evaluation Acc:=97.56\n",
            "Epoch=3 - Train Loss:=362.6246643066406, Evaluation Loss:=0.2809147834777832, Train Acc:=98.45333333333333, Evaluation Acc:=97.33000000000001\n",
            "Epoch=4 - Train Loss:=275.4794921875, Evaluation Loss:=0.27144256234169006, Train Acc:=98.63, Evaluation Acc:=97.23\n",
            "Epoch=5 - Train Loss:=224.13229370117188, Evaluation Loss:=0.2334234118461609, Train Acc:=99.15, Evaluation Acc:=97.86\n",
            "Epoch=6 - Train Loss:=188.03807067871094, Evaluation Loss:=0.25795361399650574, Train Acc:=98.95833333333334, Evaluation Acc:=97.72999999999999\n",
            "Epoch=7 - Train Loss:=160.15538024902344, Evaluation Loss:=0.2529520094394684, Train Acc:=99.18166666666667, Evaluation Acc:=97.83\n",
            "Epoch=8 - Train Loss:=124.70648956298828, Evaluation Loss:=0.23442919552326202, Train Acc:=99.46166666666667, Evaluation Acc:=97.98\n",
            "Epoch=9 - Train Loss:=100.75068664550781, Evaluation Loss:=0.34661513566970825, Train Acc:=98.96000000000001, Evaluation Acc:=97.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "qMYFaf6R1rTK",
        "outputId": "eaf10713-3a29-4f89-aef2-c28c2b2d0fc0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 97.925, and the variance is 0.03438499999999957\n",
            "the average running time for one epoch is: 32.67204792261124\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fec520bfd50>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fec520c6e10>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec520cc390>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fec520cce50>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fec520cc910>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fec520c6390>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec520c68d0>]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model4: with dropout regularization + Adam optimizer"
      ],
      "metadata": {
        "id": "4H_SCylKWx5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28 * 28\n",
        "output_size = 10\n",
        "optimizer = \"Adam\"\n",
        "\n",
        "hidden_size = 512\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "dropout = 0.2\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i+4)\n",
        "\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"mnist\")\n",
        "\n",
        "    model4 = MLP(input_size, output_size, hidden_size, optimizer=optimizer)\n",
        "    train_losses, eval_losses, train_accuracy, test_accuracy, _epoch_times = train(model4, x_train, y_train, x_test, y_test,\n",
        "            batch_size = batch_size, \n",
        "            learning_rate = learning_rate, \n",
        "            epochs = epochs,\n",
        "            dropout_rate = dropout)\n",
        "    acc = test(model4, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "XyEfCEWWW5ze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5012ee44-c63f-4d8b-d001-5380e4c6092d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=1356.58154296875, Evaluation Loss:=0.4098091721534729, Train Acc:=95.46666666666667, Evaluation Acc:=95.21\n",
            "Epoch=1 - Train Loss:=747.9056396484375, Evaluation Loss:=0.279823899269104, Train Acc:=97.36166666666666, Evaluation Acc:=96.83\n",
            "Epoch=2 - Train Loss:=551.6994018554688, Evaluation Loss:=0.2894348204135895, Train Acc:=97.845, Evaluation Acc:=97.1\n",
            "Epoch=3 - Train Loss:=449.4662170410156, Evaluation Loss:=0.23796038329601288, Train Acc:=98.38333333333334, Evaluation Acc:=97.56\n",
            "Epoch=4 - Train Loss:=390.0718688964844, Evaluation Loss:=0.2235444039106369, Train Acc:=98.57000000000001, Evaluation Acc:=97.64\n",
            "Epoch=5 - Train Loss:=327.2735595703125, Evaluation Loss:=0.21702821552753448, Train Acc:=98.75500000000001, Evaluation Acc:=97.72999999999999\n",
            "Epoch=6 - Train Loss:=297.8033752441406, Evaluation Loss:=0.24531279504299164, Train Acc:=98.92166666666667, Evaluation Acc:=97.78\n",
            "Epoch=7 - Train Loss:=261.1590270996094, Evaluation Loss:=0.2368413507938385, Train Acc:=99.01833333333333, Evaluation Acc:=97.95\n",
            "Epoch=8 - Train Loss:=226.99301147460938, Evaluation Loss:=0.26497745513916016, Train Acc:=99.02333333333333, Evaluation Acc:=97.84\n",
            "Epoch=9 - Train Loss:=192.50289916992188, Evaluation Loss:=0.2422708421945572, Train Acc:=99.18166666666667, Evaluation Acc:=97.95\n",
            "Epoch=0 - Train Loss:=1342.9666748046875, Evaluation Loss:=0.37023475766181946, Train Acc:=95.935, Evaluation Acc:=95.78\n",
            "Epoch=1 - Train Loss:=723.1802978515625, Evaluation Loss:=0.33382678031921387, Train Acc:=96.82666666666667, Evaluation Acc:=96.3\n",
            "Epoch=2 - Train Loss:=540.6011962890625, Evaluation Loss:=0.29275643825531006, Train Acc:=97.89166666666667, Evaluation Acc:=97.03\n",
            "Epoch=3 - Train Loss:=433.34814453125, Evaluation Loss:=0.23029567301273346, Train Acc:=98.54333333333334, Evaluation Acc:=97.59\n",
            "Epoch=4 - Train Loss:=346.40179443359375, Evaluation Loss:=0.24426423013210297, Train Acc:=98.59666666666666, Evaluation Acc:=97.64\n",
            "Epoch=5 - Train Loss:=315.7350158691406, Evaluation Loss:=0.23778164386749268, Train Acc:=98.94833333333334, Evaluation Acc:=97.68\n",
            "Epoch=6 - Train Loss:=264.7379455566406, Evaluation Loss:=0.20763880014419556, Train Acc:=99.08500000000001, Evaluation Acc:=97.88\n",
            "Epoch=7 - Train Loss:=224.7152557373047, Evaluation Loss:=0.2285994589328766, Train Acc:=99.03666666666666, Evaluation Acc:=97.91\n",
            "Epoch=8 - Train Loss:=203.80441284179688, Evaluation Loss:=0.24742616713047028, Train Acc:=99.13166666666666, Evaluation Acc:=97.89\n",
            "Epoch=9 - Train Loss:=183.22677612304688, Evaluation Loss:=0.24066533148288727, Train Acc:=99.39166666666667, Evaluation Acc:=97.99\n",
            "Epoch=0 - Train Loss:=1315.981689453125, Evaluation Loss:=0.39657121896743774, Train Acc:=95.58666666666666, Evaluation Acc:=95.19\n",
            "Epoch=1 - Train Loss:=770.0098876953125, Evaluation Loss:=0.37786543369293213, Train Acc:=96.74000000000001, Evaluation Acc:=96.21\n",
            "Epoch=2 - Train Loss:=567.1878662109375, Evaluation Loss:=0.29075467586517334, Train Acc:=97.53666666666668, Evaluation Acc:=96.75\n",
            "Epoch=3 - Train Loss:=439.981689453125, Evaluation Loss:=0.28973594307899475, Train Acc:=97.99666666666667, Evaluation Acc:=97.09\n",
            "Epoch=4 - Train Loss:=382.796142578125, Evaluation Loss:=0.26709219813346863, Train Acc:=98.34666666666666, Evaluation Acc:=97.45\n",
            "Epoch=5 - Train Loss:=313.35443115234375, Evaluation Loss:=0.24370412528514862, Train Acc:=98.74833333333333, Evaluation Acc:=97.63\n",
            "Epoch=6 - Train Loss:=261.05511474609375, Evaluation Loss:=0.21759657561779022, Train Acc:=99.05166666666668, Evaluation Acc:=97.86\n",
            "Epoch=7 - Train Loss:=223.02842712402344, Evaluation Loss:=0.28801780939102173, Train Acc:=98.875, Evaluation Acc:=97.53\n",
            "Epoch=8 - Train Loss:=194.5147247314453, Evaluation Loss:=0.25039035081863403, Train Acc:=99.335, Evaluation Acc:=97.87\n",
            "Epoch=9 - Train Loss:=186.34693908691406, Evaluation Loss:=0.24166610836982727, Train Acc:=99.40666666666667, Evaluation Acc:=98.11\n",
            "Epoch=0 - Train Loss:=1271.3399658203125, Evaluation Loss:=0.40401750802993774, Train Acc:=95.72666666666667, Evaluation Acc:=95.37\n",
            "Epoch=1 - Train Loss:=731.8214111328125, Evaluation Loss:=0.2823767364025116, Train Acc:=97.34666666666666, Evaluation Acc:=96.97\n",
            "Epoch=2 - Train Loss:=552.333740234375, Evaluation Loss:=0.32418832182884216, Train Acc:=97.52, Evaluation Acc:=96.73\n",
            "Epoch=3 - Train Loss:=454.3326721191406, Evaluation Loss:=0.24694621562957764, Train Acc:=98.31166666666667, Evaluation Acc:=97.36\n",
            "Epoch=4 - Train Loss:=352.6702880859375, Evaluation Loss:=0.22004470229148865, Train Acc:=98.71333333333332, Evaluation Acc:=97.44\n",
            "Epoch=5 - Train Loss:=313.5290222167969, Evaluation Loss:=0.21247917413711548, Train Acc:=98.985, Evaluation Acc:=97.76\n",
            "Epoch=6 - Train Loss:=268.79693603515625, Evaluation Loss:=0.22305817902088165, Train Acc:=98.97666666666667, Evaluation Acc:=97.78\n",
            "Epoch=7 - Train Loss:=218.95016479492188, Evaluation Loss:=0.24689042568206787, Train Acc:=98.97166666666666, Evaluation Acc:=97.67\n",
            "Epoch=8 - Train Loss:=207.64952087402344, Evaluation Loss:=0.22504858672618866, Train Acc:=99.26666666666667, Evaluation Acc:=98.03\n",
            "Epoch=9 - Train Loss:=172.5343475341797, Evaluation Loss:=0.2217772901058197, Train Acc:=99.46000000000001, Evaluation Acc:=97.99\n",
            "Epoch=0 - Train Loss:=1289.2886962890625, Evaluation Loss:=0.3100236654281616, Train Acc:=95.83, Evaluation Acc:=95.75\n",
            "Epoch=1 - Train Loss:=720.1030883789062, Evaluation Loss:=0.26294243335723877, Train Acc:=97.33500000000001, Evaluation Acc:=96.85000000000001\n",
            "Epoch=2 - Train Loss:=534.3319091796875, Evaluation Loss:=0.2560931146144867, Train Acc:=97.89833333333333, Evaluation Acc:=97.27\n",
            "Epoch=3 - Train Loss:=434.635009765625, Evaluation Loss:=0.24541059136390686, Train Acc:=98.37833333333333, Evaluation Acc:=97.57000000000001\n",
            "Epoch=4 - Train Loss:=390.0796813964844, Evaluation Loss:=0.24993449449539185, Train Acc:=98.48, Evaluation Acc:=97.39999999999999\n",
            "Epoch=5 - Train Loss:=312.1585693359375, Evaluation Loss:=0.21325409412384033, Train Acc:=98.80166666666666, Evaluation Acc:=97.74000000000001\n",
            "Epoch=6 - Train Loss:=261.8444519042969, Evaluation Loss:=0.2549158036708832, Train Acc:=98.6, Evaluation Acc:=97.67\n",
            "Epoch=7 - Train Loss:=222.6436767578125, Evaluation Loss:=0.22775812447071075, Train Acc:=99.165, Evaluation Acc:=97.92\n",
            "Epoch=8 - Train Loss:=196.52052307128906, Evaluation Loss:=0.21560300886631012, Train Acc:=99.27333333333334, Evaluation Acc:=98.08\n",
            "Epoch=9 - Train Loss:=172.3119659423828, Evaluation Loss:=0.22927117347717285, Train Acc:=99.41499999999999, Evaluation Acc:=98.04\n",
            "Epoch=0 - Train Loss:=1347.8759765625, Evaluation Loss:=0.33846181631088257, Train Acc:=95.8, Evaluation Acc:=95.45\n",
            "Epoch=1 - Train Loss:=742.8983154296875, Evaluation Loss:=0.28882867097854614, Train Acc:=97.26166666666667, Evaluation Acc:=96.89\n",
            "Epoch=2 - Train Loss:=539.9049682617188, Evaluation Loss:=0.26819905638694763, Train Acc:=97.81, Evaluation Acc:=97.05\n",
            "Epoch=3 - Train Loss:=432.8487243652344, Evaluation Loss:=0.23404863476753235, Train Acc:=98.37, Evaluation Acc:=97.45\n",
            "Epoch=4 - Train Loss:=364.4063415527344, Evaluation Loss:=0.23046526312828064, Train Acc:=98.66333333333334, Evaluation Acc:=97.49\n",
            "Epoch=5 - Train Loss:=308.3846130371094, Evaluation Loss:=0.2266174852848053, Train Acc:=98.94666666666667, Evaluation Acc:=97.88\n",
            "Epoch=6 - Train Loss:=279.7801818847656, Evaluation Loss:=0.2422713339328766, Train Acc:=98.81666666666666, Evaluation Acc:=97.74000000000001\n",
            "Epoch=7 - Train Loss:=235.87445068359375, Evaluation Loss:=0.24106653034687042, Train Acc:=99.13499999999999, Evaluation Acc:=97.86\n",
            "Epoch=8 - Train Loss:=204.3227081298828, Evaluation Loss:=0.2762874960899353, Train Acc:=99.06833333333334, Evaluation Acc:=97.44\n",
            "Epoch=9 - Train Loss:=171.63851928710938, Evaluation Loss:=0.2609000504016876, Train Acc:=99.24666666666667, Evaluation Acc:=97.83\n",
            "Epoch=0 - Train Loss:=1321.7017822265625, Evaluation Loss:=0.34748512506484985, Train Acc:=95.85166666666667, Evaluation Acc:=95.83\n",
            "Epoch=1 - Train Loss:=759.2550048828125, Evaluation Loss:=0.30974850058555603, Train Acc:=96.91666666666666, Evaluation Acc:=96.55\n",
            "Epoch=2 - Train Loss:=561.52783203125, Evaluation Loss:=0.24296928942203522, Train Acc:=97.94, Evaluation Acc:=97.28999999999999\n",
            "Epoch=3 - Train Loss:=434.083251953125, Evaluation Loss:=0.2573336362838745, Train Acc:=98.26833333333333, Evaluation Acc:=97.26\n",
            "Epoch=4 - Train Loss:=372.28570556640625, Evaluation Loss:=0.25139182806015015, Train Acc:=98.60333333333332, Evaluation Acc:=97.56\n",
            "Epoch=5 - Train Loss:=312.5398864746094, Evaluation Loss:=0.23609887063503265, Train Acc:=98.72166666666666, Evaluation Acc:=97.72999999999999\n",
            "Epoch=6 - Train Loss:=257.4345703125, Evaluation Loss:=0.22633178532123566, Train Acc:=99.10166666666666, Evaluation Acc:=97.81\n",
            "Epoch=7 - Train Loss:=233.4208526611328, Evaluation Loss:=0.2099766880273819, Train Acc:=99.29, Evaluation Acc:=98.13\n",
            "Epoch=8 - Train Loss:=196.58839416503906, Evaluation Loss:=0.228744238615036, Train Acc:=99.33333333333333, Evaluation Acc:=97.89\n",
            "Epoch=9 - Train Loss:=172.4440155029297, Evaluation Loss:=0.24840669333934784, Train Acc:=99.31, Evaluation Acc:=97.81\n",
            "Epoch=0 - Train Loss:=1318.5863037109375, Evaluation Loss:=0.5448967814445496, Train Acc:=94.055, Evaluation Acc:=93.77\n",
            "Epoch=1 - Train Loss:=742.3622436523438, Evaluation Loss:=0.32737967371940613, Train Acc:=97.03333333333333, Evaluation Acc:=96.59\n",
            "Epoch=2 - Train Loss:=579.1482543945312, Evaluation Loss:=0.2659861743450165, Train Acc:=97.75, Evaluation Acc:=97.15\n",
            "Epoch=3 - Train Loss:=468.00482177734375, Evaluation Loss:=0.24567507207393646, Train Acc:=98.28333333333333, Evaluation Acc:=97.35000000000001\n",
            "Epoch=4 - Train Loss:=377.7891845703125, Evaluation Loss:=0.20037677884101868, Train Acc:=98.56166666666667, Evaluation Acc:=97.86\n",
            "Epoch=5 - Train Loss:=326.57867431640625, Evaluation Loss:=0.2329046130180359, Train Acc:=98.66833333333334, Evaluation Acc:=97.57000000000001\n",
            "Epoch=6 - Train Loss:=273.63360595703125, Evaluation Loss:=0.2220839411020279, Train Acc:=98.97166666666666, Evaluation Acc:=97.81\n",
            "Epoch=7 - Train Loss:=236.00599670410156, Evaluation Loss:=0.21484573185443878, Train Acc:=98.98333333333333, Evaluation Acc:=97.92999999999999\n",
            "Epoch=8 - Train Loss:=222.02212524414062, Evaluation Loss:=0.20861747860908508, Train Acc:=99.34333333333333, Evaluation Acc:=98.06\n",
            "Epoch=9 - Train Loss:=186.5872039794922, Evaluation Loss:=0.2117815911769867, Train Acc:=99.41833333333334, Evaluation Acc:=98.06\n",
            "Epoch=0 - Train Loss:=1339.8673095703125, Evaluation Loss:=0.4453568458557129, Train Acc:=95.825, Evaluation Acc:=95.33\n",
            "Epoch=1 - Train Loss:=743.5113525390625, Evaluation Loss:=0.2982328236103058, Train Acc:=97.14500000000001, Evaluation Acc:=96.58\n",
            "Epoch=2 - Train Loss:=562.6759033203125, Evaluation Loss:=0.3094697892665863, Train Acc:=97.75333333333333, Evaluation Acc:=96.97\n",
            "Epoch=3 - Train Loss:=449.618408203125, Evaluation Loss:=0.2910926938056946, Train Acc:=98.235, Evaluation Acc:=97.14\n",
            "Epoch=4 - Train Loss:=381.51507568359375, Evaluation Loss:=0.26163363456726074, Train Acc:=98.38666666666667, Evaluation Acc:=97.35000000000001\n",
            "Epoch=5 - Train Loss:=313.9661865234375, Evaluation Loss:=0.23120449483394623, Train Acc:=98.785, Evaluation Acc:=97.82\n",
            "Epoch=6 - Train Loss:=283.8866271972656, Evaluation Loss:=0.2275046855211258, Train Acc:=98.96000000000001, Evaluation Acc:=97.92\n",
            "Epoch=7 - Train Loss:=257.501953125, Evaluation Loss:=0.21022692322731018, Train Acc:=99.015, Evaluation Acc:=97.91\n",
            "Epoch=8 - Train Loss:=214.4066925048828, Evaluation Loss:=0.227430522441864, Train Acc:=99.29, Evaluation Acc:=98.1\n",
            "Epoch=9 - Train Loss:=193.7477569580078, Evaluation Loss:=0.24897432327270508, Train Acc:=99.20333333333333, Evaluation Acc:=98.02\n",
            "Epoch=0 - Train Loss:=1299.5126953125, Evaluation Loss:=0.43941381573677063, Train Acc:=95.19833333333332, Evaluation Acc:=94.87\n",
            "Epoch=1 - Train Loss:=721.19091796875, Evaluation Loss:=0.28222593665122986, Train Acc:=97.20666666666666, Evaluation Acc:=96.74000000000001\n",
            "Epoch=2 - Train Loss:=522.3440551757812, Evaluation Loss:=0.2792911231517792, Train Acc:=97.67333333333333, Evaluation Acc:=96.93\n",
            "Epoch=3 - Train Loss:=427.2074890136719, Evaluation Loss:=0.2489461898803711, Train Acc:=98.395, Evaluation Acc:=97.34\n",
            "Epoch=4 - Train Loss:=356.7872619628906, Evaluation Loss:=0.22388893365859985, Train Acc:=98.65, Evaluation Acc:=97.81\n",
            "Epoch=5 - Train Loss:=303.20050048828125, Evaluation Loss:=0.2520114779472351, Train Acc:=98.68, Evaluation Acc:=97.53\n",
            "Epoch=6 - Train Loss:=273.85931396484375, Evaluation Loss:=0.23972877860069275, Train Acc:=98.93166666666666, Evaluation Acc:=97.87\n",
            "Epoch=7 - Train Loss:=238.50340270996094, Evaluation Loss:=0.2545448839664459, Train Acc:=99.045, Evaluation Acc:=97.94\n",
            "Epoch=8 - Train Loss:=199.29119873046875, Evaluation Loss:=0.21797597408294678, Train Acc:=99.11666666666666, Evaluation Acc:=97.89999999999999\n",
            "Epoch=9 - Train Loss:=185.6762237548828, Evaluation Loss:=0.27862879633903503, Train Acc:=99.20333333333333, Evaluation Acc:=97.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "ASlbPByf3gDn",
        "outputId": "720e7fee-6a83-4cb5-d3f6-4de1127a5fb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 97.95599999999999, and the variance is 0.012363999999999787\n",
            "the average running time for one epoch is: 36.38002517223358\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fec3c74c610>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fec3c7516d0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c751c10>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fec3c759550>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fec3c759050>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fec3c74cc10>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c751190>]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model5: without regularization + custom optimizer"
      ],
      "metadata": {
        "id": "6KgbPJg8w7Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "hidden_size = 512\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "optimizer=\"custom\"\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i + 5)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"mnist\")\n",
        "\n",
        "    model5 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer=optimizer)\n",
        "    train_losses, eval_losses, train_accuracy, eval_accuracy, _epoch_times = train(model5, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs)\n",
        "    acc = test(model5, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "6acXPNLZxDXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdee8cb-cd58-46a5-af02-a3a53e86e115"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=493.386962890625, Evaluation Loss:=0.12190697342157364, Train Acc:=96.85333333333334, Evaluation Acc:=96.22\n",
            "Epoch=1 - Train Loss:=189.908203125, Evaluation Loss:=0.10220446437597275, Train Acc:=98.015, Evaluation Acc:=96.99\n",
            "Epoch=2 - Train Loss:=139.19781494140625, Evaluation Loss:=0.11161176860332489, Train Acc:=98.07666666666667, Evaluation Acc:=96.73\n",
            "Epoch=3 - Train Loss:=109.76973724365234, Evaluation Loss:=0.0872621238231659, Train Acc:=99.07666666666667, Evaluation Acc:=97.57000000000001\n",
            "Epoch=4 - Train Loss:=93.10645294189453, Evaluation Loss:=0.1067657470703125, Train Acc:=98.75333333333333, Evaluation Acc:=97.18\n",
            "Epoch=5 - Train Loss:=76.36380767822266, Evaluation Loss:=0.11077536642551422, Train Acc:=98.75333333333333, Evaluation Acc:=97.36\n",
            "Epoch=6 - Train Loss:=67.23216247558594, Evaluation Loss:=0.10927281528711319, Train Acc:=99.015, Evaluation Acc:=97.28999999999999\n",
            "Epoch=7 - Train Loss:=65.19229888916016, Evaluation Loss:=0.12171436846256256, Train Acc:=99.14, Evaluation Acc:=97.33000000000001\n",
            "Epoch=8 - Train Loss:=56.697608947753906, Evaluation Loss:=0.10799045115709305, Train Acc:=99.22999999999999, Evaluation Acc:=97.72\n",
            "Epoch=9 - Train Loss:=48.215476989746094, Evaluation Loss:=0.12215903401374817, Train Acc:=99.35833333333333, Evaluation Acc:=97.72\n",
            "Epoch=0 - Train Loss:=455.6591491699219, Evaluation Loss:=0.11712735891342163, Train Acc:=97.365, Evaluation Acc:=96.38\n",
            "Epoch=1 - Train Loss:=189.5764617919922, Evaluation Loss:=0.09636230766773224, Train Acc:=98.15666666666667, Evaluation Acc:=97.14\n",
            "Epoch=2 - Train Loss:=134.70106506347656, Evaluation Loss:=0.10279012471437454, Train Acc:=98.385, Evaluation Acc:=97.05\n",
            "Epoch=3 - Train Loss:=108.69768524169922, Evaluation Loss:=0.10082746297121048, Train Acc:=98.31333333333333, Evaluation Acc:=96.99\n",
            "Epoch=4 - Train Loss:=88.40077209472656, Evaluation Loss:=0.09323030710220337, Train Acc:=98.925, Evaluation Acc:=97.38\n",
            "Epoch=5 - Train Loss:=80.39266204833984, Evaluation Loss:=0.09836046397686005, Train Acc:=99.04666666666667, Evaluation Acc:=97.75\n",
            "Epoch=6 - Train Loss:=64.42430114746094, Evaluation Loss:=0.11694633960723877, Train Acc:=99.12166666666667, Evaluation Acc:=97.59\n",
            "Epoch=7 - Train Loss:=62.05155944824219, Evaluation Loss:=0.10574338585138321, Train Acc:=99.32333333333332, Evaluation Acc:=97.68\n",
            "Epoch=8 - Train Loss:=56.382328033447266, Evaluation Loss:=0.1262558400630951, Train Acc:=99.2, Evaluation Acc:=97.69\n",
            "Epoch=9 - Train Loss:=57.23247146606445, Evaluation Loss:=0.10916976630687714, Train Acc:=99.295, Evaluation Acc:=97.76\n",
            "Epoch=0 - Train Loss:=470.1410217285156, Evaluation Loss:=0.11584744602441788, Train Acc:=97.09666666666666, Evaluation Acc:=96.35000000000001\n",
            "Epoch=1 - Train Loss:=186.69203186035156, Evaluation Loss:=0.09788534790277481, Train Acc:=98.34833333333334, Evaluation Acc:=97.22\n",
            "Epoch=2 - Train Loss:=134.5559844970703, Evaluation Loss:=0.10789201408624649, Train Acc:=98.27, Evaluation Acc:=96.94\n",
            "Epoch=3 - Train Loss:=110.31407928466797, Evaluation Loss:=0.0947418138384819, Train Acc:=98.68166666666667, Evaluation Acc:=97.24000000000001\n",
            "Epoch=4 - Train Loss:=87.5342788696289, Evaluation Loss:=0.10117916762828827, Train Acc:=98.92999999999999, Evaluation Acc:=97.31\n",
            "Epoch=5 - Train Loss:=74.55900573730469, Evaluation Loss:=0.10362069308757782, Train Acc:=99.19166666666666, Evaluation Acc:=97.7\n",
            "Epoch=6 - Train Loss:=66.43973541259766, Evaluation Loss:=0.10815998166799545, Train Acc:=99.06, Evaluation Acc:=97.57000000000001\n",
            "Epoch=7 - Train Loss:=58.31547546386719, Evaluation Loss:=0.1053655669093132, Train Acc:=99.41166666666666, Evaluation Acc:=97.83\n",
            "Epoch=8 - Train Loss:=53.869327545166016, Evaluation Loss:=0.09783707559108734, Train Acc:=99.42833333333333, Evaluation Acc:=98.02\n",
            "Epoch=9 - Train Loss:=50.82711410522461, Evaluation Loss:=0.12534520030021667, Train Acc:=99.41666666666666, Evaluation Acc:=97.65\n",
            "Epoch=0 - Train Loss:=474.5000305175781, Evaluation Loss:=0.14310160279273987, Train Acc:=96.525, Evaluation Acc:=95.52000000000001\n",
            "Epoch=1 - Train Loss:=191.7396697998047, Evaluation Loss:=0.1030866801738739, Train Acc:=98.14166666666667, Evaluation Acc:=96.89999999999999\n",
            "Epoch=2 - Train Loss:=141.0599822998047, Evaluation Loss:=0.09767260402441025, Train Acc:=98.43166666666666, Evaluation Acc:=97.11999999999999\n",
            "Epoch=3 - Train Loss:=112.0926742553711, Evaluation Loss:=0.09273787587881088, Train Acc:=98.76333333333334, Evaluation Acc:=97.44\n",
            "Epoch=4 - Train Loss:=92.22464752197266, Evaluation Loss:=0.09763399511575699, Train Acc:=98.95666666666668, Evaluation Acc:=97.6\n",
            "Epoch=5 - Train Loss:=77.61335754394531, Evaluation Loss:=0.09136447310447693, Train Acc:=99.19333333333333, Evaluation Acc:=97.66\n",
            "Epoch=6 - Train Loss:=70.6734390258789, Evaluation Loss:=0.09996059536933899, Train Acc:=99.08166666666666, Evaluation Acc:=97.59\n",
            "Epoch=7 - Train Loss:=62.740257263183594, Evaluation Loss:=0.11514046788215637, Train Acc:=99.40166666666667, Evaluation Acc:=97.6\n",
            "Epoch=8 - Train Loss:=52.739173889160156, Evaluation Loss:=0.13106974959373474, Train Acc:=98.72333333333333, Evaluation Acc:=97.14\n",
            "Epoch=9 - Train Loss:=55.99689865112305, Evaluation Loss:=0.1114255040884018, Train Acc:=99.30833333333334, Evaluation Acc:=97.7\n",
            "Epoch=0 - Train Loss:=489.1276550292969, Evaluation Loss:=0.11488393694162369, Train Acc:=97.18166666666667, Evaluation Acc:=96.45\n",
            "Epoch=1 - Train Loss:=192.21778869628906, Evaluation Loss:=0.1099381223320961, Train Acc:=97.72999999999999, Evaluation Acc:=96.7\n",
            "Epoch=2 - Train Loss:=142.10641479492188, Evaluation Loss:=0.09088947623968124, Train Acc:=98.59, Evaluation Acc:=97.23\n",
            "Epoch=3 - Train Loss:=110.87390899658203, Evaluation Loss:=0.08735816925764084, Train Acc:=98.94500000000001, Evaluation Acc:=97.67\n",
            "Epoch=4 - Train Loss:=89.84672546386719, Evaluation Loss:=0.11587744206190109, Train Acc:=98.49, Evaluation Acc:=96.84\n",
            "Epoch=5 - Train Loss:=86.05467224121094, Evaluation Loss:=0.08145930618047714, Train Acc:=99.30499999999999, Evaluation Acc:=97.92999999999999\n",
            "Epoch=6 - Train Loss:=67.09185028076172, Evaluation Loss:=0.1062440425157547, Train Acc:=99.18, Evaluation Acc:=97.68\n",
            "Epoch=7 - Train Loss:=57.206459045410156, Evaluation Loss:=0.11055158823728561, Train Acc:=99.28666666666666, Evaluation Acc:=97.45\n",
            "Epoch=8 - Train Loss:=53.794044494628906, Evaluation Loss:=0.10760530084371567, Train Acc:=99.335, Evaluation Acc:=97.74000000000001\n",
            "Epoch=9 - Train Loss:=53.80939483642578, Evaluation Loss:=0.10636523365974426, Train Acc:=99.52166666666666, Evaluation Acc:=97.95\n",
            "Epoch=0 - Train Loss:=484.20806884765625, Evaluation Loss:=0.13018833100795746, Train Acc:=97.02166666666666, Evaluation Acc:=96.04\n",
            "Epoch=1 - Train Loss:=190.0009307861328, Evaluation Loss:=0.1163865327835083, Train Acc:=97.75333333333333, Evaluation Acc:=96.56\n",
            "Epoch=2 - Train Loss:=142.85621643066406, Evaluation Loss:=0.11793898791074753, Train Acc:=98.13166666666666, Evaluation Acc:=96.7\n",
            "Epoch=3 - Train Loss:=110.97662353515625, Evaluation Loss:=0.11041613668203354, Train Acc:=98.35333333333334, Evaluation Acc:=96.92\n",
            "Epoch=4 - Train Loss:=90.13920593261719, Evaluation Loss:=0.11065220087766647, Train Acc:=98.47666666666667, Evaluation Acc:=96.94\n",
            "Epoch=5 - Train Loss:=76.36125946044922, Evaluation Loss:=0.0878843367099762, Train Acc:=99.13833333333332, Evaluation Acc:=97.67\n",
            "Epoch=6 - Train Loss:=73.22970581054688, Evaluation Loss:=0.09596008062362671, Train Acc:=99.11166666666666, Evaluation Acc:=97.66\n",
            "Epoch=7 - Train Loss:=57.555904388427734, Evaluation Loss:=0.10701938718557358, Train Acc:=99.19333333333333, Evaluation Acc:=97.78\n",
            "Epoch=8 - Train Loss:=54.95309829711914, Evaluation Loss:=0.11757158488035202, Train Acc:=99.04666666666667, Evaluation Acc:=97.6\n",
            "Epoch=9 - Train Loss:=54.713687896728516, Evaluation Loss:=0.1428600251674652, Train Acc:=98.49333333333334, Evaluation Acc:=96.96000000000001\n",
            "Epoch=0 - Train Loss:=462.367919921875, Evaluation Loss:=0.12809397280216217, Train Acc:=96.865, Evaluation Acc:=96.07\n",
            "Epoch=1 - Train Loss:=195.34640502929688, Evaluation Loss:=0.09805317968130112, Train Acc:=98.21666666666667, Evaluation Acc:=97.0\n",
            "Epoch=2 - Train Loss:=136.6630859375, Evaluation Loss:=0.09647476673126221, Train Acc:=98.43333333333332, Evaluation Acc:=97.21\n",
            "Epoch=3 - Train Loss:=106.66838836669922, Evaluation Loss:=0.09186266362667084, Train Acc:=98.81666666666666, Evaluation Acc:=97.54\n",
            "Epoch=4 - Train Loss:=90.76041412353516, Evaluation Loss:=0.10451966524124146, Train Acc:=98.25166666666667, Evaluation Acc:=96.99\n",
            "Epoch=5 - Train Loss:=76.77911376953125, Evaluation Loss:=0.09282924979925156, Train Acc:=98.97500000000001, Evaluation Acc:=97.41\n",
            "Epoch=6 - Train Loss:=66.93160247802734, Evaluation Loss:=0.11358873546123505, Train Acc:=99.03999999999999, Evaluation Acc:=97.36\n",
            "Epoch=7 - Train Loss:=66.50202178955078, Evaluation Loss:=0.09910380840301514, Train Acc:=99.425, Evaluation Acc:=97.75\n",
            "Epoch=8 - Train Loss:=50.12537384033203, Evaluation Loss:=0.11221980303525925, Train Acc:=99.30333333333333, Evaluation Acc:=97.71\n",
            "Epoch=9 - Train Loss:=51.10789489746094, Evaluation Loss:=0.10799067467451096, Train Acc:=99.37666666666667, Evaluation Acc:=97.67\n",
            "Epoch=0 - Train Loss:=498.37274169921875, Evaluation Loss:=0.14629222452640533, Train Acc:=96.51833333333333, Evaluation Acc:=95.54\n",
            "Epoch=1 - Train Loss:=190.67164611816406, Evaluation Loss:=0.09589216113090515, Train Acc:=98.295, Evaluation Acc:=97.00999999999999\n",
            "Epoch=2 - Train Loss:=139.22267150878906, Evaluation Loss:=0.10998781770467758, Train Acc:=98.01166666666667, Evaluation Acc:=96.82\n",
            "Epoch=3 - Train Loss:=110.29644012451172, Evaluation Loss:=0.09548337012529373, Train Acc:=98.64833333333334, Evaluation Acc:=97.18\n",
            "Epoch=4 - Train Loss:=92.75019836425781, Evaluation Loss:=0.09513410925865173, Train Acc:=98.88, Evaluation Acc:=97.36\n",
            "Epoch=5 - Train Loss:=76.81951141357422, Evaluation Loss:=0.08934953063726425, Train Acc:=99.14333333333335, Evaluation Acc:=97.48\n",
            "Epoch=6 - Train Loss:=71.0168228149414, Evaluation Loss:=0.1056489646434784, Train Acc:=99.15333333333334, Evaluation Acc:=97.61\n",
            "Epoch=7 - Train Loss:=62.69099044799805, Evaluation Loss:=0.0965859666466713, Train Acc:=99.41666666666666, Evaluation Acc:=97.74000000000001\n",
            "Epoch=8 - Train Loss:=57.779964447021484, Evaluation Loss:=0.12134367972612381, Train Acc:=99.155, Evaluation Acc:=97.45\n",
            "Epoch=9 - Train Loss:=51.47493362426758, Evaluation Loss:=0.10826064646244049, Train Acc:=99.41833333333334, Evaluation Acc:=97.84\n",
            "Epoch=0 - Train Loss:=478.34320068359375, Evaluation Loss:=0.12342767417430878, Train Acc:=96.845, Evaluation Acc:=95.99\n",
            "Epoch=1 - Train Loss:=194.66441345214844, Evaluation Loss:=0.09699761867523193, Train Acc:=98.21333333333332, Evaluation Acc:=97.09\n",
            "Epoch=2 - Train Loss:=139.3642578125, Evaluation Loss:=0.10643225163221359, Train Acc:=98.25, Evaluation Acc:=97.0\n",
            "Epoch=3 - Train Loss:=113.30123138427734, Evaluation Loss:=0.09421350061893463, Train Acc:=98.63833333333332, Evaluation Acc:=97.32\n",
            "Epoch=4 - Train Loss:=89.6077880859375, Evaluation Loss:=0.1046467199921608, Train Acc:=98.90166666666667, Evaluation Acc:=97.53\n",
            "Epoch=5 - Train Loss:=81.9554672241211, Evaluation Loss:=0.0933709368109703, Train Acc:=99.06, Evaluation Acc:=97.61999999999999\n",
            "Epoch=6 - Train Loss:=67.9649429321289, Evaluation Loss:=0.09366638958454132, Train Acc:=99.11833333333333, Evaluation Acc:=97.72\n",
            "Epoch=7 - Train Loss:=61.591651916503906, Evaluation Loss:=0.11006408929824829, Train Acc:=99.08333333333333, Evaluation Acc:=97.68\n",
            "Epoch=8 - Train Loss:=64.00603485107422, Evaluation Loss:=0.09381331503391266, Train Acc:=99.54833333333333, Evaluation Acc:=97.76\n",
            "Epoch=9 - Train Loss:=50.45711135864258, Evaluation Loss:=0.10656113177537918, Train Acc:=99.47, Evaluation Acc:=97.66\n",
            "Epoch=0 - Train Loss:=475.75457763671875, Evaluation Loss:=0.1204807385802269, Train Acc:=97.11999999999999, Evaluation Acc:=96.23\n",
            "Epoch=1 - Train Loss:=189.43716430664062, Evaluation Loss:=0.11653824150562286, Train Acc:=97.85666666666667, Evaluation Acc:=96.48\n",
            "Epoch=2 - Train Loss:=135.87069702148438, Evaluation Loss:=0.10291802883148193, Train Acc:=98.56333333333333, Evaluation Acc:=97.16\n",
            "Epoch=3 - Train Loss:=104.11433410644531, Evaluation Loss:=0.102665975689888, Train Acc:=98.69666666666667, Evaluation Acc:=97.37\n",
            "Epoch=4 - Train Loss:=88.22354125976562, Evaluation Loss:=0.10817626863718033, Train Acc:=98.52499999999999, Evaluation Acc:=97.19\n",
            "Epoch=5 - Train Loss:=76.42841339111328, Evaluation Loss:=0.09890145063400269, Train Acc:=98.95166666666667, Evaluation Acc:=97.31\n",
            "Epoch=6 - Train Loss:=69.8633041381836, Evaluation Loss:=0.07759562134742737, Train Acc:=99.38166666666667, Evaluation Acc:=97.89\n",
            "Epoch=7 - Train Loss:=56.15839385986328, Evaluation Loss:=0.10106070339679718, Train Acc:=99.22666666666666, Evaluation Acc:=97.83\n",
            "Epoch=8 - Train Loss:=54.93728256225586, Evaluation Loss:=0.12654754519462585, Train Acc:=99.18, Evaluation Acc:=97.19\n",
            "Epoch=9 - Train Loss:=52.1124153137207, Evaluation Loss:=0.10894817858934402, Train Acc:=99.13333333333333, Evaluation Acc:=97.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "kWBMtC4rPh1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "33610421-3dab-423a-a9a8-15a60d092112"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 97.673, and the variance is 0.06458099999999901\n",
            "the average running time for one epoch is: 36.8445925283432\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fec3c5aac50>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fec3c5b3d10>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c5b9290>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fec3c5b9d50>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fec3c5b9810>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fec3c5b3290>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c5b37d0>]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANUklEQVR4nO3dX2id933H8fdndkrmbuliR7lwEs9hbEVFMLoI062RIa0HxRsrhJHY0EGGiW8yzxmD0eGLdhe5CCxjI4wNE3e9WKN6xL1oQvA8NjfDMEzlVDDZ8pLSua2bLVGZk46lwX/23YWOO9U9x+eRpfjYP79fcFD0PM9P53ty8fbD85wjpaqQJLXrp0Y9gCTp/WXoJalxhl6SGmfoJalxhl6SGrd21AP0c9ddd9XmzZtHPYYk3TROnDjx/aoa67fvhgz95s2bmZmZGfUYknTTSPLtQfu8dCNJjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktS4G/IDU9L1kOS6PZd/90GjZOh1y7qW+CYx2rrpeOlGkhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhrXKfRJ9iaZS3IyyZO9bQeTzPYeZ5LMDlj7B711c0mmk9y+mi9AknR1a4cdkGQCeBzYApwHDid5qaoeXXLMM8A7fdbeA/w+8JGq+mGSvwN2AF9cnfElScN0OaMfB45X1btVdRF4BXj48s4kAR4BpgesXwv8dJK1wDrgjZWNLPW3fv16kryvD+B9f47169eP+P+kWjP0jB6YA55KsgH4IbAdmFmyfwp4s6pev3JhVX0vyZ8C3+mtPVJVR/o9SZLdwG6ATZs2LetFSADnzp2jqkY9xopd/gdFWi1Dz+irah54GjgCHAZmgUtLDtnJgLP5JHcCnwbuBzYCH0zymQHPs7+qJqtqcmxsbFkvQpI0WKebsVV1oKoeqKqtwDngNYDe5ZiHgYMDlm4D/r2qFqrqAvAV4NdWPrYkqauu77q5u/d1E4thf763axtwuqrODlj6HeBjSdb1ruV/Ephf2ciSpOXo+j76Q0lOAS8CT1TV273tO7jisk2SjUleBqiq48ALwKvAv/aeb/9qDC5J6iY34s2rycnJmpmZGX6gtESSZm7GtvA6dH0lOVFVk/32+clYSWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxnX5C1PSTaE+dwd8/kOjHmPF6nN3jHoENcbQqxn5kx808Vsfk1CfH/UUaomXbiSpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhrnB6bUlCSjHmHF7rzzzlGPoMYYejXjenwqNkkTn77VrcXQ65Z1rWf/17LOfxw0SoZetyzjq1uFN2MlqXGGXpIaZ+glqXGGXpIaZ+glqXGdQp9kb5K5JCeTPNnbdjDJbO9xJsnsgLU/l+SFJKeTzCf51dV8AZKkqxv69sokE8DjwBbgPHA4yUtV9eiSY54B3hnwI/4COFxVv53kA8C6lY8tSeqqyxn9OHC8qt6tqovAK8DDl3dm8dMjjwDTVy5M8iFgK3AAoKrOV9XbqzG4JKmbLqGfA6aSbEiyDtgO3Ldk/xTwZlW93mft/cAC8DdJvpHkuSQf7PckSXYnmUkys7CwsMyXIUkaZGjoq2oeeBo4AhwGZoFLSw7ZSZ+z+Z61wK8Af1VVHwX+B/jsgOfZX1WTVTU5NjbW/RVIkq6q083YqjpQVQ9U1VbgHPAaQJK1LF7GOThg6VngbFUd733/AovhlyRdJ13fdXN37+smFsP+fG/XNuB0VZ3tt66q/hP4bpIP9zZ9Eji1ooklScvS9ZeaHUqyAbgAPLHkhuoOrrhsk2Qj8FxVbe9t2gN8qfeOm28Bv7vysSVJXXUKfVVNDdj+WJ9tb7B4w/by97PA5DXOJ0laIT8ZK0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mN6xT6JHuTzCU5meTJ3raDSWZ7jzNJZq+yfk2SbyR5abUGlyR1s3bYAUkmgMeBLcB54HCSl6rq0SXHPAO8c5UfsxeYB+5Y2biSpOXqckY/Dhyvqner6iLwCvDw5Z1JAjwCTPdbnORe4DeA51Y+riRpubqEfg6YSrIhyTpgO3Dfkv1TwJtV9fqA9X8O/BHwv1d7kiS7k8wkmVlYWOgwliSpi6Ghr6p54GngCHAYmAUuLTlkJ4PP5n8TeKuqTnR4nv1VNVlVk2NjY11mlyR10OlmbFUdqKoHqmorcA54DSDJWhYv4xwcsPTjwG8lOQN8GfhEkr9d8dSSpM66vuvm7t7XTSyG/fnerm3A6ao6229dVf1xVd1bVZuBHcA/VdVnVjy1JKmzru+jP5TkFPAi8ERVvd3bvoMrLtsk2Zjk5VWcUZK0AkPfXglQVVMDtj/WZ9sbLN6wvXL714CvLWs6SdKK+clYSWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWpcp9An2ZtkLsnJJE/2th1MMtt7nEky22fdfUmOJjnVW7t3tV+AJOnq1g47IMkE8DiwBTgPHE7yUlU9uuSYZ4B3+iy/CPxhVb2a5GeBE0n+oapOrc74kqRhupzRjwPHq+rdqroIvAI8fHlnkgCPANNXLqyq/6iqV3v//d/APHDPagwuSeqmS+jngKkkG5KsA7YD9y3ZPwW8WVWvX+2HJNkMfBQ4PmD/7iQzSWYWFha6zC5J6mBo6KtqHngaOAIcBmaBS0sO2Umfs/mlkvwMcAh4sqp+MOB59lfVZFVNjo2NdRxfkjRMp5uxVXWgqh6oqq3AOeA1gCRrWbyMc3DQ2iS3sRj5L1XVV1Y+siRpOYbejAVIcndVvZVkE4th/1hv1zbgdFWdHbAuwAFgvqr+bDUGliQtT9f30R9Kcgp4EXiiqt7ubd/BFZdtkmxM8nLv248DvwN8YslbMbevxuCSpG46ndFX1dSA7Y/12fYGizdsqapjQFYwnyRphfxkrCQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1rlPok+xNMpfkZJIne9sOJpntPc4kmR2w9lNJ/i3JN5N8djWHlyQNt3bYAUkmgMeBLcB54HCSl6rq0SXHPAO802ftGuAvgV8HzgJfT/LVqjq1SvNLkobockY/Dhyvqner6iLwCvDw5Z1JAjwCTPdZuwX4ZlV9q6rOA18GPr3ysSVJXXUJ/RwwlWRDknXAduC+JfungDer6vU+a+8Bvrvk+7O9bT8hye4kM0lmFhYWuk0vSRpqaOirah54GjgCHAZmgUtLDtlJ/7P5Zamq/VU1WVWTY2NjK/1xkqSeTjdjq+pAVT1QVVuBc8BrAEnWsngZ5+CApd/jx8/+7+1tkyRdJ13fdXN37+smFsP+fG/XNuB0VZ0dsPTrwC8muT/JB4AdwFdXNrIkaTmGvuum51CSDcAF4Imqeru3fQdXXLZJshF4rqq2V9XFJL8H/D2wBvhCVZ1cpdklSR10vXQzVVUfqapfrqp/XLL9sar66yuOfaOqti/5/uWq+qWq+oWqemr1Rpeun+npaSYmJlizZg0TExNMT6/4tpR03XQ9o5duWdPT0+zbt48DBw7w4IMPcuzYMXbt2gXAzp07RzydNFyqatQz/ITJycmamZkZ9RgSABMTEzz77LM89NBDP9p29OhR9uzZw9zc3Agnk/5fkhNVNdl3n6GXrm7NmjW899573HbbbT/aduHCBW6//XYuXbp0lZXS9XO10PtLzaQhxsfHOXbs2I9tO3bsGOPj4yOaSFoeQy8NsW/fPnbt2sXRo0e5cOECR48eZdeuXezbt2/Uo0mdeDNWGuLyDdc9e/YwPz/P+Pg4Tz31lDdiddPwGr0kNcBr9JJ0CzP0ktQ4Qy9JjTP0ktQ4Qy9Jjbsh33WTZAH49qjnkPq4C/j+qIeQ+vj5qur7V5tuyNBLN6okM4PewibdqLx0I0mNM/SS1DhDLy3P/lEPIC2X1+glqXGe0UtS4wy9JDXO0EsdJPlCkreS+LcDddMx9FI3XwQ+NeohpGth6KUOquqfgf8a9RzStTD0ktQ4Qy9JjTP0ktQ4Qy9JjTP0UgdJpoF/AT6c5GySXaOeSerKX4EgSY3zjF6SGmfoJalxhl6SGmfoJalxhl6SGmfoJalxhl6SGvd/n8N+tQyN/4UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model6: with dropout regularizaition + custom optimizer"
      ],
      "metadata": {
        "id": "Zm4zRfuexDqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28 * 28\n",
        "output_size = 10\n",
        "optimizer = \"custom\"\n",
        "\n",
        "hidden_size = 512\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "dropout = 0.2\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i + 6)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"mnist\")\n",
        "\n",
        "    model6 = MLP(input_size, output_size, hidden_size, optimizer=optimizer)\n",
        "    train_losses, eval_losses, train_accuracy, test_accuracy, _epoch_times = train(model6, x_train, y_train, x_test, y_test,\n",
        "            batch_size = batch_size, \n",
        "            learning_rate = learning_rate, \n",
        "            epochs = epochs,\n",
        "            dropout_rate = dropout)\n",
        "    \n",
        "    acc = test(model6, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "95RWA7Mbxa7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9113e076-d406-4e28-8d4e-96c61e6880f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=599.5071411132812, Evaluation Loss:=0.13573551177978516, Train Acc:=96.62333333333333, Evaluation Acc:=95.87\n",
            "Epoch=1 - Train Loss:=230.21409606933594, Evaluation Loss:=0.09909538179636002, Train Acc:=97.99333333333334, Evaluation Acc:=97.02\n",
            "Epoch=2 - Train Loss:=166.8896026611328, Evaluation Loss:=0.10082519799470901, Train Acc:=98.23333333333333, Evaluation Acc:=97.06\n",
            "Epoch=3 - Train Loss:=135.11553955078125, Evaluation Loss:=0.08147529512643814, Train Acc:=98.655, Evaluation Acc:=97.59\n",
            "Epoch=4 - Train Loss:=109.17586517333984, Evaluation Loss:=0.09814437478780746, Train Acc:=98.70666666666666, Evaluation Acc:=97.32\n",
            "Epoch=5 - Train Loss:=99.18446350097656, Evaluation Loss:=0.08902967721223831, Train Acc:=98.94166666666668, Evaluation Acc:=97.44\n",
            "Epoch=6 - Train Loss:=82.13459777832031, Evaluation Loss:=0.0742400735616684, Train Acc:=99.33333333333333, Evaluation Acc:=98.03\n",
            "Epoch=7 - Train Loss:=76.38065338134766, Evaluation Loss:=0.09132698178291321, Train Acc:=99.33666666666666, Evaluation Acc:=97.86\n",
            "Epoch=8 - Train Loss:=69.42567443847656, Evaluation Loss:=0.10594658553600311, Train Acc:=99.2, Evaluation Acc:=97.66\n",
            "Epoch=9 - Train Loss:=64.2654800415039, Evaluation Loss:=0.09511750191450119, Train Acc:=99.42166666666667, Evaluation Acc:=97.85000000000001\n",
            "Epoch=0 - Train Loss:=592.3831176757812, Evaluation Loss:=0.13178445398807526, Train Acc:=96.575, Evaluation Acc:=95.95\n",
            "Epoch=1 - Train Loss:=222.26742553710938, Evaluation Loss:=0.09356808662414551, Train Acc:=97.95833333333334, Evaluation Acc:=97.00999999999999\n",
            "Epoch=2 - Train Loss:=166.4517059326172, Evaluation Loss:=0.11130974441766739, Train Acc:=97.88666666666667, Evaluation Acc:=96.56\n",
            "Epoch=3 - Train Loss:=136.23333740234375, Evaluation Loss:=0.08440806716680527, Train Acc:=98.64666666666668, Evaluation Acc:=97.42\n",
            "Epoch=4 - Train Loss:=110.58592224121094, Evaluation Loss:=0.08302684128284454, Train Acc:=98.98833333333333, Evaluation Acc:=97.52\n",
            "Epoch=5 - Train Loss:=97.21435546875, Evaluation Loss:=0.10780594497919083, Train Acc:=98.725, Evaluation Acc:=97.2\n",
            "Epoch=6 - Train Loss:=85.22390747070312, Evaluation Loss:=0.0943010002374649, Train Acc:=99.08500000000001, Evaluation Acc:=97.66\n",
            "Epoch=7 - Train Loss:=81.12376403808594, Evaluation Loss:=0.10003464668989182, Train Acc:=99.13333333333333, Evaluation Acc:=97.67\n",
            "Epoch=8 - Train Loss:=66.35514068603516, Evaluation Loss:=0.09377168118953705, Train Acc:=99.33999999999999, Evaluation Acc:=97.75\n",
            "Epoch=9 - Train Loss:=62.357635498046875, Evaluation Loss:=0.08820945024490356, Train Acc:=99.58500000000001, Evaluation Acc:=98.22999999999999\n",
            "Epoch=0 - Train Loss:=617.4647827148438, Evaluation Loss:=0.13932538032531738, Train Acc:=96.34, Evaluation Acc:=95.63000000000001\n",
            "Epoch=1 - Train Loss:=233.2628631591797, Evaluation Loss:=0.10461391508579254, Train Acc:=97.765, Evaluation Acc:=96.78\n",
            "Epoch=2 - Train Loss:=169.3698272705078, Evaluation Loss:=0.09298138320446014, Train Acc:=98.31833333333333, Evaluation Acc:=97.36\n",
            "Epoch=3 - Train Loss:=138.038330078125, Evaluation Loss:=0.08925937861204147, Train Acc:=98.73666666666666, Evaluation Acc:=97.36\n",
            "Epoch=4 - Train Loss:=112.44670104980469, Evaluation Loss:=0.07258779555559158, Train Acc:=99.14, Evaluation Acc:=97.91\n",
            "Epoch=5 - Train Loss:=95.2311782836914, Evaluation Loss:=0.10250291973352432, Train Acc:=98.86500000000001, Evaluation Acc:=97.45\n",
            "Epoch=6 - Train Loss:=84.74010467529297, Evaluation Loss:=0.09865890443325043, Train Acc:=99.18333333333334, Evaluation Acc:=97.55\n",
            "Epoch=7 - Train Loss:=82.95762634277344, Evaluation Loss:=0.09655571728944778, Train Acc:=99.225, Evaluation Acc:=97.94\n",
            "Epoch=8 - Train Loss:=67.06346893310547, Evaluation Loss:=0.10093031823635101, Train Acc:=99.335, Evaluation Acc:=97.95\n",
            "Epoch=9 - Train Loss:=65.50833129882812, Evaluation Loss:=0.1305023431777954, Train Acc:=98.85000000000001, Evaluation Acc:=97.42\n",
            "Epoch=0 - Train Loss:=631.1923217773438, Evaluation Loss:=0.1310807764530182, Train Acc:=96.65833333333333, Evaluation Acc:=95.77\n",
            "Epoch=1 - Train Loss:=232.236572265625, Evaluation Loss:=0.09828315675258636, Train Acc:=97.88666666666667, Evaluation Acc:=97.06\n",
            "Epoch=2 - Train Loss:=166.56532287597656, Evaluation Loss:=0.12910078465938568, Train Acc:=97.555, Evaluation Acc:=96.57\n",
            "Epoch=3 - Train Loss:=134.92025756835938, Evaluation Loss:=0.10979317873716354, Train Acc:=98.12166666666667, Evaluation Acc:=96.87\n",
            "Epoch=4 - Train Loss:=113.83192443847656, Evaluation Loss:=0.09340803325176239, Train Acc:=98.73166666666665, Evaluation Acc:=97.39999999999999\n",
            "Epoch=5 - Train Loss:=94.19882202148438, Evaluation Loss:=0.08103282004594803, Train Acc:=99.14, Evaluation Acc:=97.7\n",
            "Epoch=6 - Train Loss:=86.77457427978516, Evaluation Loss:=0.10345680266618729, Train Acc:=98.785, Evaluation Acc:=97.35000000000001\n",
            "Epoch=7 - Train Loss:=77.03981018066406, Evaluation Loss:=0.1207362562417984, Train Acc:=99.09, Evaluation Acc:=97.27\n",
            "Epoch=8 - Train Loss:=69.40626525878906, Evaluation Loss:=0.11886375397443771, Train Acc:=98.88833333333334, Evaluation Acc:=97.28\n",
            "Epoch=9 - Train Loss:=69.18987274169922, Evaluation Loss:=0.11002827435731888, Train Acc:=99.24, Evaluation Acc:=97.72999999999999\n",
            "Epoch=0 - Train Loss:=622.7824096679688, Evaluation Loss:=0.1237511858344078, Train Acc:=96.84, Evaluation Acc:=96.14\n",
            "Epoch=1 - Train Loss:=234.85853576660156, Evaluation Loss:=0.10306159406900406, Train Acc:=97.94, Evaluation Acc:=96.65\n",
            "Epoch=2 - Train Loss:=167.85450744628906, Evaluation Loss:=0.09469348192214966, Train Acc:=98.44833333333334, Evaluation Acc:=97.33000000000001\n",
            "Epoch=3 - Train Loss:=136.51596069335938, Evaluation Loss:=0.11869334429502487, Train Acc:=98.125, Evaluation Acc:=96.78999999999999\n",
            "Epoch=4 - Train Loss:=113.39021301269531, Evaluation Loss:=0.0915607213973999, Train Acc:=98.78, Evaluation Acc:=97.53\n",
            "Epoch=5 - Train Loss:=97.26851654052734, Evaluation Loss:=0.08440056443214417, Train Acc:=99.095, Evaluation Acc:=97.57000000000001\n",
            "Epoch=6 - Train Loss:=87.7729721069336, Evaluation Loss:=0.09654466807842255, Train Acc:=99.02166666666666, Evaluation Acc:=97.58\n",
            "Epoch=7 - Train Loss:=77.4120101928711, Evaluation Loss:=0.09413771331310272, Train Acc:=99.24333333333333, Evaluation Acc:=97.82\n",
            "Epoch=8 - Train Loss:=65.27535247802734, Evaluation Loss:=0.11663533002138138, Train Acc:=99.18666666666667, Evaluation Acc:=97.3\n",
            "Epoch=9 - Train Loss:=66.32135009765625, Evaluation Loss:=0.096310093998909, Train Acc:=99.28666666666666, Evaluation Acc:=97.92\n",
            "Epoch=0 - Train Loss:=611.8948364257812, Evaluation Loss:=0.11992783099412918, Train Acc:=96.81, Evaluation Acc:=95.99\n",
            "Epoch=1 - Train Loss:=233.6173553466797, Evaluation Loss:=0.0966118797659874, Train Acc:=97.91499999999999, Evaluation Acc:=97.0\n",
            "Epoch=2 - Train Loss:=174.52120971679688, Evaluation Loss:=0.10563991963863373, Train Acc:=97.96333333333334, Evaluation Acc:=96.82\n",
            "Epoch=3 - Train Loss:=134.85183715820312, Evaluation Loss:=0.088958241045475, Train Acc:=98.52666666666666, Evaluation Acc:=97.5\n",
            "Epoch=4 - Train Loss:=112.20597076416016, Evaluation Loss:=0.11738963425159454, Train Acc:=98.42833333333333, Evaluation Acc:=96.96000000000001\n",
            "Epoch=5 - Train Loss:=96.8912124633789, Evaluation Loss:=0.09121546894311905, Train Acc:=98.86833333333334, Evaluation Acc:=97.28\n",
            "Epoch=6 - Train Loss:=85.52644348144531, Evaluation Loss:=0.10083431750535965, Train Acc:=98.92666666666666, Evaluation Acc:=97.42\n",
            "Epoch=7 - Train Loss:=81.5437240600586, Evaluation Loss:=0.08461260050535202, Train Acc:=99.27166666666668, Evaluation Acc:=97.84\n",
            "Epoch=8 - Train Loss:=69.55977630615234, Evaluation Loss:=0.08580107241868973, Train Acc:=99.41333333333333, Evaluation Acc:=97.84\n",
            "Epoch=9 - Train Loss:=69.10822296142578, Evaluation Loss:=0.09631112217903137, Train Acc:=99.32166666666666, Evaluation Acc:=97.83\n",
            "Epoch=0 - Train Loss:=625.7708740234375, Evaluation Loss:=0.14067323505878448, Train Acc:=96.675, Evaluation Acc:=95.73\n",
            "Epoch=1 - Train Loss:=233.76388549804688, Evaluation Loss:=0.10937359929084778, Train Acc:=97.71166666666666, Evaluation Acc:=96.87\n",
            "Epoch=2 - Train Loss:=165.43081665039062, Evaluation Loss:=0.09537919610738754, Train Acc:=98.35666666666667, Evaluation Acc:=97.16\n",
            "Epoch=3 - Train Loss:=135.81436157226562, Evaluation Loss:=0.08882676064968109, Train Acc:=98.825, Evaluation Acc:=97.37\n",
            "Epoch=4 - Train Loss:=109.47822570800781, Evaluation Loss:=0.10131679475307465, Train Acc:=98.74666666666667, Evaluation Acc:=97.27\n",
            "Epoch=5 - Train Loss:=99.87871551513672, Evaluation Loss:=0.09915419667959213, Train Acc:=99.02333333333333, Evaluation Acc:=97.6\n",
            "Epoch=6 - Train Loss:=79.73148345947266, Evaluation Loss:=0.08432858437299728, Train Acc:=99.055, Evaluation Acc:=97.71\n",
            "Epoch=7 - Train Loss:=79.39542388916016, Evaluation Loss:=0.09041011333465576, Train Acc:=99.47333333333333, Evaluation Acc:=98.03\n",
            "Epoch=8 - Train Loss:=66.94065856933594, Evaluation Loss:=0.10928601026535034, Train Acc:=99.40833333333333, Evaluation Acc:=97.68\n",
            "Epoch=9 - Train Loss:=65.24351501464844, Evaluation Loss:=0.12419204413890839, Train Acc:=99.23833333333333, Evaluation Acc:=97.67\n",
            "Epoch=0 - Train Loss:=612.24853515625, Evaluation Loss:=0.12676775455474854, Train Acc:=96.52333333333334, Evaluation Acc:=96.23\n",
            "Epoch=1 - Train Loss:=236.4987335205078, Evaluation Loss:=0.09847357124090195, Train Acc:=97.90166666666667, Evaluation Acc:=96.92\n",
            "Epoch=2 - Train Loss:=171.21278381347656, Evaluation Loss:=0.10218380391597748, Train Acc:=97.87833333333333, Evaluation Acc:=96.89999999999999\n",
            "Epoch=3 - Train Loss:=137.87283325195312, Evaluation Loss:=0.10275476425886154, Train Acc:=98.41833333333334, Evaluation Acc:=97.07000000000001\n",
            "Epoch=4 - Train Loss:=116.62206268310547, Evaluation Loss:=0.0958617702126503, Train Acc:=98.73166666666665, Evaluation Acc:=97.27\n",
            "Epoch=5 - Train Loss:=93.72869873046875, Evaluation Loss:=0.08154627680778503, Train Acc:=99.03, Evaluation Acc:=97.76\n",
            "Epoch=6 - Train Loss:=88.64628601074219, Evaluation Loss:=0.09337684512138367, Train Acc:=99.27, Evaluation Acc:=97.65\n",
            "Epoch=7 - Train Loss:=75.51222229003906, Evaluation Loss:=0.08735130727291107, Train Acc:=99.29666666666667, Evaluation Acc:=97.85000000000001\n",
            "Epoch=8 - Train Loss:=69.23814392089844, Evaluation Loss:=0.09036405384540558, Train Acc:=99.41666666666666, Evaluation Acc:=98.03\n",
            "Epoch=9 - Train Loss:=64.58521270751953, Evaluation Loss:=0.11688242107629776, Train Acc:=99.28666666666666, Evaluation Acc:=97.6\n",
            "Epoch=0 - Train Loss:=596.5518798828125, Evaluation Loss:=0.13183175027370453, Train Acc:=96.81, Evaluation Acc:=95.92\n",
            "Epoch=1 - Train Loss:=227.65744018554688, Evaluation Loss:=0.11156144738197327, Train Acc:=97.72333333333333, Evaluation Acc:=96.58\n",
            "Epoch=2 - Train Loss:=171.5385284423828, Evaluation Loss:=0.08962312340736389, Train Acc:=98.38, Evaluation Acc:=97.39\n",
            "Epoch=3 - Train Loss:=139.77957153320312, Evaluation Loss:=0.09980157762765884, Train Acc:=98.55833333333334, Evaluation Acc:=97.16\n",
            "Epoch=4 - Train Loss:=113.34201049804688, Evaluation Loss:=0.09477166831493378, Train Acc:=98.96166666666667, Evaluation Acc:=97.58\n",
            "Epoch=5 - Train Loss:=99.58207702636719, Evaluation Loss:=0.11686787009239197, Train Acc:=98.80666666666667, Evaluation Acc:=96.96000000000001\n",
            "Epoch=6 - Train Loss:=89.90956115722656, Evaluation Loss:=0.11406577378511429, Train Acc:=98.41499999999999, Evaluation Acc:=96.81\n",
            "Epoch=7 - Train Loss:=75.20065307617188, Evaluation Loss:=0.10673549771308899, Train Acc:=99.32333333333332, Evaluation Acc:=97.58\n",
            "Epoch=8 - Train Loss:=72.88512420654297, Evaluation Loss:=0.09765636175870895, Train Acc:=99.48333333333333, Evaluation Acc:=97.78\n",
            "Epoch=9 - Train Loss:=65.19842529296875, Evaluation Loss:=0.10663427412509918, Train Acc:=99.33666666666666, Evaluation Acc:=97.65\n",
            "Epoch=0 - Train Loss:=627.4295043945312, Evaluation Loss:=0.13888110220432281, Train Acc:=96.40333333333334, Evaluation Acc:=95.75\n",
            "Epoch=1 - Train Loss:=236.8682403564453, Evaluation Loss:=0.10193673521280289, Train Acc:=97.83, Evaluation Acc:=96.82\n",
            "Epoch=2 - Train Loss:=166.78819274902344, Evaluation Loss:=0.08534727245569229, Train Acc:=98.45666666666666, Evaluation Acc:=97.38\n",
            "Epoch=3 - Train Loss:=142.41024780273438, Evaluation Loss:=0.08595901727676392, Train Acc:=98.40333333333334, Evaluation Acc:=97.36\n",
            "Epoch=4 - Train Loss:=113.56101989746094, Evaluation Loss:=0.11203695088624954, Train Acc:=98.45666666666666, Evaluation Acc:=97.07000000000001\n",
            "Epoch=5 - Train Loss:=96.19441986083984, Evaluation Loss:=0.0803380161523819, Train Acc:=98.97666666666667, Evaluation Acc:=97.71\n",
            "Epoch=6 - Train Loss:=88.32561492919922, Evaluation Loss:=0.09469334781169891, Train Acc:=99.16666666666667, Evaluation Acc:=97.89\n",
            "Epoch=7 - Train Loss:=76.84907531738281, Evaluation Loss:=0.10049159079790115, Train Acc:=99.25166666666667, Evaluation Acc:=97.57000000000001\n",
            "Epoch=8 - Train Loss:=69.34185028076172, Evaluation Loss:=0.10111182928085327, Train Acc:=99.21333333333334, Evaluation Acc:=97.8\n",
            "Epoch=9 - Train Loss:=68.70391082763672, Evaluation Loss:=0.1114220917224884, Train Acc:=99.27333333333334, Evaluation Acc:=97.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "GK489oKgVHi5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "363295d0-d0d9-4767-f46c-e9259fdd6dbe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 97.748, and the variance is 0.045275999999999234\n",
            "the average running time for one epoch is: 40.69951018810272\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fec3c553ed0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fec3c4ff610>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c4ffdd0>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fec3c46aa50>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fec3c46a510>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fec3c7597d0>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c4ff390>]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FASHION MNIST Dataset"
      ],
      "metadata": {
        "id": "5Wq69TLbxb08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualize sample data"
      ],
      "metadata": {
        "id": "khZmktC5xgKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y, test_x, test_y = load_data(dataset=\"fashion_mnist\")\n",
        "sample = tf.reshape(train_x[0], (28,28))\n",
        "print(f\"sample image shape: {sample.shape}\")\n",
        "print(f\"sample image label: {train_y[0]}\")\n",
        "plt.imshow(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "2q1Sj6jTxiiL",
        "outputId": "9da09631-923f-4ec4-aff0-8c6636fc50db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "sample image shape: (28, 28)\n",
            "sample image label: 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8e022e4350>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUFElEQVR4nO3da2yc1ZkH8P8z4/ElzjiJk+CE4BIuoZDCEqhJuIlSKDREVQOli4gQCxLaoF3otl0+gGhXZb+sEFpAaNntroEsYVWoWhUERREFzCULlDQmpOS2ITeHxDi2ExPbcTz2XJ794Bdqgs/zmnnnRs7/J1kezzNn5njGf78zc+acI6oKIjr+xcrdASIqDYadyBMMO5EnGHYiTzDsRJ6oKuWNVUuN1qK+lDdJ5JUUhjCqIzJRLVLYRWQpgEcAxAE8rqr3W5evRT2WyJVRbpKIDOu0zVnL+2m8iMQB/DuAawAsBLBCRBbme31EVFxRXrMvBrBTVXer6iiAXwNYXphuEVGhRQn7PAD7xv28Pzjvc0RkpYi0i0h7GiMRbo6Ioij6u/Gq2qqqLarakkBNsW+OiByihL0TQPO4n08KziOiChQl7OsBLBCRU0SkGsCNAF4oTLeIqNDyHnpT1YyI3AngDxgbelulqlsK1jMiKqhI4+yqugbAmgL1hYiKiB+XJfIEw07kCYadyBMMO5EnGHYiTzDsRJ5g2Ik8wbATeYJhJ/IEw07kCYadyBMMO5EnGHYiT5R0KWkqA5lwVeG/iLixZ3xmo1n/5LtnOGsNT78b6bbDfjepSjhrmh6NdttRhT0uljwfMx7ZiTzBsBN5gmEn8gTDTuQJhp3IEww7kScYdiJPcJz9OCfxuFnXTMasxxbZe3Vuu32q3X7YXUsMLTbbVg3nzHri5XazHmksPWwMP+R+hdjH0Sh9kyojtsbDySM7kScYdiJPMOxEnmDYiTzBsBN5gmEn8gTDTuQJjrMf58wxWYSPs+/77nSzftNF/2vW3+491VnbWzPHbKt1ZhlV37nIrJ/xH53OWqbjI/vKQ+aMh91vYeIzZriL2azZNjsw4C4a3Y4UdhHpADAIIAsgo6otUa6PiIqnEEf2b6vqwQJcDxEVEV+zE3kiatgVwMsi8p6IrJzoAiKyUkTaRaQ9jZGIN0dE+Yr6NP5SVe0UkRMAvCIi/6eqa8dfQFVbAbQCQIM0RlvdkIjyFunIrqqdwfceAM8BsKcxEVHZ5B12EakXkeSnpwFcDWBzoTpGRIUV5Wl8E4DnZGzebxWAp1X1pYL0igoml0pFaj963hGz/sNp9pzy2ljaWXszZs9X73yt2axn/8ru296Hks5a7v2LzbYzN9tj3Q3vd5n1g5fNM+u933S/om0KWU5/xqu7nDXpc0c677Cr6m4A5+bbnohKi0NvRJ5g2Ik8wbATeYJhJ/IEw07kCdGIW/Z+GQ3SqEvkypLdnjesZY9DHt8jN1xo1q/5+Rtm/azaj836YK7WWRvVaB/gfHT7t8z60O5pzlpsNGTL5JBytsleClrT9nF0xgb37163vNtsK4/NdtY+aHsER/r2Tdh7HtmJPMGwE3mCYSfyBMNO5AmGncgTDDuRJxh2Ik9wnL0ShGwPHEnI43v2e/b/+x/MsKewhokbaxsPabXZ9nC2PtJt92bcU1zTIWP8j++wp8AeMcbwASCWsR/Tq779vrN2feN6s+0Dp53jrK3TNgxoH8fZiXzGsBN5gmEn8gTDTuQJhp3IEww7kScYdiJPcMvmSlDCzzoca8eRE8z6oYapZv1Axt7SeWbcvdxzMjZstp2fsPcL7c26x9EBIJ5wL1U9qnGz7T9/4/dmPXVWwqwnxF6K+mJjHYC/3vo3Ztt67DbrLjyyE3mCYSfyBMNO5AmGncgTDDuRJxh2Ik8w7ESe4Di752bX2Nse14p7y2UAqJaMWf84PcNZ2zH8dbPthwP2ZwCWNm0x62ljLN2aZw+Ej5OfmPjErKfUHoe37tVLmuxx9I1m1S30yC4iq0SkR0Q2jzuvUUReEZEdwXf3I0pEFWEyT+OfBLD0mPPuAdCmqgsAtAU/E1EFCw27qq4F0HfM2csBrA5OrwZwbYH7RUQFlu9r9iZV7QpOHwDQ5LqgiKwEsBIAajElz5sjoqgivxuvYytWOt/tUNVWVW1R1ZYEaqLeHBHlKd+wd4vIXAAIvvcUrktEVAz5hv0FALcEp28B8HxhukNExRL6ml1EngFwOYBZIrIfwC8A3A/gNyJyG4C9AG4oZiePeyHrxkvcnnutGfdYd3yGPSr6rembzHpvtsGsH87a78NMjx911gYz7r3bAaBv2L7uM2u6zPqGo/OdtdnV9ji51W8A6BidZdYX1Bww6w90u/dPaK499v3wz8tceZmzpuv+6KyFhl1VVzhK3O2B6CuEH5cl8gTDTuQJhp3IEww7kScYdiJPcIprJQhZSlqq7IfJGnrbd9tZZtsrpthLJr+TmmfWZ1cNmnVrmuncmn6zbbIpZdbDhv0aq9zTdwezdWbbKbERsx72e59fbS+D/dNXz3fWkmcfMts2JIxjtDGKyyM7kScYdiJPMOxEnmDYiTzBsBN5gmEn8gTDTuQJjrNXAElUm/Vcyh5vtszaNGrWD2btJY+nx+ypntUhSy5bWyNf3LjHbNsbMha+YfgUs56Mu7eEnh2zx8mbE/ZY96ZUs1lfM3S6Wb/te686a8+0XmW2rX7pHWdN1P148chO5AmGncgTDDuRJxh2Ik8w7ESeYNiJPMGwE3niqzXObiy5LFX2eLHEQ/6vxex6LmXMb87ZY81hNG2PhUfxyH89atb3Zaab9QNpux625HLWmGD97vA0s21tzN4uenbVgFkfyNnj9JbBnL3MtTVPHwjv+90zdzhrz/Z/x2ybLx7ZiTzBsBN5gmEn8gTDTuQJhp3IEww7kScYdiJPVNQ4e5T10cPGqtUe9iyr4eWLzfq+a+1x/JvO+5OzdiCTNNu+b2xrDADTjDnhAFAfsr56St2ff/h41N5OOmys2loXHgBOMMbhs2of5zrTdt/ChH3+YH/GWNP++/Zc++lP5dWl8CO7iKwSkR4R2TzuvPtEpFNENgZfy/K7eSIqlck8jX8SwNIJzn9YVRcFX2sK2y0iKrTQsKvqWgB9JegLERVRlDfo7hSRD4Kn+c4XOCKyUkTaRaQ9Dfv1HREVT75h/yWA0wAsAtAF4EHXBVW1VVVbVLUlgZo8b46Iosor7KrarapZVc0BeAyA/XYyEZVdXmEXkbnjfrwOwGbXZYmoMoSOs4vIMwAuBzBLRPYD+AWAy0VkEQAF0AHg9kJ0xhpHj6pq7hyznj6lyaz3neXeC/zoHGNTbACLlm0z67c2/bdZ7802mPWEGPuzp2eabc+b0mHWX+tfaNYPVk0169Y4/cX17jndAHA4Z++/fmLVJ2b97p0/dNaapthj2Y+fbA8wpTVn1ren7Zes/Tn3fPh/WPi62fY5zDbrLqFhV9UVE5z9RF63RkRlw4/LEnmCYSfyBMNO5AmGncgTDDuRJypqiuvINReY9RN+tttZW9Sw32y7sO4ts57K2UtRW9Mttw7PM9sezdlbMu8YtYcF+zP2EFRc3MNAPaP2FNcH99jLFrct/k+z/vOPJ5oj9RexOnXWDmXtYbvrp9pLRQP2Y3b719Y6a6dW95htXxyaa9Y/DpkC25ToN+vzE73O2g+SH5pt8x1645GdyBMMO5EnGHYiTzDsRJ5g2Ik8wbATeYJhJ/JEacfZxV4uesm/rDebX5nc4qwdVXtKYdg4eti4qWValb1s8Ejavpt70vYU1jBn1Bxw1q5r2Gi2XfvoErN+aepHZn3XFfb03LZh91TO3oz9e9+45wqzvuGjZrN+4fw9zto5yU6zbdhnG5LxlFm3ph0DwFDO/ff6bsr+/EG+eGQn8gTDTuQJhp3IEww7kScYdiJPMOxEnmDYiTwhqu75xoVWN6dZT7v5H5311jv+zWz/dN+Fzlpzrb0d3cnVB836zLi9/a8lGbPHXL+esMdcXxw6yay/cfhMs/7NZIezlhB7u+fLp+w067f+9C6znqm1l9EemO8+nmTq7b+9hnMPmfUfnf6aWa82fvfDWXscPex+C9uSOYy1BkEyZm+T/eCy65y1P3Y8if7hrgkfFB7ZiTzBsBN5gmEn8gTDTuQJhp3IEww7kScYdiJPlHQ+eywNTOl2jy++OLDIbH9qnXut7YNpe330Pxw5x6yfVGdv/2ttPXy6MZ8cADamppv1l3q/YdZPrLPXT+9OT3PWDqXrzbZHjXnVAPDEww+Z9Qe77XXnr2vc4KydW22Pox/O2ceirSHr7Q/map21lNrrG/SHjMMnjb8HAEirHa24seXz9Jg9hj9wjnsb7my3+3ZDj+wi0iwir4vIVhHZIiI/Ds5vFJFXRGRH8D3/1R+IqOgm8zQ+A+AuVV0I4EIAd4jIQgD3AGhT1QUA2oKfiahChYZdVbtUdUNwehDANgDzACwHsDq42GoA1xark0QU3Zd6g05E5gM4D8A6AE2q2hWUDgBocrRZKSLtItKeGRmK0FUiimLSYReRqQB+B+Anqvq5d4x0bDbNhLMaVLVVVVtUtaWqxn6ziIiKZ1JhF5EExoL+K1V9Nji7W0TmBvW5AOxtMYmorEKH3kREADwBYJuqjh+HeQHALQDuD74/H3Zd8dEckvtGnPWc2tMlXzvonurZVDtotl2U3GfWtx+1h3E2DZ/orG2o+prZti7u3u4ZAKZV21Nk66vc9xkAzEq4f/dTauz/wdY0UABYn7J/t7+b/YZZ/yjjHqT5/dAZZtutR933OQDMCFnCe9OAu/3RjL2N9kjWjkYqYw/lTquxH9MLGvc6a9thbxfde64xbfhtd7vJjLNfAuBmAJtE5NNFyO/FWMh/IyK3AdgL4IZJXBcRlUlo2FX1LQCuQ+6Vhe0OERULPy5L5AmGncgTDDuRJxh2Ik8w7ESeKO2WzUeGEXvzfWf5ty9fYjb/p+W/ddbeDFlu+cUD9rjowKg91XP2FPdHfRuMcW4AaEzYHxMO2/K5NmT7308y7k8mjsTsqZxZ50DLmAMj7umzAPB2boFZT+fcWzaPGDUg/PMJfaOzzPqJdf3O2mDGPf0VADoGG836wX57W+XUFDtab2VPc9aWznFvTQ4AdT3uxyxm/KnwyE7kCYadyBMMO5EnGHYiTzDsRJ5g2Ik8wbATeaKkWzY3SKMukfwnyvXf5N6y+dS/3262XTx9j1nfMGDP2/7IGHdNhyx5nIi5lw0GgCmJUbNeGzLeXB13z0mPTbyA0GdyIePs9XG7b2Fz7Ruq3PO6k3F7znfM2NZ4MuLG7/6n/vmRrjsZ8ntn1P6buGjaLmdt1Z6LzbbTlrm32V6nbRjQPm7ZTOQzhp3IEww7kScYdiJPMOxEnmDYiTzBsBN5ovTj7PGr3RfI2WuYRzF0/RKzvuTe9XY96R4XPbO622ybgD1eXBsynlwfs8fCU8ZjGPbf/K3hZrOeDbmG1z45y6ynjfHm7qMNZtuE8fmBybD2IRjOhGzZPGzPd4/H7Nyk3rDn2s/c6v7sRM0a+2/RwnF2ImLYiXzBsBN5gmEn8gTDTuQJhp3IEww7kSdCx9lFpBnAUwCaACiAVlV9RETuA/C3AHqDi96rqmus64o6n71SyQX2mvTDc+rMes0he2704Ml2+4Zd7nXpYyP2mvO5P28z6/TVYo2zT2aTiAyAu1R1g4gkAbwnIq8EtYdV9V8L1VEiKp7J7M/eBaArOD0oItsAzCt2x4iosL7Ua3YRmQ/gPADrgrPuFJEPRGSViMxwtFkpIu0i0p6G/XSViIpn0mEXkakAfgfgJ6o6AOCXAE4DsAhjR/4HJ2qnqq2q2qKqLQnY+6kRUfFMKuwiksBY0H+lqs8CgKp2q2pWVXMAHgOwuHjdJKKoQsMuIgLgCQDbVPWhcefPHXex6wBsLnz3iKhQJvNu/CUAbgawSUQ2BufdC2CFiCzC2HBcB4Dbi9LDrwBdv8ms25MlwzW8k3/baIsx0/FkMu/GvwVMuLi4OaZORJWFn6Aj8gTDTuQJhp3IEww7kScYdiJPMOxEnmDYiTzBsBN5gmEn8gTDTuQJhp3IEww7kScYdiJPMOxEnijpls0i0gtg77izZgE4WLIOfDmV2rdK7RfAvuWrkH07WVVnT1Qoadi/cOMi7araUrYOGCq1b5XaL4B9y1ep+san8USeYNiJPFHusLeW+fYtldq3Su0XwL7lqyR9K+trdiIqnXIf2YmoRBh2Ik+UJewislREtovIThG5pxx9cBGRDhHZJCIbRaS9zH1ZJSI9IrJ53HmNIvKKiOwIvk+4x16Z+nafiHQG991GEVlWpr41i8jrIrJVRLaIyI+D88t63xn9Ksn9VvLX7CISB/AhgKsA7AewHsAKVd1a0o44iEgHgBZVLfsHMETkMgBHADylqmcH5z0AoE9V7w/+Uc5Q1bsrpG/3AThS7m28g92K5o7fZhzAtQBuRRnvO6NfN6AE91s5juyLAexU1d2qOgrg1wCWl6EfFU9V1wLoO+bs5QBWB6dXY+yPpeQcfasIqtqlqhuC04MAPt1mvKz3ndGvkihH2OcB2Dfu5/2orP3eFcDLIvKeiKwsd2cm0KSqXcHpAwCaytmZCYRu411Kx2wzXjH3XT7bn0fFN+i+6FJVPR/ANQDuCJ6uViQdew1WSWOnk9rGu1Qm2Gb8M+W87/Ld/jyqcoS9E0DzuJ9PCs6rCKraGXzvAfAcKm8r6u5Pd9ANvveUuT+fqaRtvCfaZhwVcN+Vc/vzcoR9PYAFInKKiFQDuBHAC2XoxxeISH3wxglEpB7A1ai8rahfAHBLcPoWAM+XsS+fUynbeLu2GUeZ77uyb3+uqiX/ArAMY+/I7wLws3L0wdGvUwH8OfjaUu6+AXgGY0/r0hh7b+M2ADMBtAHYAeBVAI0V1Lf/AbAJwAcYC9bcMvXtUow9Rf8AwMbga1m57zujXyW53/hxWSJP8A06Ik8w7ESeYNiJPMGwE3mCYSfyBMNO5AmGncgT/w8K8iUImXY9pQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model7: without regularization + SGD optimizer"
      ],
      "metadata": {
        "id": "-3QX0gR-8hNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "optimizer = \"SGD\"\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "learning_rate = 10e-5\n",
        "epochs = 20\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i + 7)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"fashion_mnist\")\n",
        "\n",
        "    model7 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer = optimizer)\n",
        "    train_losses, test_losses, train_accuracy, test_accuracy, _epoch_times = train(model7, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs)\n",
        "    acc = test(model7, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "3Cj-vyBm8m84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0bdee0-89bd-4719-cf93-41da4985703b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=3783.4521484375, Evaluation Loss:=1.342052698135376, Train Acc:=74.22833333333332, Evaluation Acc:=72.78\n",
            "Epoch=1 - Train Loss:=2100.505126953125, Evaluation Loss:=1.095248818397522, Train Acc:=77.39166666666667, Evaluation Acc:=75.71\n",
            "Epoch=2 - Train Loss:=1780.143310546875, Evaluation Loss:=0.9963443279266357, Train Acc:=78.7, Evaluation Acc:=77.13\n",
            "Epoch=3 - Train Loss:=1598.6038818359375, Evaluation Loss:=0.9535218477249146, Train Acc:=78.46, Evaluation Acc:=76.82\n",
            "Epoch=4 - Train Loss:=1467.3974609375, Evaluation Loss:=0.8602468967437744, Train Acc:=80.7, Evaluation Acc:=78.97\n",
            "Epoch=5 - Train Loss:=1373.647216796875, Evaluation Loss:=0.8127198815345764, Train Acc:=81.44, Evaluation Acc:=79.47\n",
            "Epoch=6 - Train Loss:=1295.8265380859375, Evaluation Loss:=0.7745304107666016, Train Acc:=82.11500000000001, Evaluation Acc:=80.25\n",
            "Epoch=7 - Train Loss:=1236.91015625, Evaluation Loss:=0.7525080442428589, Train Acc:=82.625, Evaluation Acc:=80.47999999999999\n",
            "Epoch=8 - Train Loss:=1180.963134765625, Evaluation Loss:=0.75624680519104, Train Acc:=82.57666666666667, Evaluation Acc:=80.28\n",
            "Epoch=9 - Train Loss:=1139.066650390625, Evaluation Loss:=0.7125661373138428, Train Acc:=83.25166666666667, Evaluation Acc:=80.88\n",
            "Epoch=10 - Train Loss:=1097.4432373046875, Evaluation Loss:=0.6896803975105286, Train Acc:=83.76, Evaluation Acc:=81.36\n",
            "Epoch=11 - Train Loss:=1065.1817626953125, Evaluation Loss:=0.6875671148300171, Train Acc:=83.92833333333334, Evaluation Acc:=81.53\n",
            "Epoch=12 - Train Loss:=1033.9771728515625, Evaluation Loss:=0.6748910546302795, Train Acc:=84.1, Evaluation Acc:=81.65\n",
            "Epoch=13 - Train Loss:=1007.8560180664062, Evaluation Loss:=0.6624006032943726, Train Acc:=84.55166666666668, Evaluation Acc:=82.07\n",
            "Epoch=14 - Train Loss:=982.392333984375, Evaluation Loss:=0.6524675488471985, Train Acc:=84.49333333333333, Evaluation Acc:=81.78999999999999\n",
            "Epoch=15 - Train Loss:=960.8514404296875, Evaluation Loss:=0.6309597492218018, Train Acc:=85.09166666666667, Evaluation Acc:=82.34\n",
            "Epoch=16 - Train Loss:=941.1055297851562, Evaluation Loss:=0.6264483332633972, Train Acc:=85.31333333333333, Evaluation Acc:=82.65\n",
            "Epoch=17 - Train Loss:=921.9085083007812, Evaluation Loss:=0.6198152303695679, Train Acc:=85.18666666666667, Evaluation Acc:=82.44\n",
            "Epoch=18 - Train Loss:=905.9564208984375, Evaluation Loss:=0.651894748210907, Train Acc:=83.905, Evaluation Acc:=81.28\n",
            "Epoch=19 - Train Loss:=888.7827758789062, Evaluation Loss:=0.5982518196105957, Train Acc:=85.79666666666667, Evaluation Acc:=82.78\n",
            "Epoch=0 - Train Loss:=3693.073974609375, Evaluation Loss:=1.3114979267120361, Train Acc:=73.71, Evaluation Acc:=72.6\n",
            "Epoch=1 - Train Loss:=2060.11181640625, Evaluation Loss:=1.092347264289856, Train Acc:=76.80666666666667, Evaluation Acc:=75.47\n",
            "Epoch=2 - Train Loss:=1728.5587158203125, Evaluation Loss:=0.9750621914863586, Train Acc:=78.32000000000001, Evaluation Acc:=76.9\n",
            "Epoch=3 - Train Loss:=1539.3570556640625, Evaluation Loss:=0.8752844929695129, Train Acc:=80.43166666666667, Evaluation Acc:=78.51\n",
            "Epoch=4 - Train Loss:=1415.0447998046875, Evaluation Loss:=0.8310630917549133, Train Acc:=80.68333333333332, Evaluation Acc:=78.86\n",
            "Epoch=5 - Train Loss:=1322.320068359375, Evaluation Loss:=0.780021607875824, Train Acc:=81.94666666666667, Evaluation Acc:=79.88\n",
            "Epoch=6 - Train Loss:=1252.7755126953125, Evaluation Loss:=0.7604377865791321, Train Acc:=82.28333333333333, Evaluation Acc:=80.05\n",
            "Epoch=7 - Train Loss:=1193.364501953125, Evaluation Loss:=0.7258983254432678, Train Acc:=82.99333333333333, Evaluation Acc:=80.66\n",
            "Epoch=8 - Train Loss:=1141.989501953125, Evaluation Loss:=0.717164158821106, Train Acc:=83.20166666666667, Evaluation Acc:=80.76\n",
            "Epoch=9 - Train Loss:=1100.663818359375, Evaluation Loss:=0.7005170583724976, Train Acc:=83.54, Evaluation Acc:=81.13\n",
            "Epoch=10 - Train Loss:=1063.28759765625, Evaluation Loss:=0.6838441491127014, Train Acc:=83.64, Evaluation Acc:=80.94\n",
            "Epoch=11 - Train Loss:=1032.25634765625, Evaluation Loss:=0.6607629060745239, Train Acc:=84.30166666666666, Evaluation Acc:=81.63\n",
            "Epoch=12 - Train Loss:=1003.368408203125, Evaluation Loss:=0.6518802046775818, Train Acc:=84.42333333333333, Evaluation Acc:=81.63\n",
            "Epoch=13 - Train Loss:=976.9735107421875, Evaluation Loss:=0.6482685804367065, Train Acc:=84.62166666666666, Evaluation Acc:=81.71000000000001\n",
            "Epoch=14 - Train Loss:=953.5509643554688, Evaluation Loss:=0.6352294683456421, Train Acc:=84.74833333333333, Evaluation Acc:=81.72\n",
            "Epoch=15 - Train Loss:=930.3841552734375, Evaluation Loss:=0.6495038866996765, Train Acc:=84.14, Evaluation Acc:=81.38\n",
            "Epoch=16 - Train Loss:=912.3648681640625, Evaluation Loss:=0.6147967576980591, Train Acc:=85.48666666666666, Evaluation Acc:=82.34\n",
            "Epoch=17 - Train Loss:=894.3817138671875, Evaluation Loss:=0.6083897352218628, Train Acc:=85.63833333333334, Evaluation Acc:=82.36\n",
            "Epoch=18 - Train Loss:=878.1742553710938, Evaluation Loss:=0.5989375710487366, Train Acc:=85.86500000000001, Evaluation Acc:=82.56\n",
            "Epoch=19 - Train Loss:=861.115234375, Evaluation Loss:=0.5887895226478577, Train Acc:=86.14666666666668, Evaluation Acc:=82.63000000000001\n",
            "Epoch=0 - Train Loss:=3870.19287109375, Evaluation Loss:=1.3131873607635498, Train Acc:=73.64166666666667, Evaluation Acc:=72.54\n",
            "Epoch=1 - Train Loss:=2067.6962890625, Evaluation Loss:=1.0958548784255981, Train Acc:=76.2, Evaluation Acc:=75.12\n",
            "Epoch=2 - Train Loss:=1736.3428955078125, Evaluation Loss:=0.9439160227775574, Train Acc:=78.70166666666667, Evaluation Acc:=76.9\n",
            "Epoch=3 - Train Loss:=1546.5439453125, Evaluation Loss:=0.9024804830551147, Train Acc:=79.36, Evaluation Acc:=77.46\n",
            "Epoch=4 - Train Loss:=1419.30712890625, Evaluation Loss:=0.8298833966255188, Train Acc:=80.44, Evaluation Acc:=78.42\n",
            "Epoch=5 - Train Loss:=1329.2786865234375, Evaluation Loss:=0.8344991207122803, Train Acc:=80.90833333333333, Evaluation Acc:=78.82000000000001\n",
            "Epoch=6 - Train Loss:=1255.6485595703125, Evaluation Loss:=0.7650352120399475, Train Acc:=81.76166666666667, Evaluation Acc:=79.82000000000001\n",
            "Epoch=7 - Train Loss:=1197.6431884765625, Evaluation Loss:=0.7264907360076904, Train Acc:=82.59666666666666, Evaluation Acc:=80.49\n",
            "Epoch=8 - Train Loss:=1150.015869140625, Evaluation Loss:=0.7131726741790771, Train Acc:=82.74666666666667, Evaluation Acc:=80.56\n",
            "Epoch=9 - Train Loss:=1109.244873046875, Evaluation Loss:=0.6850096583366394, Train Acc:=83.165, Evaluation Acc:=80.88\n",
            "Epoch=10 - Train Loss:=1069.9603271484375, Evaluation Loss:=0.6621462106704712, Train Acc:=83.8, Evaluation Acc:=81.32000000000001\n",
            "Epoch=11 - Train Loss:=1038.9862060546875, Evaluation Loss:=0.6604669690132141, Train Acc:=83.74666666666667, Evaluation Acc:=81.38\n",
            "Epoch=12 - Train Loss:=1010.3604125976562, Evaluation Loss:=0.6415004134178162, Train Acc:=84.19833333333332, Evaluation Acc:=81.77\n",
            "Epoch=13 - Train Loss:=983.4935302734375, Evaluation Loss:=0.6391514539718628, Train Acc:=84.10833333333333, Evaluation Acc:=81.6\n",
            "Epoch=14 - Train Loss:=961.2655639648438, Evaluation Loss:=0.6353997588157654, Train Acc:=84.37166666666667, Evaluation Acc:=81.83\n",
            "Epoch=15 - Train Loss:=938.4871826171875, Evaluation Loss:=0.6189153790473938, Train Acc:=84.75666666666667, Evaluation Acc:=82.55\n",
            "Epoch=16 - Train Loss:=919.13818359375, Evaluation Loss:=0.5992136001586914, Train Acc:=85.265, Evaluation Acc:=82.66\n",
            "Epoch=17 - Train Loss:=899.851318359375, Evaluation Loss:=0.600342333316803, Train Acc:=85.03166666666667, Evaluation Acc:=82.33\n",
            "Epoch=18 - Train Loss:=884.0699462890625, Evaluation Loss:=0.5928381681442261, Train Acc:=85.245, Evaluation Acc:=82.37\n",
            "Epoch=19 - Train Loss:=867.9212036132812, Evaluation Loss:=0.5850599408149719, Train Acc:=85.65833333333333, Evaluation Acc:=82.77\n",
            "Epoch=0 - Train Loss:=3988.39404296875, Evaluation Loss:=1.3395718336105347, Train Acc:=72.91666666666666, Evaluation Acc:=72.19\n",
            "Epoch=1 - Train Loss:=2134.176025390625, Evaluation Loss:=1.0814263820648193, Train Acc:=76.69500000000001, Evaluation Acc:=76.02\n",
            "Epoch=2 - Train Loss:=1778.5609130859375, Evaluation Loss:=0.9409416913986206, Train Acc:=78.86166666666666, Evaluation Acc:=77.36\n",
            "Epoch=3 - Train Loss:=1577.4830322265625, Evaluation Loss:=0.8684115409851074, Train Acc:=80.08166666666666, Evaluation Acc:=78.9\n",
            "Epoch=4 - Train Loss:=1449.0594482421875, Evaluation Loss:=0.845390796661377, Train Acc:=79.935, Evaluation Acc:=78.62\n",
            "Epoch=5 - Train Loss:=1352.468017578125, Evaluation Loss:=0.7866591811180115, Train Acc:=81.55833333333334, Evaluation Acc:=79.96\n",
            "Epoch=6 - Train Loss:=1276.1614990234375, Evaluation Loss:=0.7476992011070251, Train Acc:=82.52166666666668, Evaluation Acc:=80.82000000000001\n",
            "Epoch=7 - Train Loss:=1216.1251220703125, Evaluation Loss:=0.7266701459884644, Train Acc:=82.61500000000001, Evaluation Acc:=80.71000000000001\n",
            "Epoch=8 - Train Loss:=1161.9398193359375, Evaluation Loss:=0.7199647426605225, Train Acc:=82.78166666666667, Evaluation Acc:=80.88\n",
            "Epoch=9 - Train Loss:=1119.96142578125, Evaluation Loss:=0.7333135604858398, Train Acc:=82.64, Evaluation Acc:=80.78999999999999\n",
            "Epoch=10 - Train Loss:=1087.0750732421875, Evaluation Loss:=0.6927947998046875, Train Acc:=83.34833333333333, Evaluation Acc:=81.26\n",
            "Epoch=11 - Train Loss:=1053.9312744140625, Evaluation Loss:=0.6640772223472595, Train Acc:=83.94, Evaluation Acc:=81.89999999999999\n",
            "Epoch=12 - Train Loss:=1019.7448120117188, Evaluation Loss:=0.6484286189079285, Train Acc:=84.27333333333334, Evaluation Acc:=82.32000000000001\n",
            "Epoch=13 - Train Loss:=995.139404296875, Evaluation Loss:=0.6420896649360657, Train Acc:=84.63833333333334, Evaluation Acc:=82.26\n",
            "Epoch=14 - Train Loss:=969.5130004882812, Evaluation Loss:=0.6282803416252136, Train Acc:=84.73666666666666, Evaluation Acc:=82.34\n",
            "Epoch=15 - Train Loss:=948.946533203125, Evaluation Loss:=0.6243467926979065, Train Acc:=84.89, Evaluation Acc:=82.72\n",
            "Epoch=16 - Train Loss:=928.7794189453125, Evaluation Loss:=0.6156620383262634, Train Acc:=85.16166666666666, Evaluation Acc:=82.8\n",
            "Epoch=17 - Train Loss:=909.9826049804688, Evaluation Loss:=0.6062113046646118, Train Acc:=85.33666666666667, Evaluation Acc:=82.89\n",
            "Epoch=18 - Train Loss:=891.0579833984375, Evaluation Loss:=0.6101040840148926, Train Acc:=85.43333333333332, Evaluation Acc:=82.91\n",
            "Epoch=19 - Train Loss:=878.1791381835938, Evaluation Loss:=0.594290018081665, Train Acc:=85.79333333333334, Evaluation Acc:=83.09\n",
            "Epoch=0 - Train Loss:=3661.4052734375, Evaluation Loss:=1.258967399597168, Train Acc:=73.78833333333333, Evaluation Acc:=72.86\n",
            "Epoch=1 - Train Loss:=2029.4464111328125, Evaluation Loss:=1.033449649810791, Train Acc:=77.03999999999999, Evaluation Acc:=76.03\n",
            "Epoch=2 - Train Loss:=1694.35791015625, Evaluation Loss:=0.9090015888214111, Train Acc:=78.90333333333334, Evaluation Acc:=78.06\n",
            "Epoch=3 - Train Loss:=1514.4234619140625, Evaluation Loss:=0.8400633931159973, Train Acc:=80.11833333333334, Evaluation Acc:=78.93\n",
            "Epoch=4 - Train Loss:=1394.6788330078125, Evaluation Loss:=0.8007102608680725, Train Acc:=81.17833333333333, Evaluation Acc:=79.83\n",
            "Epoch=5 - Train Loss:=1306.2281494140625, Evaluation Loss:=0.7672573924064636, Train Acc:=81.68333333333334, Evaluation Acc:=80.58\n",
            "Epoch=6 - Train Loss:=1235.6842041015625, Evaluation Loss:=0.7349834442138672, Train Acc:=82.37833333333333, Evaluation Acc:=81.13\n",
            "Epoch=7 - Train Loss:=1178.9805908203125, Evaluation Loss:=0.7159363031387329, Train Acc:=82.60166666666666, Evaluation Acc:=80.9\n",
            "Epoch=8 - Train Loss:=1131.2119140625, Evaluation Loss:=0.6864972114562988, Train Acc:=83.32666666666667, Evaluation Acc:=81.47999999999999\n",
            "Epoch=9 - Train Loss:=1092.0245361328125, Evaluation Loss:=0.6766680479049683, Train Acc:=83.68166666666667, Evaluation Acc:=81.78\n",
            "Epoch=10 - Train Loss:=1059.025634765625, Evaluation Loss:=0.6592977046966553, Train Acc:=83.93333333333334, Evaluation Acc:=81.69\n",
            "Epoch=11 - Train Loss:=1027.0845947265625, Evaluation Loss:=0.6503492593765259, Train Acc:=83.93833333333333, Evaluation Acc:=81.83\n",
            "Epoch=12 - Train Loss:=995.9114379882812, Evaluation Loss:=0.6519149541854858, Train Acc:=83.99666666666667, Evaluation Acc:=82.02000000000001\n",
            "Epoch=13 - Train Loss:=972.3739624023438, Evaluation Loss:=0.6289018392562866, Train Acc:=84.625, Evaluation Acc:=82.66\n",
            "Epoch=14 - Train Loss:=951.2257690429688, Evaluation Loss:=0.6246488094329834, Train Acc:=84.56666666666666, Evaluation Acc:=82.42\n",
            "Epoch=15 - Train Loss:=928.78955078125, Evaluation Loss:=0.6068786382675171, Train Acc:=85.08833333333334, Evaluation Acc:=82.80999999999999\n",
            "Epoch=16 - Train Loss:=909.09814453125, Evaluation Loss:=0.6034055352210999, Train Acc:=84.95333333333333, Evaluation Acc:=82.63000000000001\n",
            "Epoch=17 - Train Loss:=892.3636474609375, Evaluation Loss:=0.589003324508667, Train Acc:=85.58833333333334, Evaluation Acc:=83.07\n",
            "Epoch=18 - Train Loss:=875.5423583984375, Evaluation Loss:=0.5857728123664856, Train Acc:=85.89666666666666, Evaluation Acc:=83.17\n",
            "Epoch=19 - Train Loss:=860.0817260742188, Evaluation Loss:=0.5866488218307495, Train Acc:=85.61, Evaluation Acc:=83.05\n",
            "Epoch=0 - Train Loss:=4110.92529296875, Evaluation Loss:=1.3481820821762085, Train Acc:=72.83500000000001, Evaluation Acc:=72.21\n",
            "Epoch=1 - Train Loss:=2192.724365234375, Evaluation Loss:=1.088886022567749, Train Acc:=76.08, Evaluation Acc:=75.06\n",
            "Epoch=2 - Train Loss:=1825.804931640625, Evaluation Loss:=0.9823313355445862, Train Acc:=78.21666666666667, Evaluation Acc:=77.0\n",
            "Epoch=3 - Train Loss:=1613.80419921875, Evaluation Loss:=0.8995496034622192, Train Acc:=79.00833333333334, Evaluation Acc:=77.63\n",
            "Epoch=4 - Train Loss:=1476.012451171875, Evaluation Loss:=0.8360681533813477, Train Acc:=80.60000000000001, Evaluation Acc:=79.0\n",
            "Epoch=5 - Train Loss:=1372.9093017578125, Evaluation Loss:=0.7965436577796936, Train Acc:=80.955, Evaluation Acc:=79.61\n",
            "Epoch=6 - Train Loss:=1291.2047119140625, Evaluation Loss:=0.7478795647621155, Train Acc:=82.07166666666666, Evaluation Acc:=80.25999999999999\n",
            "Epoch=7 - Train Loss:=1232.43994140625, Evaluation Loss:=0.7236588597297668, Train Acc:=82.66, Evaluation Acc:=80.67\n",
            "Epoch=8 - Train Loss:=1175.3988037109375, Evaluation Loss:=0.7194474339485168, Train Acc:=82.88333333333333, Evaluation Acc:=80.73\n",
            "Epoch=9 - Train Loss:=1132.5052490234375, Evaluation Loss:=0.6899536848068237, Train Acc:=83.28999999999999, Evaluation Acc:=81.17\n",
            "Epoch=10 - Train Loss:=1092.1878662109375, Evaluation Loss:=0.7204254865646362, Train Acc:=82.10833333333333, Evaluation Acc:=79.80000000000001\n",
            "Epoch=11 - Train Loss:=1056.3577880859375, Evaluation Loss:=0.6756925582885742, Train Acc:=83.44833333333334, Evaluation Acc:=81.15\n",
            "Epoch=12 - Train Loss:=1027.487548828125, Evaluation Loss:=0.6447471380233765, Train Acc:=84.55166666666668, Evaluation Acc:=82.07\n",
            "Epoch=13 - Train Loss:=999.5877075195312, Evaluation Loss:=0.6360687613487244, Train Acc:=84.42333333333333, Evaluation Acc:=82.08\n",
            "Epoch=14 - Train Loss:=972.58447265625, Evaluation Loss:=0.6317542791366577, Train Acc:=84.67666666666666, Evaluation Acc:=82.11\n",
            "Epoch=15 - Train Loss:=951.5257568359375, Evaluation Loss:=0.614875078201294, Train Acc:=85.1, Evaluation Acc:=82.46\n",
            "Epoch=16 - Train Loss:=930.1051025390625, Evaluation Loss:=0.6028121709823608, Train Acc:=85.28333333333333, Evaluation Acc:=82.6\n",
            "Epoch=17 - Train Loss:=911.2540893554688, Evaluation Loss:=0.6107943058013916, Train Acc:=85.31, Evaluation Acc:=82.52000000000001\n",
            "Epoch=18 - Train Loss:=892.14697265625, Evaluation Loss:=0.6019721627235413, Train Acc:=85.53833333333334, Evaluation Acc:=82.83\n",
            "Epoch=19 - Train Loss:=877.0279541015625, Evaluation Loss:=0.5977339148521423, Train Acc:=85.68666666666667, Evaluation Acc:=82.94\n",
            "Epoch=0 - Train Loss:=3848.22509765625, Evaluation Loss:=1.4357717037200928, Train Acc:=72.66333333333334, Evaluation Acc:=71.32\n",
            "Epoch=1 - Train Loss:=2187.156494140625, Evaluation Loss:=1.1448609828948975, Train Acc:=76.19666666666667, Evaluation Acc:=74.47\n",
            "Epoch=2 - Train Loss:=1824.08544921875, Evaluation Loss:=1.0314959287643433, Train Acc:=78.23166666666667, Evaluation Acc:=76.5\n",
            "Epoch=3 - Train Loss:=1622.4644775390625, Evaluation Loss:=0.9364190697669983, Train Acc:=79.71166666666667, Evaluation Acc:=78.07\n",
            "Epoch=4 - Train Loss:=1484.6409912109375, Evaluation Loss:=0.8672624826431274, Train Acc:=80.84333333333333, Evaluation Acc:=78.46\n",
            "Epoch=5 - Train Loss:=1383.69482421875, Evaluation Loss:=0.8396605253219604, Train Acc:=81.24166666666667, Evaluation Acc:=79.05\n",
            "Epoch=6 - Train Loss:=1303.469482421875, Evaluation Loss:=0.7985738515853882, Train Acc:=82.01166666666667, Evaluation Acc:=79.51\n",
            "Epoch=7 - Train Loss:=1237.753662109375, Evaluation Loss:=0.7608938217163086, Train Acc:=82.785, Evaluation Acc:=80.47\n",
            "Epoch=8 - Train Loss:=1182.349853515625, Evaluation Loss:=0.7396103739738464, Train Acc:=83.11833333333334, Evaluation Acc:=80.55\n",
            "Epoch=9 - Train Loss:=1137.3267822265625, Evaluation Loss:=0.712865948677063, Train Acc:=83.45666666666666, Evaluation Acc:=80.93\n",
            "Epoch=10 - Train Loss:=1099.9677734375, Evaluation Loss:=0.7091363668441772, Train Acc:=83.50166666666667, Evaluation Acc:=81.33\n",
            "Epoch=11 - Train Loss:=1063.5072021484375, Evaluation Loss:=0.6898762583732605, Train Acc:=83.82, Evaluation Acc:=81.24\n",
            "Epoch=12 - Train Loss:=1034.0455322265625, Evaluation Loss:=0.6727730631828308, Train Acc:=84.36, Evaluation Acc:=82.17999999999999\n",
            "Epoch=13 - Train Loss:=1005.1932373046875, Evaluation Loss:=0.6528404355049133, Train Acc:=84.82166666666666, Evaluation Acc:=82.16\n",
            "Epoch=14 - Train Loss:=980.6610717773438, Evaluation Loss:=0.6514720916748047, Train Acc:=84.78999999999999, Evaluation Acc:=81.96\n",
            "Epoch=15 - Train Loss:=957.5474853515625, Evaluation Loss:=0.6327924728393555, Train Acc:=85.17666666666666, Evaluation Acc:=82.38\n",
            "Epoch=16 - Train Loss:=939.5819091796875, Evaluation Loss:=0.6236269474029541, Train Acc:=85.48833333333333, Evaluation Acc:=82.49\n",
            "Epoch=17 - Train Loss:=917.2216796875, Evaluation Loss:=0.6382935047149658, Train Acc:=84.485, Evaluation Acc:=81.62\n",
            "Epoch=18 - Train Loss:=899.0689697265625, Evaluation Loss:=0.6070554852485657, Train Acc:=85.78333333333333, Evaluation Acc:=82.62\n",
            "Epoch=19 - Train Loss:=884.3130493164062, Evaluation Loss:=0.6003769636154175, Train Acc:=85.97666666666667, Evaluation Acc:=82.94\n",
            "Epoch=0 - Train Loss:=3921.079833984375, Evaluation Loss:=1.2904382944107056, Train Acc:=73.265, Evaluation Acc:=72.99\n",
            "Epoch=1 - Train Loss:=2078.7939453125, Evaluation Loss:=1.048746109008789, Train Acc:=77.35, Evaluation Acc:=76.38000000000001\n",
            "Epoch=2 - Train Loss:=1731.9130859375, Evaluation Loss:=0.9318361282348633, Train Acc:=79.27166666666666, Evaluation Acc:=77.88000000000001\n",
            "Epoch=3 - Train Loss:=1544.030029296875, Evaluation Loss:=0.8714436292648315, Train Acc:=80.35833333333333, Evaluation Acc:=78.9\n",
            "Epoch=4 - Train Loss:=1420.584716796875, Evaluation Loss:=0.8472605347633362, Train Acc:=80.63833333333334, Evaluation Acc:=79.14999999999999\n",
            "Epoch=5 - Train Loss:=1328.4912109375, Evaluation Loss:=0.7876995205879211, Train Acc:=81.57166666666666, Evaluation Acc:=79.67\n",
            "Epoch=6 - Train Loss:=1258.3955078125, Evaluation Loss:=0.7548843622207642, Train Acc:=82.43, Evaluation Acc:=80.85\n",
            "Epoch=7 - Train Loss:=1198.8492431640625, Evaluation Loss:=0.737017810344696, Train Acc:=82.61333333333334, Evaluation Acc:=80.42\n",
            "Epoch=8 - Train Loss:=1150.2060546875, Evaluation Loss:=0.7231539487838745, Train Acc:=83.17, Evaluation Acc:=80.81\n",
            "Epoch=9 - Train Loss:=1105.3609619140625, Evaluation Loss:=0.694359302520752, Train Acc:=83.55166666666666, Evaluation Acc:=81.22\n",
            "Epoch=10 - Train Loss:=1071.243896484375, Evaluation Loss:=0.679253339767456, Train Acc:=83.91833333333332, Evaluation Acc:=81.3\n",
            "Epoch=11 - Train Loss:=1036.1280517578125, Evaluation Loss:=0.6725749969482422, Train Acc:=84.16833333333334, Evaluation Acc:=81.71000000000001\n",
            "Epoch=12 - Train Loss:=1006.92333984375, Evaluation Loss:=0.6584668755531311, Train Acc:=84.43166666666667, Evaluation Acc:=81.94\n",
            "Epoch=13 - Train Loss:=981.3199462890625, Evaluation Loss:=0.6492772698402405, Train Acc:=84.38833333333334, Evaluation Acc:=81.69999999999999\n",
            "Epoch=14 - Train Loss:=957.8944091796875, Evaluation Loss:=0.6306591629981995, Train Acc:=84.93, Evaluation Acc:=81.99\n",
            "Epoch=15 - Train Loss:=937.4559326171875, Evaluation Loss:=0.6238082051277161, Train Acc:=85.12666666666667, Evaluation Acc:=82.21000000000001\n",
            "Epoch=16 - Train Loss:=914.939697265625, Evaluation Loss:=0.630510151386261, Train Acc:=85.28166666666667, Evaluation Acc:=82.71\n",
            "Epoch=17 - Train Loss:=897.8305053710938, Evaluation Loss:=0.6194742321968079, Train Acc:=85.275, Evaluation Acc:=81.89999999999999\n",
            "Epoch=18 - Train Loss:=880.2343139648438, Evaluation Loss:=0.6145504713058472, Train Acc:=85.43833333333333, Evaluation Acc:=82.36\n",
            "Epoch=19 - Train Loss:=865.6175537109375, Evaluation Loss:=0.6013844013214111, Train Acc:=85.68, Evaluation Acc:=82.37\n",
            "Epoch=0 - Train Loss:=4089.47412109375, Evaluation Loss:=1.3248367309570312, Train Acc:=72.82666666666667, Evaluation Acc:=72.36\n",
            "Epoch=1 - Train Loss:=2174.6845703125, Evaluation Loss:=1.0991339683532715, Train Acc:=75.81166666666667, Evaluation Acc:=75.19\n",
            "Epoch=2 - Train Loss:=1804.2802734375, Evaluation Loss:=0.946459174156189, Train Acc:=78.31666666666666, Evaluation Acc:=77.83\n",
            "Epoch=3 - Train Loss:=1600.6900634765625, Evaluation Loss:=0.8867493867874146, Train Acc:=79.61833333333334, Evaluation Acc:=78.86\n",
            "Epoch=4 - Train Loss:=1466.826416015625, Evaluation Loss:=0.839134156703949, Train Acc:=80.39833333333334, Evaluation Acc:=79.33\n",
            "Epoch=5 - Train Loss:=1365.414794921875, Evaluation Loss:=0.7942097187042236, Train Acc:=80.93333333333334, Evaluation Acc:=79.42\n",
            "Epoch=6 - Train Loss:=1288.8892822265625, Evaluation Loss:=0.7486891746520996, Train Acc:=81.98166666666667, Evaluation Acc:=80.71000000000001\n",
            "Epoch=7 - Train Loss:=1222.8662109375, Evaluation Loss:=0.7607811093330383, Train Acc:=81.94666666666667, Evaluation Acc:=80.67999999999999\n",
            "Epoch=8 - Train Loss:=1170.57666015625, Evaluation Loss:=0.7272862195968628, Train Acc:=82.255, Evaluation Acc:=80.78999999999999\n",
            "Epoch=9 - Train Loss:=1128.51806640625, Evaluation Loss:=0.6898179650306702, Train Acc:=83.465, Evaluation Acc:=81.47\n",
            "Epoch=10 - Train Loss:=1087.779296875, Evaluation Loss:=0.7014389634132385, Train Acc:=82.88499999999999, Evaluation Acc:=80.73\n",
            "Epoch=11 - Train Loss:=1052.4134521484375, Evaluation Loss:=0.6537034511566162, Train Acc:=84.175, Evaluation Acc:=82.08\n",
            "Epoch=12 - Train Loss:=1022.9987182617188, Evaluation Loss:=0.6609910130500793, Train Acc:=84.10166666666666, Evaluation Acc:=82.47\n",
            "Epoch=13 - Train Loss:=996.1671752929688, Evaluation Loss:=0.6439595222473145, Train Acc:=84.23833333333334, Evaluation Acc:=81.67\n",
            "Epoch=14 - Train Loss:=971.7005615234375, Evaluation Loss:=0.6304839849472046, Train Acc:=84.91166666666666, Evaluation Acc:=82.28999999999999\n",
            "Epoch=15 - Train Loss:=950.3591918945312, Evaluation Loss:=0.6176676750183105, Train Acc:=85.01333333333334, Evaluation Acc:=82.57\n",
            "Epoch=16 - Train Loss:=930.47412109375, Evaluation Loss:=0.6050900220870972, Train Acc:=85.345, Evaluation Acc:=82.69999999999999\n",
            "Epoch=17 - Train Loss:=909.9763793945312, Evaluation Loss:=0.6107349395751953, Train Acc:=85.01666666666667, Evaluation Acc:=82.16\n",
            "Epoch=18 - Train Loss:=889.6240234375, Evaluation Loss:=0.5944000482559204, Train Acc:=85.72333333333333, Evaluation Acc:=82.8\n",
            "Epoch=19 - Train Loss:=874.8129272460938, Evaluation Loss:=0.5953367948532104, Train Acc:=85.57000000000001, Evaluation Acc:=82.59\n",
            "Epoch=0 - Train Loss:=3889.094970703125, Evaluation Loss:=1.3466137647628784, Train Acc:=73.55666666666667, Evaluation Acc:=72.08\n",
            "Epoch=1 - Train Loss:=2123.1796875, Evaluation Loss:=1.161943793296814, Train Acc:=75.02499999999999, Evaluation Acc:=73.69\n",
            "Epoch=2 - Train Loss:=1771.136962890625, Evaluation Loss:=0.9803249835968018, Train Acc:=78.48, Evaluation Acc:=76.92\n",
            "Epoch=3 - Train Loss:=1572.41064453125, Evaluation Loss:=0.8848972916603088, Train Acc:=80.04166666666667, Evaluation Acc:=78.14999999999999\n",
            "Epoch=4 - Train Loss:=1441.2001953125, Evaluation Loss:=0.8309970498085022, Train Acc:=80.91333333333334, Evaluation Acc:=79.05\n",
            "Epoch=5 - Train Loss:=1345.82470703125, Evaluation Loss:=0.8066647052764893, Train Acc:=81.315, Evaluation Acc:=78.9\n",
            "Epoch=6 - Train Loss:=1270.5430908203125, Evaluation Loss:=0.7548454403877258, Train Acc:=82.485, Evaluation Acc:=80.15\n",
            "Epoch=7 - Train Loss:=1207.420166015625, Evaluation Loss:=0.7524958252906799, Train Acc:=82.44833333333334, Evaluation Acc:=80.11\n",
            "Epoch=8 - Train Loss:=1159.03759765625, Evaluation Loss:=0.7337509393692017, Train Acc:=82.69999999999999, Evaluation Acc:=80.23\n",
            "Epoch=9 - Train Loss:=1110.30712890625, Evaluation Loss:=0.7011532783508301, Train Acc:=83.45333333333333, Evaluation Acc:=80.76\n",
            "Epoch=10 - Train Loss:=1074.1973876953125, Evaluation Loss:=0.6917873024940491, Train Acc:=83.39833333333333, Evaluation Acc:=80.92\n",
            "Epoch=11 - Train Loss:=1042.6280517578125, Evaluation Loss:=0.6647782325744629, Train Acc:=84.05833333333334, Evaluation Acc:=81.39\n",
            "Epoch=12 - Train Loss:=1012.0777587890625, Evaluation Loss:=0.6540809273719788, Train Acc:=84.41333333333333, Evaluation Acc:=81.58999999999999\n",
            "Epoch=13 - Train Loss:=984.8262939453125, Evaluation Loss:=0.6571208238601685, Train Acc:=84.17333333333333, Evaluation Acc:=81.08999999999999\n",
            "Epoch=14 - Train Loss:=960.0654907226562, Evaluation Loss:=0.6315326690673828, Train Acc:=84.93166666666667, Evaluation Acc:=81.83\n",
            "Epoch=15 - Train Loss:=938.1436767578125, Evaluation Loss:=0.6546446084976196, Train Acc:=84.37833333333333, Evaluation Acc:=81.47999999999999\n",
            "Epoch=16 - Train Loss:=917.901611328125, Evaluation Loss:=0.6180480718612671, Train Acc:=85.35666666666667, Evaluation Acc:=82.21000000000001\n",
            "Epoch=17 - Train Loss:=899.5003662109375, Evaluation Loss:=0.6304481625556946, Train Acc:=85.06166666666667, Evaluation Acc:=81.67999999999999\n",
            "Epoch=18 - Train Loss:=882.6109008789062, Evaluation Loss:=0.6047442555427551, Train Acc:=85.55333333333334, Evaluation Acc:=82.28999999999999\n",
            "Epoch=19 - Train Loss:=866.8316650390625, Evaluation Loss:=0.6105015873908997, Train Acc:=85.41666666666666, Evaluation Acc:=81.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "yTEMDUnpfow9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "d32f2731-00f0-40af-b12e-577b242c3331"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 82.71300000000001, and the variance is 0.10582099999999946\n",
            "the average running time for one epoch is: 16.031194831132886\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7fec3c2ed690>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7fec3c2f4750>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c2f4c90>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7fec3c27d790>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7fec3c27d250>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7fec3c2edc90>,\n",
              "  <matplotlib.lines.Line2D at 0x7fec3c2f4210>]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model8: with dropout and L2 regularization + SGD optimizer"
      ],
      "metadata": {
        "id": "mqnBm_yj8njX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "optimizer = \"SGD\"\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "learning_rate = 10e-5\n",
        "dropout = 0.1\n",
        "beta = 0.001\n",
        "epochs = 20\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i+8)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"fashion_mnist\")\n",
        "\n",
        "    model8 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer = optimizer)\n",
        "    train_losses, test_losses, train_accuracy, test_accuracy, _epoch_times = train(model8, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs,\n",
        "                                                                 dropout_rate=dropout,\n",
        "                                                                 beta=beta)\n",
        "    acc = test(model8, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "luWqg4JD86UX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a683fe6a-3ab3-4f42-c803-491b842fbdfb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=33756.90625, Evaluation Loss:=1.4648934602737427, Train Acc:=75.15833333333333, Evaluation Acc:=74.09\n",
            "Epoch=1 - Train Loss:=30323.48046875, Evaluation Loss:=1.2087197303771973, Train Acc:=77.71166666666667, Evaluation Acc:=76.22\n",
            "Epoch=2 - Train Loss:=29770.123046875, Evaluation Loss:=1.0429911613464355, Train Acc:=79.53333333333333, Evaluation Acc:=78.21000000000001\n",
            "Epoch=3 - Train Loss:=29490.4296875, Evaluation Loss:=0.9679412245750427, Train Acc:=80.54833333333333, Evaluation Acc:=79.10000000000001\n",
            "Epoch=4 - Train Loss:=29272.873046875, Evaluation Loss:=0.9241588115692139, Train Acc:=80.945, Evaluation Acc:=79.23\n",
            "Epoch=5 - Train Loss:=29141.552734375, Evaluation Loss:=0.8637877106666565, Train Acc:=81.53333333333333, Evaluation Acc:=79.95\n",
            "Epoch=6 - Train Loss:=29005.37890625, Evaluation Loss:=0.8473775386810303, Train Acc:=81.66166666666666, Evaluation Acc:=80.01\n",
            "Epoch=7 - Train Loss:=28898.71484375, Evaluation Loss:=0.7982996702194214, Train Acc:=82.41833333333334, Evaluation Acc:=80.63\n",
            "Epoch=8 - Train Loss:=28805.263671875, Evaluation Loss:=0.7874258756637573, Train Acc:=82.52333333333334, Evaluation Acc:=80.78\n",
            "Epoch=9 - Train Loss:=28750.62890625, Evaluation Loss:=0.7368187308311462, Train Acc:=83.60333333333332, Evaluation Acc:=81.69999999999999\n",
            "Epoch=10 - Train Loss:=28675.689453125, Evaluation Loss:=0.7637503147125244, Train Acc:=82.64166666666667, Evaluation Acc:=80.76\n",
            "Epoch=11 - Train Loss:=28624.8046875, Evaluation Loss:=0.7194646000862122, Train Acc:=83.51666666666667, Evaluation Acc:=81.47\n",
            "Epoch=12 - Train Loss:=28567.267578125, Evaluation Loss:=0.7092629075050354, Train Acc:=83.72500000000001, Evaluation Acc:=81.42\n",
            "Epoch=13 - Train Loss:=28498.869140625, Evaluation Loss:=0.6889169216156006, Train Acc:=83.97166666666666, Evaluation Acc:=81.92\n",
            "Epoch=14 - Train Loss:=28462.134765625, Evaluation Loss:=0.697006344795227, Train Acc:=83.73833333333334, Evaluation Acc:=81.32000000000001\n",
            "Epoch=15 - Train Loss:=28425.689453125, Evaluation Loss:=0.6898202300071716, Train Acc:=83.74000000000001, Evaluation Acc:=81.42\n",
            "Epoch=16 - Train Loss:=28380.31640625, Evaluation Loss:=0.6548346281051636, Train Acc:=84.59333333333333, Evaluation Acc:=82.14\n",
            "Epoch=17 - Train Loss:=28352.357421875, Evaluation Loss:=0.633367657661438, Train Acc:=84.93833333333333, Evaluation Acc:=82.38\n",
            "Epoch=18 - Train Loss:=28320.791015625, Evaluation Loss:=0.6471142768859863, Train Acc:=84.57333333333334, Evaluation Acc:=82.1\n",
            "Epoch=19 - Train Loss:=28271.49609375, Evaluation Loss:=0.6317139863967896, Train Acc:=84.74666666666667, Evaluation Acc:=82.41000000000001\n",
            "Epoch=0 - Train Loss:=34272.6875, Evaluation Loss:=1.5608069896697998, Train Acc:=73.78333333333333, Evaluation Acc:=73.03\n",
            "Epoch=1 - Train Loss:=30401.875, Evaluation Loss:=1.4346498250961304, Train Acc:=74.11666666666666, Evaluation Acc:=73.00999999999999\n",
            "Epoch=2 - Train Loss:=29799.50390625, Evaluation Loss:=1.2385824918746948, Train Acc:=76.47666666666667, Evaluation Acc:=75.46000000000001\n",
            "Epoch=3 - Train Loss:=29507.142578125, Evaluation Loss:=1.1721962690353394, Train Acc:=76.99833333333333, Evaluation Acc:=75.84\n",
            "Epoch=4 - Train Loss:=29280.544921875, Evaluation Loss:=1.1722400188446045, Train Acc:=76.57166666666667, Evaluation Acc:=75.24\n",
            "Epoch=5 - Train Loss:=29142.173828125, Evaluation Loss:=0.980426549911499, Train Acc:=79.34, Evaluation Acc:=77.86\n",
            "Epoch=6 - Train Loss:=29006.513671875, Evaluation Loss:=0.8971734642982483, Train Acc:=80.59333333333333, Evaluation Acc:=78.96\n",
            "Epoch=7 - Train Loss:=28915.509765625, Evaluation Loss:=0.883618175983429, Train Acc:=80.91833333333334, Evaluation Acc:=79.62\n",
            "Epoch=8 - Train Loss:=28802.75390625, Evaluation Loss:=0.852896511554718, Train Acc:=81.07333333333332, Evaluation Acc:=79.3\n",
            "Epoch=9 - Train Loss:=28720.392578125, Evaluation Loss:=0.9236844778060913, Train Acc:=79.45166666666667, Evaluation Acc:=77.66999999999999\n",
            "Epoch=10 - Train Loss:=28669.2890625, Evaluation Loss:=0.8233425617218018, Train Acc:=81.3, Evaluation Acc:=79.43\n",
            "Epoch=11 - Train Loss:=28591.04296875, Evaluation Loss:=0.8658474683761597, Train Acc:=80.185, Evaluation Acc:=78.29\n",
            "Epoch=12 - Train Loss:=28558.736328125, Evaluation Loss:=0.8008043169975281, Train Acc:=81.55333333333333, Evaluation Acc:=79.23\n",
            "Epoch=13 - Train Loss:=28496.64453125, Evaluation Loss:=0.806153416633606, Train Acc:=81.15333333333334, Evaluation Acc:=78.92\n",
            "Epoch=14 - Train Loss:=28441.296875, Evaluation Loss:=0.8253268599510193, Train Acc:=80.79333333333332, Evaluation Acc:=79.05\n",
            "Epoch=15 - Train Loss:=28422.08203125, Evaluation Loss:=0.6997945308685303, Train Acc:=83.38333333333333, Evaluation Acc:=81.07\n",
            "Epoch=16 - Train Loss:=28362.15625, Evaluation Loss:=0.7380024194717407, Train Acc:=82.12666666666667, Evaluation Acc:=80.23\n",
            "Epoch=17 - Train Loss:=28336.123046875, Evaluation Loss:=0.7363293170928955, Train Acc:=82.07666666666667, Evaluation Acc:=80.23\n",
            "Epoch=18 - Train Loss:=28300.9609375, Evaluation Loss:=0.7163656949996948, Train Acc:=82.645, Evaluation Acc:=80.62\n",
            "Epoch=19 - Train Loss:=28265.216796875, Evaluation Loss:=0.7293756604194641, Train Acc:=82.17333333333333, Evaluation Acc:=80.13\n",
            "Epoch=0 - Train Loss:=34530.48046875, Evaluation Loss:=1.5963859558105469, Train Acc:=73.855, Evaluation Acc:=72.71\n",
            "Epoch=1 - Train Loss:=30451.125, Evaluation Loss:=1.2063363790512085, Train Acc:=77.69500000000001, Evaluation Acc:=76.5\n",
            "Epoch=2 - Train Loss:=29836.82421875, Evaluation Loss:=1.0647358894348145, Train Acc:=79.20666666666666, Evaluation Acc:=78.12\n",
            "Epoch=3 - Train Loss:=29523.5, Evaluation Loss:=1.0296462774276733, Train Acc:=79.525, Evaluation Acc:=78.01\n",
            "Epoch=4 - Train Loss:=29332.1796875, Evaluation Loss:=0.9732379913330078, Train Acc:=79.93333333333334, Evaluation Acc:=78.41\n",
            "Epoch=5 - Train Loss:=29186.392578125, Evaluation Loss:=0.8765754699707031, Train Acc:=81.475, Evaluation Acc:=79.92\n",
            "Epoch=6 - Train Loss:=29036.53125, Evaluation Loss:=0.8655271530151367, Train Acc:=81.56, Evaluation Acc:=79.9\n",
            "Epoch=7 - Train Loss:=28942.0, Evaluation Loss:=0.8036924600601196, Train Acc:=82.36833333333334, Evaluation Acc:=80.58999999999999\n",
            "Epoch=8 - Train Loss:=28848.833984375, Evaluation Loss:=0.8063968420028687, Train Acc:=82.38, Evaluation Acc:=80.58\n",
            "Epoch=9 - Train Loss:=28786.38671875, Evaluation Loss:=0.7382335662841797, Train Acc:=83.55, Evaluation Acc:=81.83\n",
            "Epoch=10 - Train Loss:=28721.88671875, Evaluation Loss:=0.7370581030845642, Train Acc:=83.36, Evaluation Acc:=81.28999999999999\n",
            "Epoch=11 - Train Loss:=28649.77734375, Evaluation Loss:=0.7270147204399109, Train Acc:=83.275, Evaluation Acc:=81.14\n",
            "Epoch=12 - Train Loss:=28603.08984375, Evaluation Loss:=0.7656773924827576, Train Acc:=82.485, Evaluation Acc:=80.53\n",
            "Epoch=13 - Train Loss:=28558.419921875, Evaluation Loss:=0.6937246322631836, Train Acc:=83.92166666666667, Evaluation Acc:=81.72\n",
            "Epoch=14 - Train Loss:=28520.46484375, Evaluation Loss:=0.6889477372169495, Train Acc:=83.77, Evaluation Acc:=81.47\n",
            "Epoch=15 - Train Loss:=28466.087890625, Evaluation Loss:=0.6893914937973022, Train Acc:=83.79166666666666, Evaluation Acc:=81.64\n",
            "Epoch=16 - Train Loss:=28435.298828125, Evaluation Loss:=0.6540941596031189, Train Acc:=84.48666666666666, Evaluation Acc:=82.39999999999999\n",
            "Epoch=17 - Train Loss:=28392.255859375, Evaluation Loss:=0.6572588086128235, Train Acc:=84.175, Evaluation Acc:=82.07\n",
            "Epoch=18 - Train Loss:=28350.4296875, Evaluation Loss:=0.6445925831794739, Train Acc:=84.425, Evaluation Acc:=82.58\n",
            "Epoch=19 - Train Loss:=28326.9453125, Evaluation Loss:=0.6321589946746826, Train Acc:=84.83500000000001, Evaluation Acc:=82.66\n",
            "Epoch=0 - Train Loss:=33783.56640625, Evaluation Loss:=1.398555874824524, Train Acc:=74.55166666666668, Evaluation Acc:=73.99\n",
            "Epoch=1 - Train Loss:=30251.30859375, Evaluation Loss:=1.1569186449050903, Train Acc:=77.49666666666667, Evaluation Acc:=76.25999999999999\n",
            "Epoch=2 - Train Loss:=29682.24609375, Evaluation Loss:=1.0149489641189575, Train Acc:=79.03666666666666, Evaluation Acc:=78.02\n",
            "Epoch=3 - Train Loss:=29386.265625, Evaluation Loss:=0.9507609605789185, Train Acc:=79.68666666666667, Evaluation Acc:=78.53999999999999\n",
            "Epoch=4 - Train Loss:=29170.849609375, Evaluation Loss:=0.8963638544082642, Train Acc:=80.49, Evaluation Acc:=79.11\n",
            "Epoch=5 - Train Loss:=29022.150390625, Evaluation Loss:=0.8487853407859802, Train Acc:=81.00500000000001, Evaluation Acc:=79.41\n",
            "Epoch=6 - Train Loss:=28898.73828125, Evaluation Loss:=0.8088791966438293, Train Acc:=81.82333333333334, Evaluation Acc:=80.0\n",
            "Epoch=7 - Train Loss:=28800.064453125, Evaluation Loss:=0.7893065214157104, Train Acc:=81.90666666666667, Evaluation Acc:=80.17999999999999\n",
            "Epoch=8 - Train Loss:=28740.462890625, Evaluation Loss:=0.7610461115837097, Train Acc:=82.42833333333334, Evaluation Acc:=80.31\n",
            "Epoch=9 - Train Loss:=28662.037109375, Evaluation Loss:=0.7150740623474121, Train Acc:=83.37666666666667, Evaluation Acc:=81.25\n",
            "Epoch=10 - Train Loss:=28595.37109375, Evaluation Loss:=0.7513736486434937, Train Acc:=82.19166666666666, Evaluation Acc:=80.08\n",
            "Epoch=11 - Train Loss:=28526.00390625, Evaluation Loss:=0.7331374883651733, Train Acc:=82.47, Evaluation Acc:=80.42\n",
            "Epoch=12 - Train Loss:=28494.7109375, Evaluation Loss:=0.689130425453186, Train Acc:=83.31333333333333, Evaluation Acc:=81.49\n",
            "Epoch=13 - Train Loss:=28424.015625, Evaluation Loss:=0.6920779347419739, Train Acc:=83.13166666666667, Evaluation Acc:=80.85\n",
            "Epoch=14 - Train Loss:=28394.630859375, Evaluation Loss:=0.658822774887085, Train Acc:=83.865, Evaluation Acc:=81.41000000000001\n",
            "Epoch=15 - Train Loss:=28346.345703125, Evaluation Loss:=0.7044971585273743, Train Acc:=82.73833333333333, Evaluation Acc:=80.52\n",
            "Epoch=16 - Train Loss:=28311.39453125, Evaluation Loss:=0.652295708656311, Train Acc:=83.85833333333333, Evaluation Acc:=81.39\n",
            "Epoch=17 - Train Loss:=28307.1953125, Evaluation Loss:=0.6464577913284302, Train Acc:=83.79666666666667, Evaluation Acc:=81.43\n",
            "Epoch=18 - Train Loss:=28247.77734375, Evaluation Loss:=0.6333890557289124, Train Acc:=84.11999999999999, Evaluation Acc:=81.95\n",
            "Epoch=19 - Train Loss:=28222.322265625, Evaluation Loss:=0.603011965751648, Train Acc:=84.94333333333334, Evaluation Acc:=82.41000000000001\n",
            "Epoch=0 - Train Loss:=34486.359375, Evaluation Loss:=1.783982276916504, Train Acc:=73.01666666666667, Evaluation Acc:=71.96000000000001\n",
            "Epoch=1 - Train Loss:=30579.82421875, Evaluation Loss:=1.3346220254898071, Train Acc:=76.61333333333333, Evaluation Acc:=75.58\n",
            "Epoch=2 - Train Loss:=29944.80078125, Evaluation Loss:=1.1875239610671997, Train Acc:=78.09333333333333, Evaluation Acc:=77.32\n",
            "Epoch=3 - Train Loss:=29607.67578125, Evaluation Loss:=1.0958757400512695, Train Acc:=78.82333333333334, Evaluation Acc:=77.69\n",
            "Epoch=4 - Train Loss:=29381.814453125, Evaluation Loss:=1.000570297241211, Train Acc:=80.26333333333334, Evaluation Acc:=79.01\n",
            "Epoch=5 - Train Loss:=29213.1015625, Evaluation Loss:=0.9367601275444031, Train Acc:=80.77499999999999, Evaluation Acc:=79.39\n",
            "Epoch=6 - Train Loss:=29081.986328125, Evaluation Loss:=0.9548842906951904, Train Acc:=80.46, Evaluation Acc:=79.38\n",
            "Epoch=7 - Train Loss:=28987.447265625, Evaluation Loss:=0.9049832820892334, Train Acc:=81.16666666666667, Evaluation Acc:=80.07\n",
            "Epoch=8 - Train Loss:=28882.974609375, Evaluation Loss:=0.8325538039207458, Train Acc:=81.95666666666666, Evaluation Acc:=80.76\n",
            "Epoch=9 - Train Loss:=28793.4765625, Evaluation Loss:=0.8197313547134399, Train Acc:=82.245, Evaluation Acc:=80.66\n",
            "Epoch=10 - Train Loss:=28730.00390625, Evaluation Loss:=0.782195508480072, Train Acc:=82.57, Evaluation Acc:=80.96\n",
            "Epoch=11 - Train Loss:=28668.55859375, Evaluation Loss:=0.8246952891349792, Train Acc:=81.825, Evaluation Acc:=80.14\n",
            "Epoch=12 - Train Loss:=28607.6484375, Evaluation Loss:=0.7852981686592102, Train Acc:=82.57, Evaluation Acc:=80.92\n",
            "Epoch=13 - Train Loss:=28549.62109375, Evaluation Loss:=0.7771052122116089, Train Acc:=82.43333333333334, Evaluation Acc:=81.21000000000001\n",
            "Epoch=14 - Train Loss:=28516.8046875, Evaluation Loss:=0.7780174612998962, Train Acc:=82.30333333333333, Evaluation Acc:=80.74\n",
            "Epoch=15 - Train Loss:=28469.966796875, Evaluation Loss:=0.7188214659690857, Train Acc:=83.40666666666667, Evaluation Acc:=81.93\n",
            "Epoch=16 - Train Loss:=28418.328125, Evaluation Loss:=0.7157382369041443, Train Acc:=83.20666666666666, Evaluation Acc:=81.78999999999999\n",
            "Epoch=17 - Train Loss:=28385.943359375, Evaluation Loss:=0.7179875373840332, Train Acc:=83.30833333333332, Evaluation Acc:=81.67999999999999\n",
            "Epoch=18 - Train Loss:=28349.435546875, Evaluation Loss:=0.729977548122406, Train Acc:=82.965, Evaluation Acc:=81.44\n",
            "Epoch=19 - Train Loss:=28317.251953125, Evaluation Loss:=0.6744154691696167, Train Acc:=83.985, Evaluation Acc:=82.17999999999999\n",
            "Epoch=0 - Train Loss:=34144.85546875, Evaluation Loss:=1.6497191190719604, Train Acc:=74.07000000000001, Evaluation Acc:=72.66\n",
            "Epoch=1 - Train Loss:=30470.65234375, Evaluation Loss:=1.2584989070892334, Train Acc:=77.08166666666668, Evaluation Acc:=75.78\n",
            "Epoch=2 - Train Loss:=29871.099609375, Evaluation Loss:=1.1442252397537231, Train Acc:=78.84666666666666, Evaluation Acc:=77.57\n",
            "Epoch=3 - Train Loss:=29557.505859375, Evaluation Loss:=1.1409896612167358, Train Acc:=78.02, Evaluation Acc:=76.97\n",
            "Epoch=4 - Train Loss:=29326.93359375, Evaluation Loss:=0.9727813601493835, Train Acc:=80.19666666666667, Evaluation Acc:=79.06\n",
            "Epoch=5 - Train Loss:=29192.646484375, Evaluation Loss:=0.9276090860366821, Train Acc:=80.71166666666667, Evaluation Acc:=79.59\n",
            "Epoch=6 - Train Loss:=29070.4140625, Evaluation Loss:=0.8810945153236389, Train Acc:=81.48166666666667, Evaluation Acc:=80.19\n",
            "Epoch=7 - Train Loss:=28958.91796875, Evaluation Loss:=0.8645658493041992, Train Acc:=81.76833333333333, Evaluation Acc:=80.19\n",
            "Epoch=8 - Train Loss:=28884.3359375, Evaluation Loss:=0.8519658446311951, Train Acc:=81.96333333333334, Evaluation Acc:=80.62\n",
            "Epoch=9 - Train Loss:=28800.427734375, Evaluation Loss:=0.8040850758552551, Train Acc:=82.38, Evaluation Acc:=80.63\n",
            "Epoch=10 - Train Loss:=28728.771484375, Evaluation Loss:=0.8085906505584717, Train Acc:=82.36, Evaluation Acc:=80.78\n",
            "Epoch=11 - Train Loss:=28665.39453125, Evaluation Loss:=0.7561262249946594, Train Acc:=83.21166666666666, Evaluation Acc:=81.37\n",
            "Epoch=12 - Train Loss:=28623.228515625, Evaluation Loss:=0.7683379650115967, Train Acc:=82.88666666666667, Evaluation Acc:=81.36\n",
            "Epoch=13 - Train Loss:=28558.21484375, Evaluation Loss:=0.7091665267944336, Train Acc:=84.00333333333333, Evaluation Acc:=82.21000000000001\n",
            "Epoch=14 - Train Loss:=28509.52734375, Evaluation Loss:=0.7647256255149841, Train Acc:=82.71, Evaluation Acc:=81.05\n",
            "Epoch=15 - Train Loss:=28470.71875, Evaluation Loss:=0.6987643241882324, Train Acc:=84.08, Evaluation Acc:=82.22\n",
            "Epoch=16 - Train Loss:=28413.24609375, Evaluation Loss:=0.6753628849983215, Train Acc:=84.33500000000001, Evaluation Acc:=82.48\n",
            "Epoch=17 - Train Loss:=28361.90625, Evaluation Loss:=0.7053442597389221, Train Acc:=83.23333333333333, Evaluation Acc:=81.12\n",
            "Epoch=18 - Train Loss:=28341.443359375, Evaluation Loss:=0.6763431429862976, Train Acc:=84.19333333333333, Evaluation Acc:=82.35\n",
            "Epoch=19 - Train Loss:=28318.341796875, Evaluation Loss:=0.6709334254264832, Train Acc:=84.13666666666667, Evaluation Acc:=82.24000000000001\n",
            "Epoch=0 - Train Loss:=34267.7578125, Evaluation Loss:=1.5580724477767944, Train Acc:=73.48333333333333, Evaluation Acc:=73.04\n",
            "Epoch=1 - Train Loss:=30327.181640625, Evaluation Loss:=1.342582106590271, Train Acc:=75.75833333333334, Evaluation Acc:=75.19\n",
            "Epoch=2 - Train Loss:=29753.373046875, Evaluation Loss:=1.0885238647460938, Train Acc:=78.255, Evaluation Acc:=77.62\n",
            "Epoch=3 - Train Loss:=29438.845703125, Evaluation Loss:=1.0631910562515259, Train Acc:=78.655, Evaluation Acc:=77.98\n",
            "Epoch=4 - Train Loss:=29272.818359375, Evaluation Loss:=0.9380771517753601, Train Acc:=80.32166666666667, Evaluation Acc:=79.22\n",
            "Epoch=5 - Train Loss:=29116.01171875, Evaluation Loss:=0.9168596863746643, Train Acc:=80.415, Evaluation Acc:=79.25999999999999\n",
            "Epoch=6 - Train Loss:=28968.826171875, Evaluation Loss:=0.8777757883071899, Train Acc:=80.91166666666668, Evaluation Acc:=79.73\n",
            "Epoch=7 - Train Loss:=28869.734375, Evaluation Loss:=0.8512939214706421, Train Acc:=81.09166666666667, Evaluation Acc:=79.86999999999999\n",
            "Epoch=8 - Train Loss:=28788.673828125, Evaluation Loss:=0.8242427706718445, Train Acc:=81.255, Evaluation Acc:=80.03\n",
            "Epoch=9 - Train Loss:=28711.873046875, Evaluation Loss:=0.7978277206420898, Train Acc:=81.49166666666666, Evaluation Acc:=80.13\n",
            "Epoch=10 - Train Loss:=28647.466796875, Evaluation Loss:=0.8074654340744019, Train Acc:=81.465, Evaluation Acc:=80.17999999999999\n",
            "Epoch=11 - Train Loss:=28587.875, Evaluation Loss:=0.7622317671775818, Train Acc:=82.25333333333333, Evaluation Acc:=81.07\n",
            "Epoch=12 - Train Loss:=28535.966796875, Evaluation Loss:=0.7880899310112, Train Acc:=81.58333333333333, Evaluation Acc:=80.17\n",
            "Epoch=13 - Train Loss:=28485.07421875, Evaluation Loss:=0.7360833287239075, Train Acc:=82.53333333333333, Evaluation Acc:=81.05\n",
            "Epoch=14 - Train Loss:=28443.044921875, Evaluation Loss:=0.7094612717628479, Train Acc:=82.74166666666667, Evaluation Acc:=81.3\n",
            "Epoch=15 - Train Loss:=28419.212890625, Evaluation Loss:=0.715362548828125, Train Acc:=82.57666666666667, Evaluation Acc:=81.28999999999999\n",
            "Epoch=16 - Train Loss:=28360.94921875, Evaluation Loss:=0.7016729712486267, Train Acc:=83.01833333333335, Evaluation Acc:=81.44\n",
            "Epoch=17 - Train Loss:=28327.283203125, Evaluation Loss:=0.672622799873352, Train Acc:=83.25166666666667, Evaluation Acc:=81.5\n",
            "Epoch=18 - Train Loss:=28293.125, Evaluation Loss:=0.7156229019165039, Train Acc:=82.32333333333334, Evaluation Acc:=80.67\n",
            "Epoch=19 - Train Loss:=28248.69921875, Evaluation Loss:=0.6373741030693054, Train Acc:=84.06166666666667, Evaluation Acc:=82.6\n",
            "Epoch=0 - Train Loss:=34822.25390625, Evaluation Loss:=1.6296265125274658, Train Acc:=73.07166666666667, Evaluation Acc:=72.3\n",
            "Epoch=1 - Train Loss:=30565.126953125, Evaluation Loss:=1.4508649110794067, Train Acc:=74.08, Evaluation Acc:=72.89999999999999\n",
            "Epoch=2 - Train Loss:=29905.99609375, Evaluation Loss:=1.28130042552948, Train Acc:=75.83333333333333, Evaluation Acc:=74.7\n",
            "Epoch=3 - Train Loss:=29584.060546875, Evaluation Loss:=1.1094902753829956, Train Acc:=78.15833333333333, Evaluation Acc:=77.14999999999999\n",
            "Epoch=4 - Train Loss:=29373.779296875, Evaluation Loss:=1.0451295375823975, Train Acc:=78.72833333333334, Evaluation Acc:=77.10000000000001\n",
            "Epoch=5 - Train Loss:=29222.6015625, Evaluation Loss:=1.0397487878799438, Train Acc:=78.35, Evaluation Acc:=77.01\n",
            "Epoch=6 - Train Loss:=29082.94140625, Evaluation Loss:=0.99361252784729, Train Acc:=78.74666666666667, Evaluation Acc:=77.05\n",
            "Epoch=7 - Train Loss:=28989.103515625, Evaluation Loss:=0.9266279339790344, Train Acc:=79.7, Evaluation Acc:=78.13\n",
            "Epoch=8 - Train Loss:=28875.93359375, Evaluation Loss:=0.8767390847206116, Train Acc:=80.48833333333333, Evaluation Acc:=78.93\n",
            "Epoch=9 - Train Loss:=28801.0, Evaluation Loss:=0.8403950333595276, Train Acc:=80.82000000000001, Evaluation Acc:=79.28\n",
            "Epoch=10 - Train Loss:=28737.65625, Evaluation Loss:=0.8931745290756226, Train Acc:=79.53333333333333, Evaluation Acc:=77.94\n",
            "Epoch=11 - Train Loss:=28668.232421875, Evaluation Loss:=0.8355267643928528, Train Acc:=80.31166666666667, Evaluation Acc:=78.86999999999999\n",
            "Epoch=12 - Train Loss:=28638.443359375, Evaluation Loss:=0.8267704844474792, Train Acc:=80.56, Evaluation Acc:=78.89\n",
            "Epoch=13 - Train Loss:=28583.576171875, Evaluation Loss:=0.9048458933830261, Train Acc:=78.74666666666667, Evaluation Acc:=76.77000000000001\n",
            "Epoch=14 - Train Loss:=28529.361328125, Evaluation Loss:=0.7845561504364014, Train Acc:=81.16, Evaluation Acc:=79.42\n",
            "Epoch=15 - Train Loss:=28489.544921875, Evaluation Loss:=0.7676244974136353, Train Acc:=81.28999999999999, Evaluation Acc:=79.36\n",
            "Epoch=16 - Train Loss:=28442.91796875, Evaluation Loss:=0.7622323632240295, Train Acc:=81.12666666666667, Evaluation Acc:=79.05\n",
            "Epoch=17 - Train Loss:=28405.244140625, Evaluation Loss:=0.7435700297355652, Train Acc:=81.42666666666668, Evaluation Acc:=79.52\n",
            "Epoch=18 - Train Loss:=28355.03515625, Evaluation Loss:=0.7619286179542542, Train Acc:=80.82833333333333, Evaluation Acc:=79.08\n",
            "Epoch=19 - Train Loss:=28329.5390625, Evaluation Loss:=0.7259261012077332, Train Acc:=81.61500000000001, Evaluation Acc:=79.75999999999999\n",
            "Epoch=0 - Train Loss:=34033.5625, Evaluation Loss:=1.5039153099060059, Train Acc:=74.40166666666667, Evaluation Acc:=72.97\n",
            "Epoch=1 - Train Loss:=30405.748046875, Evaluation Loss:=1.3634033203125, Train Acc:=74.495, Evaluation Acc:=73.02\n",
            "Epoch=2 - Train Loss:=29834.525390625, Evaluation Loss:=1.0922843217849731, Train Acc:=78.36166666666666, Evaluation Acc:=76.57000000000001\n",
            "Epoch=3 - Train Loss:=29502.2109375, Evaluation Loss:=1.0039410591125488, Train Acc:=79.39166666666667, Evaluation Acc:=77.64\n",
            "Epoch=4 - Train Loss:=29301.15234375, Evaluation Loss:=0.923911452293396, Train Acc:=80.65666666666667, Evaluation Acc:=78.78\n",
            "Epoch=5 - Train Loss:=29149.515625, Evaluation Loss:=0.9182775616645813, Train Acc:=80.28333333333333, Evaluation Acc:=78.03\n",
            "Epoch=6 - Train Loss:=29009.341796875, Evaluation Loss:=0.856924831867218, Train Acc:=81.17333333333333, Evaluation Acc:=78.86999999999999\n",
            "Epoch=7 - Train Loss:=28926.4453125, Evaluation Loss:=0.9034758806228638, Train Acc:=80.065, Evaluation Acc:=77.97\n",
            "Epoch=8 - Train Loss:=28826.505859375, Evaluation Loss:=0.8406746983528137, Train Acc:=81.19333333333333, Evaluation Acc:=78.95\n",
            "Epoch=9 - Train Loss:=28768.48046875, Evaluation Loss:=0.8139883875846863, Train Acc:=81.43833333333333, Evaluation Acc:=78.83\n",
            "Epoch=10 - Train Loss:=28702.00390625, Evaluation Loss:=0.7818450927734375, Train Acc:=81.97833333333332, Evaluation Acc:=79.5\n",
            "Epoch=11 - Train Loss:=28647.83203125, Evaluation Loss:=0.7464414834976196, Train Acc:=82.60333333333332, Evaluation Acc:=79.97\n",
            "Epoch=12 - Train Loss:=28594.720703125, Evaluation Loss:=0.7445242404937744, Train Acc:=82.715, Evaluation Acc:=80.08999999999999\n",
            "Epoch=13 - Train Loss:=28524.376953125, Evaluation Loss:=0.7718663811683655, Train Acc:=81.58666666666666, Evaluation Acc:=78.88\n",
            "Epoch=14 - Train Loss:=28483.51171875, Evaluation Loss:=0.7115578651428223, Train Acc:=83.16833333333334, Evaluation Acc:=80.54\n",
            "Epoch=15 - Train Loss:=28437.57421875, Evaluation Loss:=0.6983042359352112, Train Acc:=83.39333333333333, Evaluation Acc:=80.85\n",
            "Epoch=16 - Train Loss:=28411.533203125, Evaluation Loss:=0.6652499437332153, Train Acc:=84.205, Evaluation Acc:=81.58999999999999\n",
            "Epoch=17 - Train Loss:=28363.3046875, Evaluation Loss:=0.6764368414878845, Train Acc:=83.72833333333334, Evaluation Acc:=81.46\n",
            "Epoch=18 - Train Loss:=28342.958984375, Evaluation Loss:=0.7130271196365356, Train Acc:=82.515, Evaluation Acc:=80.15\n",
            "Epoch=19 - Train Loss:=28300.8515625, Evaluation Loss:=0.7452060580253601, Train Acc:=81.58166666666666, Evaluation Acc:=79.08\n",
            "Epoch=0 - Train Loss:=34802.72265625, Evaluation Loss:=1.391278624534607, Train Acc:=74.72, Evaluation Acc:=73.57000000000001\n",
            "Epoch=1 - Train Loss:=30429.12890625, Evaluation Loss:=1.13872230052948, Train Acc:=77.95333333333333, Evaluation Acc:=77.02\n",
            "Epoch=2 - Train Loss:=29755.091796875, Evaluation Loss:=1.013445496559143, Train Acc:=79.345, Evaluation Acc:=78.45\n",
            "Epoch=3 - Train Loss:=29451.984375, Evaluation Loss:=0.9540625214576721, Train Acc:=80.16499999999999, Evaluation Acc:=78.96\n",
            "Epoch=4 - Train Loss:=29254.380859375, Evaluation Loss:=0.9075967073440552, Train Acc:=80.81666666666666, Evaluation Acc:=79.60000000000001\n",
            "Epoch=5 - Train Loss:=29112.09375, Evaluation Loss:=0.8676861524581909, Train Acc:=81.05499999999999, Evaluation Acc:=79.59\n",
            "Epoch=6 - Train Loss:=29011.9140625, Evaluation Loss:=0.8039701581001282, Train Acc:=82.425, Evaluation Acc:=81.13\n",
            "Epoch=7 - Train Loss:=28911.92578125, Evaluation Loss:=0.7686567902565002, Train Acc:=82.52666666666667, Evaluation Acc:=80.89\n",
            "Epoch=8 - Train Loss:=28812.30078125, Evaluation Loss:=0.746558427810669, Train Acc:=82.99833333333333, Evaluation Acc:=81.37\n",
            "Epoch=9 - Train Loss:=28743.33203125, Evaluation Loss:=0.7416411638259888, Train Acc:=82.87666666666667, Evaluation Acc:=81.24\n",
            "Epoch=10 - Train Loss:=28677.576171875, Evaluation Loss:=0.7070363759994507, Train Acc:=83.84666666666666, Evaluation Acc:=82.19999999999999\n",
            "Epoch=11 - Train Loss:=28627.3046875, Evaluation Loss:=0.7290531992912292, Train Acc:=82.98833333333333, Evaluation Acc:=80.88\n",
            "Epoch=12 - Train Loss:=28582.408203125, Evaluation Loss:=0.702521562576294, Train Acc:=83.28999999999999, Evaluation Acc:=81.37\n",
            "Epoch=13 - Train Loss:=28535.060546875, Evaluation Loss:=0.6796330809593201, Train Acc:=83.98833333333333, Evaluation Acc:=81.82000000000001\n",
            "Epoch=14 - Train Loss:=28482.51171875, Evaluation Loss:=0.7005600929260254, Train Acc:=83.015, Evaluation Acc:=81.05\n",
            "Epoch=15 - Train Loss:=28447.33203125, Evaluation Loss:=0.6471953392028809, Train Acc:=84.32333333333332, Evaluation Acc:=82.17999999999999\n",
            "Epoch=16 - Train Loss:=28417.71484375, Evaluation Loss:=0.651533305644989, Train Acc:=84.26166666666667, Evaluation Acc:=82.06\n",
            "Epoch=17 - Train Loss:=28362.90234375, Evaluation Loss:=0.6508069634437561, Train Acc:=84.07833333333333, Evaluation Acc:=81.88\n",
            "Epoch=18 - Train Loss:=28321.416015625, Evaluation Loss:=0.6351645588874817, Train Acc:=84.28999999999999, Evaluation Acc:=82.24000000000001\n",
            "Epoch=19 - Train Loss:=28301.5390625, Evaluation Loss:=0.6174613833427429, Train Acc:=84.82166666666666, Evaluation Acc:=82.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "tEepAfYf16-2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "e1d64b6b-2b3c-4b3f-944c-bbc238fe377f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 81.608, and the variance is 1.7098160000000076\n",
            "the average running time for one epoch is: 24.38683134198189\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7f8dee341050>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f8dee34d110>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dee34d650>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7f8dee353150>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f8dee34dbd0>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7f8dee341650>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dee341b90>]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model9: without regularization + Adam optimizer"
      ],
      "metadata": {
        "id": "46Isl_7v86lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "optimizer = \"Adam\"\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "learning_rate = 10e-5\n",
        "epochs = 20\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i + 9)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"fashion_mnist\")\n",
        "\n",
        "    model9 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer = optimizer)\n",
        "    train_losses, test_losses, train_accuracy, test_accuracy, _epoch_times = train(model9, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs)\n",
        "    acc = test(model9, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "UXwFq0nC8_V9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4630d2-2134-4144-c2cb-692667e703b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=2727.406005859375, Evaluation Loss:=1.2817211151123047, Train Acc:=81.54166666666667, Evaluation Acc:=80.13\n",
            "Epoch=1 - Train Loss:=1791.2176513671875, Evaluation Loss:=1.3142421245574951, Train Acc:=79.30666666666667, Evaluation Acc:=76.9\n",
            "Epoch=2 - Train Loss:=1664.578857421875, Evaluation Loss:=0.9824911952018738, Train Acc:=84.92, Evaluation Acc:=82.46\n",
            "Epoch=3 - Train Loss:=1625.421875, Evaluation Loss:=0.9002737402915955, Train Acc:=86.53833333333333, Evaluation Acc:=84.34\n",
            "Epoch=4 - Train Loss:=1649.998291015625, Evaluation Loss:=0.9558119177818298, Train Acc:=86.45333333333333, Evaluation Acc:=83.72\n",
            "Epoch=5 - Train Loss:=1677.6854248046875, Evaluation Loss:=1.1325474977493286, Train Acc:=85.04833333333333, Evaluation Acc:=82.64\n",
            "Epoch=6 - Train Loss:=1750.032958984375, Evaluation Loss:=1.165761947631836, Train Acc:=86.45666666666668, Evaluation Acc:=84.46000000000001\n",
            "Epoch=7 - Train Loss:=1762.091796875, Evaluation Loss:=1.3647196292877197, Train Acc:=85.02499999999999, Evaluation Acc:=82.8\n",
            "Epoch=8 - Train Loss:=1835.2958984375, Evaluation Loss:=1.211416244506836, Train Acc:=87.91666666666667, Evaluation Acc:=85.5\n",
            "Epoch=9 - Train Loss:=1939.6055908203125, Evaluation Loss:=1.0366567373275757, Train Acc:=88.62333333333333, Evaluation Acc:=85.63\n",
            "Epoch=10 - Train Loss:=1977.857666015625, Evaluation Loss:=1.3718955516815186, Train Acc:=85.71, Evaluation Acc:=82.3\n",
            "Epoch=11 - Train Loss:=2037.3914794921875, Evaluation Loss:=1.197419285774231, Train Acc:=88.45333333333333, Evaluation Acc:=85.26\n",
            "Epoch=12 - Train Loss:=2049.23193359375, Evaluation Loss:=1.2900711297988892, Train Acc:=87.94333333333333, Evaluation Acc:=85.35000000000001\n",
            "Epoch=13 - Train Loss:=2099.513671875, Evaluation Loss:=1.7164562940597534, Train Acc:=85.87833333333333, Evaluation Acc:=83.26\n",
            "Epoch=14 - Train Loss:=2155.844482421875, Evaluation Loss:=1.4094542264938354, Train Acc:=87.83833333333332, Evaluation Acc:=84.84\n",
            "Epoch=15 - Train Loss:=2151.635498046875, Evaluation Loss:=1.663333535194397, Train Acc:=86.76833333333333, Evaluation Acc:=83.73\n",
            "Epoch=16 - Train Loss:=2150.011474609375, Evaluation Loss:=1.5423575639724731, Train Acc:=87.36333333333334, Evaluation Acc:=84.72\n",
            "Epoch=17 - Train Loss:=2180.183349609375, Evaluation Loss:=1.941389799118042, Train Acc:=85.355, Evaluation Acc:=82.45\n",
            "Epoch=18 - Train Loss:=2207.391845703125, Evaluation Loss:=1.452722191810608, Train Acc:=86.85333333333334, Evaluation Acc:=84.31\n",
            "Epoch=19 - Train Loss:=2201.203369140625, Evaluation Loss:=1.454231858253479, Train Acc:=88.925, Evaluation Acc:=85.72\n",
            "Epoch=0 - Train Loss:=2842.185546875, Evaluation Loss:=1.1141533851623535, Train Acc:=82.06333333333333, Evaluation Acc:=80.44\n",
            "Epoch=1 - Train Loss:=1694.96533203125, Evaluation Loss:=1.0671154260635376, Train Acc:=83.15666666666667, Evaluation Acc:=81.24\n",
            "Epoch=2 - Train Loss:=1530.5416259765625, Evaluation Loss:=1.018031120300293, Train Acc:=83.86166666666666, Evaluation Acc:=81.67\n",
            "Epoch=3 - Train Loss:=1537.315673828125, Evaluation Loss:=0.9791687726974487, Train Acc:=84.56666666666666, Evaluation Acc:=82.50999999999999\n",
            "Epoch=4 - Train Loss:=1588.875, Evaluation Loss:=0.9653183817863464, Train Acc:=86.565, Evaluation Acc:=84.27\n",
            "Epoch=5 - Train Loss:=1666.38720703125, Evaluation Loss:=0.9594317674636841, Train Acc:=85.885, Evaluation Acc:=84.14\n",
            "Epoch=6 - Train Loss:=1702.8253173828125, Evaluation Loss:=1.0310511589050293, Train Acc:=87.21666666666667, Evaluation Acc:=84.78\n",
            "Epoch=7 - Train Loss:=1786.5164794921875, Evaluation Loss:=1.0248706340789795, Train Acc:=87.1, Evaluation Acc:=84.46000000000001\n",
            "Epoch=8 - Train Loss:=1835.9150390625, Evaluation Loss:=1.2886435985565186, Train Acc:=85.61166666666666, Evaluation Acc:=83.34\n",
            "Epoch=9 - Train Loss:=1952.5286865234375, Evaluation Loss:=1.5730334520339966, Train Acc:=84.76, Evaluation Acc:=82.93\n",
            "Epoch=10 - Train Loss:=1953.07275390625, Evaluation Loss:=1.3019744157791138, Train Acc:=86.735, Evaluation Acc:=84.21\n",
            "Epoch=11 - Train Loss:=1996.602294921875, Evaluation Loss:=1.3574965000152588, Train Acc:=86.59166666666667, Evaluation Acc:=83.88\n",
            "Epoch=12 - Train Loss:=2045.961181640625, Evaluation Loss:=1.3166797161102295, Train Acc:=87.80166666666666, Evaluation Acc:=85.03\n",
            "Epoch=13 - Train Loss:=2058.428955078125, Evaluation Loss:=1.5439811944961548, Train Acc:=87.42999999999999, Evaluation Acc:=84.84\n",
            "Epoch=14 - Train Loss:=2107.364990234375, Evaluation Loss:=1.1959822177886963, Train Acc:=89.01833333333333, Evaluation Acc:=86.04\n",
            "Epoch=15 - Train Loss:=2085.373291015625, Evaluation Loss:=1.4963876008987427, Train Acc:=87.44, Evaluation Acc:=84.43\n",
            "Epoch=16 - Train Loss:=2046.48193359375, Evaluation Loss:=1.9634422063827515, Train Acc:=84.725, Evaluation Acc:=82.45\n",
            "Epoch=17 - Train Loss:=2076.698486328125, Evaluation Loss:=1.3593790531158447, Train Acc:=89.17, Evaluation Acc:=85.48\n",
            "Epoch=18 - Train Loss:=2076.150634765625, Evaluation Loss:=1.6032837629318237, Train Acc:=88.10666666666667, Evaluation Acc:=85.49\n",
            "Epoch=19 - Train Loss:=2158.99951171875, Evaluation Loss:=1.4054263830184937, Train Acc:=89.445, Evaluation Acc:=86.83\n",
            "Epoch=0 - Train Loss:=2599.2880859375, Evaluation Loss:=0.9538430571556091, Train Acc:=84.20333333333333, Evaluation Acc:=82.73\n",
            "Epoch=1 - Train Loss:=1629.4954833984375, Evaluation Loss:=0.9272955060005188, Train Acc:=84.495, Evaluation Acc:=82.25\n",
            "Epoch=2 - Train Loss:=1516.5870361328125, Evaluation Loss:=0.8639408349990845, Train Acc:=86.07333333333334, Evaluation Acc:=83.58\n",
            "Epoch=3 - Train Loss:=1504.9976806640625, Evaluation Loss:=1.0075322389602661, Train Acc:=82.88666666666667, Evaluation Acc:=80.11\n",
            "Epoch=4 - Train Loss:=1575.516845703125, Evaluation Loss:=1.0304036140441895, Train Acc:=84.84333333333333, Evaluation Acc:=82.63000000000001\n",
            "Epoch=5 - Train Loss:=1681.8173828125, Evaluation Loss:=1.4183976650238037, Train Acc:=83.65333333333334, Evaluation Acc:=81.64\n",
            "Epoch=6 - Train Loss:=1830.2125244140625, Evaluation Loss:=1.5410269498825073, Train Acc:=82.235, Evaluation Acc:=80.25\n",
            "Epoch=7 - Train Loss:=1925.760009765625, Evaluation Loss:=1.1670736074447632, Train Acc:=86.67333333333333, Evaluation Acc:=84.54\n",
            "Epoch=8 - Train Loss:=1962.671142578125, Evaluation Loss:=1.1395034790039062, Train Acc:=87.10666666666667, Evaluation Acc:=84.52\n",
            "Epoch=9 - Train Loss:=2027.6905517578125, Evaluation Loss:=1.4700621366500854, Train Acc:=86.74833333333333, Evaluation Acc:=84.63000000000001\n",
            "Epoch=10 - Train Loss:=2061.128662109375, Evaluation Loss:=1.4463471174240112, Train Acc:=85.64166666666667, Evaluation Acc:=83.19\n",
            "Epoch=11 - Train Loss:=2037.8594970703125, Evaluation Loss:=1.248374581336975, Train Acc:=87.47666666666667, Evaluation Acc:=84.57000000000001\n",
            "Epoch=12 - Train Loss:=2093.977294921875, Evaluation Loss:=1.6082309484481812, Train Acc:=85.19666666666666, Evaluation Acc:=82.5\n",
            "Epoch=13 - Train Loss:=2157.351318359375, Evaluation Loss:=1.779875636100769, Train Acc:=86.16833333333334, Evaluation Acc:=83.54\n",
            "Epoch=14 - Train Loss:=2190.749755859375, Evaluation Loss:=1.5827347040176392, Train Acc:=87.05666666666667, Evaluation Acc:=84.44\n",
            "Epoch=15 - Train Loss:=2124.473876953125, Evaluation Loss:=1.5505478382110596, Train Acc:=87.835, Evaluation Acc:=85.16\n",
            "Epoch=16 - Train Loss:=2105.721435546875, Evaluation Loss:=1.5882456302642822, Train Acc:=86.68833333333333, Evaluation Acc:=83.82\n",
            "Epoch=17 - Train Loss:=2072.787109375, Evaluation Loss:=1.1872812509536743, Train Acc:=89.18166666666667, Evaluation Acc:=86.13\n",
            "Epoch=18 - Train Loss:=2033.7923583984375, Evaluation Loss:=1.4943795204162598, Train Acc:=88.05833333333334, Evaluation Acc:=85.48\n",
            "Epoch=19 - Train Loss:=2060.41943359375, Evaluation Loss:=1.4975531101226807, Train Acc:=88.24166666666666, Evaluation Acc:=84.96000000000001\n",
            "Epoch=0 - Train Loss:=2818.6005859375, Evaluation Loss:=1.1818044185638428, Train Acc:=80.97666666666666, Evaluation Acc:=78.92\n",
            "Epoch=1 - Train Loss:=1772.427734375, Evaluation Loss:=0.9716282486915588, Train Acc:=84.88666666666667, Evaluation Acc:=82.67999999999999\n",
            "Epoch=2 - Train Loss:=1667.8404541015625, Evaluation Loss:=0.9327112436294556, Train Acc:=85.59666666666666, Evaluation Acc:=83.59\n",
            "Epoch=3 - Train Loss:=1647.8341064453125, Evaluation Loss:=1.0641387701034546, Train Acc:=84.115, Evaluation Acc:=81.56\n",
            "Epoch=4 - Train Loss:=1681.484130859375, Evaluation Loss:=1.0965557098388672, Train Acc:=86.32166666666666, Evaluation Acc:=84.16\n",
            "Epoch=5 - Train Loss:=1672.660888671875, Evaluation Loss:=1.139874815940857, Train Acc:=84.765, Evaluation Acc:=82.21000000000001\n",
            "Epoch=6 - Train Loss:=1707.684326171875, Evaluation Loss:=1.172640085220337, Train Acc:=87.20333333333333, Evaluation Acc:=84.93\n",
            "Epoch=7 - Train Loss:=1772.6837158203125, Evaluation Loss:=1.322461724281311, Train Acc:=84.67833333333333, Evaluation Acc:=82.59\n",
            "Epoch=8 - Train Loss:=1851.1824951171875, Evaluation Loss:=1.4053109884262085, Train Acc:=86.61, Evaluation Acc:=84.48\n",
            "Epoch=9 - Train Loss:=1950.2989501953125, Evaluation Loss:=1.333930253982544, Train Acc:=86.17166666666667, Evaluation Acc:=83.82\n",
            "Epoch=10 - Train Loss:=2017.6419677734375, Evaluation Loss:=1.644976019859314, Train Acc:=85.28, Evaluation Acc:=82.56\n",
            "Epoch=11 - Train Loss:=1973.6328125, Evaluation Loss:=1.214281439781189, Train Acc:=88.31333333333333, Evaluation Acc:=85.48\n",
            "Epoch=12 - Train Loss:=1953.8538818359375, Evaluation Loss:=1.3520386219024658, Train Acc:=87.41499999999999, Evaluation Acc:=84.75\n",
            "Epoch=13 - Train Loss:=2013.4566650390625, Evaluation Loss:=1.27567720413208, Train Acc:=87.66000000000001, Evaluation Acc:=85.16\n",
            "Epoch=14 - Train Loss:=2010.779541015625, Evaluation Loss:=1.473182201385498, Train Acc:=86.66166666666668, Evaluation Acc:=84.00999999999999\n",
            "Epoch=15 - Train Loss:=2082.99072265625, Evaluation Loss:=1.4115874767303467, Train Acc:=87.77166666666668, Evaluation Acc:=85.19\n",
            "Epoch=16 - Train Loss:=2087.472412109375, Evaluation Loss:=1.2957348823547363, Train Acc:=88.55833333333332, Evaluation Acc:=85.74000000000001\n",
            "Epoch=17 - Train Loss:=2118.7958984375, Evaluation Loss:=1.5948094129562378, Train Acc:=88.28833333333334, Evaluation Acc:=85.69\n",
            "Epoch=18 - Train Loss:=2178.8720703125, Evaluation Loss:=2.0131406784057617, Train Acc:=86.4, Evaluation Acc:=83.95\n",
            "Epoch=19 - Train Loss:=2186.49267578125, Evaluation Loss:=1.363743782043457, Train Acc:=88.38833333333334, Evaluation Acc:=85.27\n",
            "Epoch=0 - Train Loss:=2773.73681640625, Evaluation Loss:=1.1480027437210083, Train Acc:=82.54, Evaluation Acc:=80.5\n",
            "Epoch=1 - Train Loss:=1722.1263427734375, Evaluation Loss:=0.9386413097381592, Train Acc:=84.575, Evaluation Acc:=82.43\n",
            "Epoch=2 - Train Loss:=1546.96044921875, Evaluation Loss:=0.9271684288978577, Train Acc:=84.60499999999999, Evaluation Acc:=82.21000000000001\n",
            "Epoch=3 - Train Loss:=1553.854248046875, Evaluation Loss:=1.0760947465896606, Train Acc:=84.25333333333333, Evaluation Acc:=81.62\n",
            "Epoch=4 - Train Loss:=1584.4691162109375, Evaluation Loss:=1.0266082286834717, Train Acc:=86.88333333333334, Evaluation Acc:=84.46000000000001\n",
            "Epoch=5 - Train Loss:=1660.8260498046875, Evaluation Loss:=1.2603979110717773, Train Acc:=84.29166666666667, Evaluation Acc:=82.56\n",
            "Epoch=6 - Train Loss:=1750.7144775390625, Evaluation Loss:=1.0077391862869263, Train Acc:=87.00166666666667, Evaluation Acc:=84.27\n",
            "Epoch=7 - Train Loss:=1838.1705322265625, Evaluation Loss:=0.9957085251808167, Train Acc:=87.52333333333333, Evaluation Acc:=84.96000000000001\n",
            "Epoch=8 - Train Loss:=1856.6219482421875, Evaluation Loss:=1.4438657760620117, Train Acc:=83.27666666666667, Evaluation Acc:=80.97\n",
            "Epoch=9 - Train Loss:=1956.83984375, Evaluation Loss:=1.2931389808654785, Train Acc:=85.115, Evaluation Acc:=82.28\n",
            "Epoch=10 - Train Loss:=2034.40771484375, Evaluation Loss:=1.0978279113769531, Train Acc:=88.295, Evaluation Acc:=85.28\n",
            "Epoch=11 - Train Loss:=2077.07177734375, Evaluation Loss:=1.3883144855499268, Train Acc:=87.095, Evaluation Acc:=84.36\n",
            "Epoch=12 - Train Loss:=2158.423095703125, Evaluation Loss:=1.5707329511642456, Train Acc:=86.91, Evaluation Acc:=84.71\n",
            "Epoch=13 - Train Loss:=2105.108154296875, Evaluation Loss:=1.3070049285888672, Train Acc:=88.77000000000001, Evaluation Acc:=85.85000000000001\n",
            "Epoch=14 - Train Loss:=2184.40234375, Evaluation Loss:=1.8580058813095093, Train Acc:=84.04833333333333, Evaluation Acc:=81.33\n",
            "Epoch=15 - Train Loss:=2194.81689453125, Evaluation Loss:=1.902209997177124, Train Acc:=85.42166666666667, Evaluation Acc:=82.8\n",
            "Epoch=16 - Train Loss:=2243.379150390625, Evaluation Loss:=1.7663111686706543, Train Acc:=86.35666666666667, Evaluation Acc:=83.45\n",
            "Epoch=17 - Train Loss:=2280.19921875, Evaluation Loss:=2.0149784088134766, Train Acc:=84.43666666666667, Evaluation Acc:=81.53\n",
            "Epoch=18 - Train Loss:=2280.125, Evaluation Loss:=1.3939838409423828, Train Acc:=88.58166666666666, Evaluation Acc:=85.37\n",
            "Epoch=19 - Train Loss:=2206.0751953125, Evaluation Loss:=1.830038070678711, Train Acc:=86.905, Evaluation Acc:=84.17999999999999\n",
            "Epoch=0 - Train Loss:=2685.237060546875, Evaluation Loss:=0.9609046578407288, Train Acc:=83.90666666666667, Evaluation Acc:=82.58\n",
            "Epoch=1 - Train Loss:=1693.8411865234375, Evaluation Loss:=0.9141613245010376, Train Acc:=84.22166666666666, Evaluation Acc:=81.87\n",
            "Epoch=2 - Train Loss:=1571.5782470703125, Evaluation Loss:=1.1057952642440796, Train Acc:=83.02666666666667, Evaluation Acc:=80.93\n",
            "Epoch=3 - Train Loss:=1555.418212890625, Evaluation Loss:=1.0416581630706787, Train Acc:=84.55833333333334, Evaluation Acc:=82.05\n",
            "Epoch=4 - Train Loss:=1555.8057861328125, Evaluation Loss:=0.9031403064727783, Train Acc:=87.06666666666666, Evaluation Acc:=84.48\n",
            "Epoch=5 - Train Loss:=1629.0364990234375, Evaluation Loss:=1.0633920431137085, Train Acc:=85.855, Evaluation Acc:=84.14\n",
            "Epoch=6 - Train Loss:=1687.291015625, Evaluation Loss:=1.0701544284820557, Train Acc:=86.5, Evaluation Acc:=84.1\n",
            "Epoch=7 - Train Loss:=1712.405029296875, Evaluation Loss:=1.575631856918335, Train Acc:=82.50333333333333, Evaluation Acc:=80.47\n",
            "Epoch=8 - Train Loss:=1811.8759765625, Evaluation Loss:=1.1789298057556152, Train Acc:=87.30166666666666, Evaluation Acc:=85.11\n",
            "Epoch=9 - Train Loss:=1909.261962890625, Evaluation Loss:=1.2176278829574585, Train Acc:=86.11, Evaluation Acc:=83.77\n",
            "Epoch=10 - Train Loss:=1914.3555908203125, Evaluation Loss:=1.1285521984100342, Train Acc:=88.35166666666666, Evaluation Acc:=85.72999999999999\n",
            "Epoch=11 - Train Loss:=1995.525634765625, Evaluation Loss:=1.2611922025680542, Train Acc:=87.14333333333333, Evaluation Acc:=84.71\n",
            "Epoch=12 - Train Loss:=2058.966796875, Evaluation Loss:=1.2271512746810913, Train Acc:=86.53, Evaluation Acc:=84.33\n",
            "Epoch=13 - Train Loss:=2023.10400390625, Evaluation Loss:=1.484357476234436, Train Acc:=85.37833333333333, Evaluation Acc:=83.21\n",
            "Epoch=14 - Train Loss:=2058.177001953125, Evaluation Loss:=1.4322242736816406, Train Acc:=88.19500000000001, Evaluation Acc:=85.48\n",
            "Epoch=15 - Train Loss:=2115.5908203125, Evaluation Loss:=1.4121601581573486, Train Acc:=88.00333333333333, Evaluation Acc:=85.24000000000001\n",
            "Epoch=16 - Train Loss:=2074.13330078125, Evaluation Loss:=1.6768003702163696, Train Acc:=87.69, Evaluation Acc:=85.16\n",
            "Epoch=17 - Train Loss:=2138.7763671875, Evaluation Loss:=1.4411667585372925, Train Acc:=87.815, Evaluation Acc:=85.33\n",
            "Epoch=18 - Train Loss:=2136.65869140625, Evaluation Loss:=1.6408988237380981, Train Acc:=87.25166666666667, Evaluation Acc:=84.28999999999999\n",
            "Epoch=19 - Train Loss:=2241.89453125, Evaluation Loss:=2.2684028148651123, Train Acc:=82.86166666666666, Evaluation Acc:=79.75\n",
            "Epoch=0 - Train Loss:=2894.0419921875, Evaluation Loss:=1.3628902435302734, Train Acc:=80.13, Evaluation Acc:=79.31\n",
            "Epoch=1 - Train Loss:=1770.732421875, Evaluation Loss:=1.0701792240142822, Train Acc:=82.46833333333333, Evaluation Acc:=80.97\n",
            "Epoch=2 - Train Loss:=1621.9163818359375, Evaluation Loss:=0.9465100765228271, Train Acc:=85.63499999999999, Evaluation Acc:=83.55\n",
            "Epoch=3 - Train Loss:=1555.5059814453125, Evaluation Loss:=1.064756155014038, Train Acc:=84.13833333333334, Evaluation Acc:=82.14\n",
            "Epoch=4 - Train Loss:=1585.5322265625, Evaluation Loss:=1.3275867700576782, Train Acc:=81.91166666666668, Evaluation Acc:=79.71000000000001\n",
            "Epoch=5 - Train Loss:=1641.2994384765625, Evaluation Loss:=1.110176682472229, Train Acc:=85.81833333333333, Evaluation Acc:=83.6\n",
            "Epoch=6 - Train Loss:=1701.101806640625, Evaluation Loss:=0.999089241027832, Train Acc:=87.43166666666666, Evaluation Acc:=85.26\n",
            "Epoch=7 - Train Loss:=1711.9912109375, Evaluation Loss:=1.0466927289962769, Train Acc:=87.52499999999999, Evaluation Acc:=84.92\n",
            "Epoch=8 - Train Loss:=1805.454833984375, Evaluation Loss:=1.2144945859909058, Train Acc:=85.455, Evaluation Acc:=83.44\n",
            "Epoch=9 - Train Loss:=1928.32080078125, Evaluation Loss:=1.3605517148971558, Train Acc:=87.61, Evaluation Acc:=85.24000000000001\n",
            "Epoch=10 - Train Loss:=2021.7890625, Evaluation Loss:=1.158583164215088, Train Acc:=87.95500000000001, Evaluation Acc:=85.5\n",
            "Epoch=11 - Train Loss:=2065.845703125, Evaluation Loss:=1.5010610818862915, Train Acc:=86.26666666666667, Evaluation Acc:=83.52000000000001\n",
            "Epoch=12 - Train Loss:=2051.298095703125, Evaluation Loss:=1.7484711408615112, Train Acc:=85.49, Evaluation Acc:=83.33\n",
            "Epoch=13 - Train Loss:=2100.970703125, Evaluation Loss:=1.686607837677002, Train Acc:=84.265, Evaluation Acc:=81.26\n",
            "Epoch=14 - Train Loss:=2186.73046875, Evaluation Loss:=1.7581908702850342, Train Acc:=85.77166666666668, Evaluation Acc:=83.17999999999999\n",
            "Epoch=15 - Train Loss:=2161.859375, Evaluation Loss:=1.803744912147522, Train Acc:=85.27833333333334, Evaluation Acc:=83.2\n",
            "Epoch=16 - Train Loss:=2199.493408203125, Evaluation Loss:=1.608208179473877, Train Acc:=86.96833333333333, Evaluation Acc:=83.95\n",
            "Epoch=17 - Train Loss:=2290.83544921875, Evaluation Loss:=1.5765336751937866, Train Acc:=87.62166666666667, Evaluation Acc:=84.99\n",
            "Epoch=18 - Train Loss:=2343.152099609375, Evaluation Loss:=1.9900856018066406, Train Acc:=85.81666666666666, Evaluation Acc:=83.13000000000001\n",
            "Epoch=19 - Train Loss:=2316.214111328125, Evaluation Loss:=1.5831646919250488, Train Acc:=86.53166666666667, Evaluation Acc:=83.44\n",
            "Epoch=0 - Train Loss:=2663.893798828125, Evaluation Loss:=1.0821276903152466, Train Acc:=83.40666666666667, Evaluation Acc:=81.17\n",
            "Epoch=1 - Train Loss:=1772.3350830078125, Evaluation Loss:=1.3560729026794434, Train Acc:=79.28333333333333, Evaluation Acc:=76.69\n",
            "Epoch=2 - Train Loss:=1644.5908203125, Evaluation Loss:=1.2162073850631714, Train Acc:=82.84166666666667, Evaluation Acc:=80.46\n",
            "Epoch=3 - Train Loss:=1609.0606689453125, Evaluation Loss:=1.0171304941177368, Train Acc:=84.54, Evaluation Acc:=81.63\n",
            "Epoch=4 - Train Loss:=1662.729736328125, Evaluation Loss:=1.0226796865463257, Train Acc:=85.08666666666667, Evaluation Acc:=83.21\n",
            "Epoch=5 - Train Loss:=1694.75830078125, Evaluation Loss:=1.0587773323059082, Train Acc:=86.63833333333332, Evaluation Acc:=84.36\n",
            "Epoch=6 - Train Loss:=1738.920654296875, Evaluation Loss:=1.1971344947814941, Train Acc:=85.11, Evaluation Acc:=83.19\n",
            "Epoch=7 - Train Loss:=1800.3623046875, Evaluation Loss:=0.9567386507987976, Train Acc:=87.71166666666666, Evaluation Acc:=85.07000000000001\n",
            "Epoch=8 - Train Loss:=1870.548828125, Evaluation Loss:=1.4230849742889404, Train Acc:=84.84833333333334, Evaluation Acc:=82.52000000000001\n",
            "Epoch=9 - Train Loss:=1894.977783203125, Evaluation Loss:=1.3153892755508423, Train Acc:=86.185, Evaluation Acc:=83.25\n",
            "Epoch=10 - Train Loss:=1988.2445068359375, Evaluation Loss:=1.1776078939437866, Train Acc:=87.91833333333334, Evaluation Acc:=85.42\n",
            "Epoch=11 - Train Loss:=1996.1607666015625, Evaluation Loss:=1.3161816596984863, Train Acc:=87.835, Evaluation Acc:=84.8\n",
            "Epoch=12 - Train Loss:=2012.396240234375, Evaluation Loss:=1.3630884885787964, Train Acc:=86.955, Evaluation Acc:=84.13000000000001\n",
            "Epoch=13 - Train Loss:=2074.696533203125, Evaluation Loss:=1.3956990242004395, Train Acc:=86.19833333333334, Evaluation Acc:=83.5\n",
            "Epoch=14 - Train Loss:=2127.618408203125, Evaluation Loss:=1.330018401145935, Train Acc:=87.39, Evaluation Acc:=84.57000000000001\n",
            "Epoch=15 - Train Loss:=2147.81396484375, Evaluation Loss:=1.9812161922454834, Train Acc:=84.625, Evaluation Acc:=82.66\n",
            "Epoch=16 - Train Loss:=2137.00927734375, Evaluation Loss:=1.3316221237182617, Train Acc:=88.765, Evaluation Acc:=86.16\n",
            "Epoch=17 - Train Loss:=2195.4873046875, Evaluation Loss:=1.3223512172698975, Train Acc:=89.04833333333333, Evaluation Acc:=86.16\n",
            "Epoch=18 - Train Loss:=2212.565673828125, Evaluation Loss:=1.891033411026001, Train Acc:=85.90166666666667, Evaluation Acc:=82.97\n",
            "Epoch=19 - Train Loss:=2163.714111328125, Evaluation Loss:=1.5539443492889404, Train Acc:=86.935, Evaluation Acc:=83.74000000000001\n",
            "Epoch=0 - Train Loss:=2553.623779296875, Evaluation Loss:=1.1599078178405762, Train Acc:=82.285, Evaluation Acc:=80.4\n",
            "Epoch=1 - Train Loss:=1694.472900390625, Evaluation Loss:=1.0882009267807007, Train Acc:=82.59166666666667, Evaluation Acc:=80.61\n",
            "Epoch=2 - Train Loss:=1588.187255859375, Evaluation Loss:=0.8392467498779297, Train Acc:=86.43666666666667, Evaluation Acc:=83.53\n",
            "Epoch=3 - Train Loss:=1549.504638671875, Evaluation Loss:=1.1693987846374512, Train Acc:=82.67833333333333, Evaluation Acc:=80.88\n",
            "Epoch=4 - Train Loss:=1603.4593505859375, Evaluation Loss:=0.8999834060668945, Train Acc:=86.60333333333334, Evaluation Acc:=84.41\n",
            "Epoch=5 - Train Loss:=1663.1536865234375, Evaluation Loss:=1.2973332405090332, Train Acc:=83.44666666666667, Evaluation Acc:=80.96\n",
            "Epoch=6 - Train Loss:=1727.0804443359375, Evaluation Loss:=1.0979957580566406, Train Acc:=85.86333333333333, Evaluation Acc:=83.37\n",
            "Epoch=7 - Train Loss:=1794.463623046875, Evaluation Loss:=1.3978958129882812, Train Acc:=83.56166666666667, Evaluation Acc:=81.31\n",
            "Epoch=8 - Train Loss:=1905.4930419921875, Evaluation Loss:=1.3815789222717285, Train Acc:=84.56333333333333, Evaluation Acc:=82.19999999999999\n",
            "Epoch=9 - Train Loss:=1942.6961669921875, Evaluation Loss:=1.2891390323638916, Train Acc:=87.18833333333333, Evaluation Acc:=84.38\n",
            "Epoch=10 - Train Loss:=2029.08251953125, Evaluation Loss:=1.1509170532226562, Train Acc:=88.04833333333333, Evaluation Acc:=85.28999999999999\n",
            "Epoch=11 - Train Loss:=2024.7706298828125, Evaluation Loss:=1.324263095855713, Train Acc:=87.36500000000001, Evaluation Acc:=84.7\n",
            "Epoch=12 - Train Loss:=2119.752685546875, Evaluation Loss:=1.5410645008087158, Train Acc:=86.13166666666666, Evaluation Acc:=83.57\n",
            "Epoch=13 - Train Loss:=2115.123779296875, Evaluation Loss:=1.6480443477630615, Train Acc:=87.10833333333333, Evaluation Acc:=84.45\n",
            "Epoch=14 - Train Loss:=2205.0458984375, Evaluation Loss:=1.4163480997085571, Train Acc:=86.17166666666667, Evaluation Acc:=83.47\n",
            "Epoch=15 - Train Loss:=2157.54736328125, Evaluation Loss:=1.5615744590759277, Train Acc:=85.775, Evaluation Acc:=83.38\n",
            "Epoch=16 - Train Loss:=2174.510986328125, Evaluation Loss:=2.058387041091919, Train Acc:=84.655, Evaluation Acc:=82.11\n",
            "Epoch=17 - Train Loss:=2220.572021484375, Evaluation Loss:=1.7432669401168823, Train Acc:=85.18166666666667, Evaluation Acc:=82.43\n",
            "Epoch=18 - Train Loss:=2292.655517578125, Evaluation Loss:=1.4753518104553223, Train Acc:=88.66166666666668, Evaluation Acc:=85.33\n",
            "Epoch=19 - Train Loss:=2400.86083984375, Evaluation Loss:=1.8476155996322632, Train Acc:=88.02499999999999, Evaluation Acc:=85.64\n",
            "Epoch=0 - Train Loss:=2799.4091796875, Evaluation Loss:=1.0686521530151367, Train Acc:=83.165, Evaluation Acc:=81.22\n",
            "Epoch=1 - Train Loss:=1753.5345458984375, Evaluation Loss:=0.9090334177017212, Train Acc:=85.34166666666667, Evaluation Acc:=82.69\n",
            "Epoch=2 - Train Loss:=1606.2110595703125, Evaluation Loss:=0.8350964784622192, Train Acc:=86.09833333333333, Evaluation Acc:=84.06\n",
            "Epoch=3 - Train Loss:=1566.5216064453125, Evaluation Loss:=0.9295247793197632, Train Acc:=85.44166666666668, Evaluation Acc:=83.24000000000001\n",
            "Epoch=4 - Train Loss:=1601.7314453125, Evaluation Loss:=1.004255771636963, Train Acc:=86.44, Evaluation Acc:=83.89\n",
            "Epoch=5 - Train Loss:=1644.206298828125, Evaluation Loss:=1.1042931079864502, Train Acc:=84.58333333333333, Evaluation Acc:=81.92\n",
            "Epoch=6 - Train Loss:=1723.9466552734375, Evaluation Loss:=1.2879185676574707, Train Acc:=83.14166666666667, Evaluation Acc:=81.08999999999999\n",
            "Epoch=7 - Train Loss:=1813.69091796875, Evaluation Loss:=1.2485390901565552, Train Acc:=85.69, Evaluation Acc:=83.28\n",
            "Epoch=8 - Train Loss:=1861.8427734375, Evaluation Loss:=1.1193193197250366, Train Acc:=87.67, Evaluation Acc:=85.21\n",
            "Epoch=9 - Train Loss:=1983.1640625, Evaluation Loss:=1.8751150369644165, Train Acc:=80.67666666666666, Evaluation Acc:=78.17\n",
            "Epoch=10 - Train Loss:=2037.1846923828125, Evaluation Loss:=1.3470442295074463, Train Acc:=87.07833333333333, Evaluation Acc:=84.36\n",
            "Epoch=11 - Train Loss:=2058.126953125, Evaluation Loss:=1.332780122756958, Train Acc:=86.25500000000001, Evaluation Acc:=83.78\n",
            "Epoch=12 - Train Loss:=1993.6248779296875, Evaluation Loss:=1.3509974479675293, Train Acc:=87.95500000000001, Evaluation Acc:=84.94\n",
            "Epoch=13 - Train Loss:=2016.218017578125, Evaluation Loss:=1.4666144847869873, Train Acc:=83.99166666666666, Evaluation Acc:=81.12\n",
            "Epoch=14 - Train Loss:=2040.736083984375, Evaluation Loss:=1.4926289319992065, Train Acc:=85.84, Evaluation Acc:=83.06\n",
            "Epoch=15 - Train Loss:=2020.7115478515625, Evaluation Loss:=1.1844576597213745, Train Acc:=88.45333333333333, Evaluation Acc:=85.84\n",
            "Epoch=16 - Train Loss:=1983.9053955078125, Evaluation Loss:=2.0900626182556152, Train Acc:=84.94500000000001, Evaluation Acc:=82.3\n",
            "Epoch=17 - Train Loss:=1976.414794921875, Evaluation Loss:=1.5962510108947754, Train Acc:=85.30333333333333, Evaluation Acc:=82.66\n",
            "Epoch=18 - Train Loss:=2052.209228515625, Evaluation Loss:=1.5222251415252686, Train Acc:=86.87833333333333, Evaluation Acc:=83.8\n",
            "Epoch=19 - Train Loss:=2057.156494140625, Evaluation Loss:=1.5786035060882568, Train Acc:=87.295, Evaluation Acc:=84.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "PTDDKbvHnnfu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "176bfa14-0fa1-4b6e-e860-6b3198ba34e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 84.396, and the variance is 3.328423999999999\n",
            "the average running time for one epoch is: 40.13925552606583\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7f8dee174ad0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f8dee0fab50>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dee1020d0>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7f8dee102b90>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f8dee102650>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7f8dee0fa0d0>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dee0fa610>]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model10: with dropout and L2 regularization + Adam optimizer"
      ],
      "metadata": {
        "id": "kboErjFH8_np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "optimizer = \"Adam\"\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "learning_rate = 10e-5\n",
        "dropout = 0.1\n",
        "beta = 0.001\n",
        "epochs = 20\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i+10)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"fashion_mnist\")\n",
        "\n",
        "    model10 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer = optimizer)\n",
        "    train_losses, test_losses, train_accuracy, test_accuracy, _epoch_times = train(model10, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs,\n",
        "                                                                 dropout_rate=dropout,\n",
        "                                                                 beta=beta)\n",
        "    acc = test(model10, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "3XjLSnIb9Fhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5608e875-163d-403c-b243-faf0f24bf00d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=19619.216796875, Evaluation Loss:=0.7594018578529358, Train Acc:=83.395, Evaluation Acc:=82.06\n",
            "Epoch=1 - Train Loss:=9751.0859375, Evaluation Loss:=0.6168406009674072, Train Acc:=83.53333333333333, Evaluation Acc:=82.41000000000001\n",
            "Epoch=2 - Train Loss:=6965.02783203125, Evaluation Loss:=0.6041348576545715, Train Acc:=84.51166666666666, Evaluation Acc:=82.95\n",
            "Epoch=3 - Train Loss:=5437.71826171875, Evaluation Loss:=0.4992223083972931, Train Acc:=86.05333333333334, Evaluation Acc:=84.61999999999999\n",
            "Epoch=4 - Train Loss:=4443.2470703125, Evaluation Loss:=0.5003650188446045, Train Acc:=85.92166666666667, Evaluation Acc:=84.13000000000001\n",
            "Epoch=5 - Train Loss:=3788.18505859375, Evaluation Loss:=0.5043657422065735, Train Acc:=85.75166666666667, Evaluation Acc:=84.26\n",
            "Epoch=6 - Train Loss:=3288.885009765625, Evaluation Loss:=0.6061462163925171, Train Acc:=83.76666666666667, Evaluation Acc:=82.24000000000001\n",
            "Epoch=7 - Train Loss:=2923.3623046875, Evaluation Loss:=0.5052078366279602, Train Acc:=85.57666666666667, Evaluation Acc:=84.00999999999999\n",
            "Epoch=8 - Train Loss:=2642.390380859375, Evaluation Loss:=0.5524464249610901, Train Acc:=84.33166666666668, Evaluation Acc:=82.57\n",
            "Epoch=9 - Train Loss:=2441.33251953125, Evaluation Loss:=0.5978637933731079, Train Acc:=84.165, Evaluation Acc:=82.52000000000001\n",
            "Epoch=10 - Train Loss:=2287.665771484375, Evaluation Loss:=0.539294421672821, Train Acc:=85.32833333333333, Evaluation Acc:=83.65\n",
            "Epoch=11 - Train Loss:=2163.053955078125, Evaluation Loss:=0.479521781206131, Train Acc:=86.18, Evaluation Acc:=84.33\n",
            "Epoch=12 - Train Loss:=2085.476318359375, Evaluation Loss:=0.4853181540966034, Train Acc:=86.31833333333333, Evaluation Acc:=84.56\n",
            "Epoch=13 - Train Loss:=2014.4095458984375, Evaluation Loss:=0.7198572754859924, Train Acc:=81.84833333333333, Evaluation Acc:=80.49\n",
            "Epoch=14 - Train Loss:=1967.3494873046875, Evaluation Loss:=0.5458502173423767, Train Acc:=85.07666666666667, Evaluation Acc:=83.53\n",
            "Epoch=15 - Train Loss:=1926.7156982421875, Evaluation Loss:=0.616019606590271, Train Acc:=83.02666666666667, Evaluation Acc:=81.78999999999999\n",
            "Epoch=16 - Train Loss:=1904.8404541015625, Evaluation Loss:=0.5956700444221497, Train Acc:=84.23, Evaluation Acc:=83.33\n",
            "Epoch=17 - Train Loss:=1882.0670166015625, Evaluation Loss:=0.5084492564201355, Train Acc:=85.91, Evaluation Acc:=84.44\n",
            "Epoch=18 - Train Loss:=1862.788330078125, Evaluation Loss:=0.6173756718635559, Train Acc:=84.16666666666667, Evaluation Acc:=82.95\n",
            "Epoch=19 - Train Loss:=1861.17333984375, Evaluation Loss:=0.4921480119228363, Train Acc:=86.47333333333333, Evaluation Acc:=84.83000000000001\n",
            "Epoch=0 - Train Loss:=19065.859375, Evaluation Loss:=0.9994707107543945, Train Acc:=78.51333333333334, Evaluation Acc:=77.5\n",
            "Epoch=1 - Train Loss:=9453.9609375, Evaluation Loss:=0.5697562098503113, Train Acc:=84.53500000000001, Evaluation Acc:=82.45\n",
            "Epoch=2 - Train Loss:=6829.28466796875, Evaluation Loss:=0.6375371813774109, Train Acc:=83.31166666666667, Evaluation Acc:=81.55\n",
            "Epoch=3 - Train Loss:=5337.54541015625, Evaluation Loss:=0.5261145234107971, Train Acc:=84.28666666666666, Evaluation Acc:=82.22\n",
            "Epoch=4 - Train Loss:=4370.54052734375, Evaluation Loss:=0.535872757434845, Train Acc:=84.87666666666667, Evaluation Acc:=83.48\n",
            "Epoch=5 - Train Loss:=3712.179443359375, Evaluation Loss:=0.5676624178886414, Train Acc:=84.61666666666666, Evaluation Acc:=83.31\n",
            "Epoch=6 - Train Loss:=3253.177734375, Evaluation Loss:=0.5479984879493713, Train Acc:=84.22833333333334, Evaluation Acc:=83.00999999999999\n",
            "Epoch=7 - Train Loss:=2920.768310546875, Evaluation Loss:=0.5136056542396545, Train Acc:=85.81166666666667, Evaluation Acc:=84.39999999999999\n",
            "Epoch=8 - Train Loss:=2676.80810546875, Evaluation Loss:=0.5117059946060181, Train Acc:=85.58333333333333, Evaluation Acc:=84.16\n",
            "Epoch=9 - Train Loss:=2489.503173828125, Evaluation Loss:=0.7195290327072144, Train Acc:=81.72333333333334, Evaluation Acc:=80.5\n",
            "Epoch=10 - Train Loss:=2343.592529296875, Evaluation Loss:=0.5014039278030396, Train Acc:=86.36500000000001, Evaluation Acc:=84.48\n",
            "Epoch=11 - Train Loss:=2224.3994140625, Evaluation Loss:=0.5275018811225891, Train Acc:=85.04, Evaluation Acc:=83.08\n",
            "Epoch=12 - Train Loss:=2134.314208984375, Evaluation Loss:=0.4869873523712158, Train Acc:=86.04666666666667, Evaluation Acc:=84.36\n",
            "Epoch=13 - Train Loss:=2069.898193359375, Evaluation Loss:=0.5424595475196838, Train Acc:=85.08166666666666, Evaluation Acc:=83.21\n",
            "Epoch=14 - Train Loss:=2017.3067626953125, Evaluation Loss:=0.6222798824310303, Train Acc:=83.82, Evaluation Acc:=82.39\n",
            "Epoch=15 - Train Loss:=1971.7303466796875, Evaluation Loss:=0.5675449967384338, Train Acc:=84.705, Evaluation Acc:=83.05\n",
            "Epoch=16 - Train Loss:=1940.7572021484375, Evaluation Loss:=0.49237510561943054, Train Acc:=86.30333333333333, Evaluation Acc:=84.74000000000001\n",
            "Epoch=17 - Train Loss:=1931.32958984375, Evaluation Loss:=0.6034356951713562, Train Acc:=84.07333333333334, Evaluation Acc:=82.39\n",
            "Epoch=18 - Train Loss:=1919.9140625, Evaluation Loss:=0.6125349402427673, Train Acc:=84.32666666666667, Evaluation Acc:=82.62\n",
            "Epoch=19 - Train Loss:=1901.428955078125, Evaluation Loss:=0.46276065707206726, Train Acc:=86.91333333333333, Evaluation Acc:=84.95\n",
            "Epoch=0 - Train Loss:=19857.373046875, Evaluation Loss:=0.8607620000839233, Train Acc:=82.75833333333334, Evaluation Acc:=81.07\n",
            "Epoch=1 - Train Loss:=10114.615234375, Evaluation Loss:=0.5565473437309265, Train Acc:=85.72833333333332, Evaluation Acc:=83.6\n",
            "Epoch=2 - Train Loss:=7195.95556640625, Evaluation Loss:=0.5713642835617065, Train Acc:=84.73833333333334, Evaluation Acc:=83.2\n",
            "Epoch=3 - Train Loss:=5557.93603515625, Evaluation Loss:=0.5195657014846802, Train Acc:=85.01333333333334, Evaluation Acc:=83.3\n",
            "Epoch=4 - Train Loss:=4543.46337890625, Evaluation Loss:=0.5275859832763672, Train Acc:=85.93499999999999, Evaluation Acc:=83.91\n",
            "Epoch=5 - Train Loss:=3833.87890625, Evaluation Loss:=0.5082367062568665, Train Acc:=85.52166666666666, Evaluation Acc:=83.93\n",
            "Epoch=6 - Train Loss:=3322.376953125, Evaluation Loss:=0.5720675587654114, Train Acc:=85.69166666666666, Evaluation Acc:=84.11\n",
            "Epoch=7 - Train Loss:=2973.181884765625, Evaluation Loss:=0.5144992470741272, Train Acc:=85.47833333333334, Evaluation Acc:=83.91999999999999\n",
            "Epoch=8 - Train Loss:=2698.061767578125, Evaluation Loss:=0.5598667860031128, Train Acc:=84.98833333333333, Evaluation Acc:=83.42\n",
            "Epoch=9 - Train Loss:=2495.14697265625, Evaluation Loss:=0.5169793367385864, Train Acc:=85.80166666666666, Evaluation Acc:=84.27\n",
            "Epoch=10 - Train Loss:=2345.10009765625, Evaluation Loss:=0.5308278203010559, Train Acc:=85.32499999999999, Evaluation Acc:=83.43\n",
            "Epoch=11 - Train Loss:=2226.660400390625, Evaluation Loss:=0.4953102469444275, Train Acc:=86.04, Evaluation Acc:=84.26\n",
            "Epoch=12 - Train Loss:=2148.53173828125, Evaluation Loss:=0.5945613384246826, Train Acc:=84.275, Evaluation Acc:=82.71\n",
            "Epoch=13 - Train Loss:=2076.43798828125, Evaluation Loss:=0.5172069072723389, Train Acc:=85.08166666666666, Evaluation Acc:=83.52000000000001\n",
            "Epoch=14 - Train Loss:=2024.24169921875, Evaluation Loss:=0.5533513426780701, Train Acc:=84.31, Evaluation Acc:=82.50999999999999\n",
            "Epoch=15 - Train Loss:=1986.0423583984375, Evaluation Loss:=0.6281254291534424, Train Acc:=83.615, Evaluation Acc:=82.09\n",
            "Epoch=16 - Train Loss:=1953.4088134765625, Evaluation Loss:=0.5175879001617432, Train Acc:=86.065, Evaluation Acc:=84.0\n",
            "Epoch=17 - Train Loss:=1922.419677734375, Evaluation Loss:=0.5389673113822937, Train Acc:=85.41166666666666, Evaluation Acc:=83.85000000000001\n",
            "Epoch=18 - Train Loss:=1899.959716796875, Evaluation Loss:=0.6134172081947327, Train Acc:=84.15166666666667, Evaluation Acc:=82.88\n",
            "Epoch=19 - Train Loss:=1895.6981201171875, Evaluation Loss:=0.5462366938591003, Train Acc:=84.57000000000001, Evaluation Acc:=82.69999999999999\n",
            "Epoch=0 - Train Loss:=19576.248046875, Evaluation Loss:=0.8060587644577026, Train Acc:=84.13166666666667, Evaluation Acc:=82.59\n",
            "Epoch=1 - Train Loss:=9890.412109375, Evaluation Loss:=0.5612695217132568, Train Acc:=85.25166666666667, Evaluation Acc:=83.50999999999999\n",
            "Epoch=2 - Train Loss:=7061.47119140625, Evaluation Loss:=0.4936276972293854, Train Acc:=85.955, Evaluation Acc:=83.86\n",
            "Epoch=3 - Train Loss:=5493.21044921875, Evaluation Loss:=0.5711935758590698, Train Acc:=84.17666666666666, Evaluation Acc:=82.6\n",
            "Epoch=4 - Train Loss:=4474.986328125, Evaluation Loss:=0.620708703994751, Train Acc:=83.45166666666667, Evaluation Acc:=82.06\n",
            "Epoch=5 - Train Loss:=3775.606201171875, Evaluation Loss:=0.6111581325531006, Train Acc:=83.77666666666667, Evaluation Acc:=82.39999999999999\n",
            "Epoch=6 - Train Loss:=3297.1875, Evaluation Loss:=0.5504360795021057, Train Acc:=85.17666666666666, Evaluation Acc:=83.66\n",
            "Epoch=7 - Train Loss:=2944.489501953125, Evaluation Loss:=0.5300629138946533, Train Acc:=85.30666666666666, Evaluation Acc:=83.52000000000001\n",
            "Epoch=8 - Train Loss:=2668.19873046875, Evaluation Loss:=0.505390465259552, Train Acc:=85.44, Evaluation Acc:=84.26\n",
            "Epoch=9 - Train Loss:=2469.63427734375, Evaluation Loss:=0.5072697997093201, Train Acc:=85.00333333333333, Evaluation Acc:=83.33\n",
            "Epoch=10 - Train Loss:=2327.781005859375, Evaluation Loss:=0.49651816487312317, Train Acc:=86.435, Evaluation Acc:=84.69\n",
            "Epoch=11 - Train Loss:=2203.260498046875, Evaluation Loss:=0.5161433815956116, Train Acc:=85.42166666666667, Evaluation Acc:=83.88\n",
            "Epoch=12 - Train Loss:=2120.59423828125, Evaluation Loss:=0.6158661246299744, Train Acc:=83.98, Evaluation Acc:=82.72\n",
            "Epoch=13 - Train Loss:=2041.5302734375, Evaluation Loss:=0.5062355399131775, Train Acc:=86.04666666666667, Evaluation Acc:=84.52\n",
            "Epoch=14 - Train Loss:=1991.6756591796875, Evaluation Loss:=0.5240591764450073, Train Acc:=85.87166666666667, Evaluation Acc:=83.85000000000001\n",
            "Epoch=15 - Train Loss:=1941.2078857421875, Evaluation Loss:=0.5459288358688354, Train Acc:=85.065, Evaluation Acc:=83.48\n",
            "Epoch=16 - Train Loss:=1904.895751953125, Evaluation Loss:=0.539952278137207, Train Acc:=85.64333333333335, Evaluation Acc:=83.41\n",
            "Epoch=17 - Train Loss:=1884.2655029296875, Evaluation Loss:=0.5003538727760315, Train Acc:=85.79, Evaluation Acc:=84.11999999999999\n",
            "Epoch=18 - Train Loss:=1865.7003173828125, Evaluation Loss:=0.5267146229743958, Train Acc:=85.885, Evaluation Acc:=83.81\n",
            "Epoch=19 - Train Loss:=1861.3292236328125, Evaluation Loss:=0.5613508820533752, Train Acc:=84.41166666666666, Evaluation Acc:=83.04\n",
            "Epoch=0 - Train Loss:=19228.595703125, Evaluation Loss:=0.7654163837432861, Train Acc:=83.85666666666667, Evaluation Acc:=82.54\n",
            "Epoch=1 - Train Loss:=9549.4755859375, Evaluation Loss:=0.65566486120224, Train Acc:=82.41833333333334, Evaluation Acc:=80.9\n",
            "Epoch=2 - Train Loss:=6755.16259765625, Evaluation Loss:=0.6222225427627563, Train Acc:=83.03333333333333, Evaluation Acc:=81.75\n",
            "Epoch=3 - Train Loss:=5174.8193359375, Evaluation Loss:=0.5284166932106018, Train Acc:=85.245, Evaluation Acc:=83.48\n",
            "Epoch=4 - Train Loss:=4212.58056640625, Evaluation Loss:=0.5226883292198181, Train Acc:=85.52833333333332, Evaluation Acc:=83.73\n",
            "Epoch=5 - Train Loss:=3572.029541015625, Evaluation Loss:=0.5017604231834412, Train Acc:=85.93333333333332, Evaluation Acc:=84.27\n",
            "Epoch=6 - Train Loss:=3113.41943359375, Evaluation Loss:=0.532211422920227, Train Acc:=85.34, Evaluation Acc:=83.56\n",
            "Epoch=7 - Train Loss:=2792.423828125, Evaluation Loss:=0.5040838718414307, Train Acc:=85.86500000000001, Evaluation Acc:=84.13000000000001\n",
            "Epoch=8 - Train Loss:=2547.003662109375, Evaluation Loss:=0.6051573157310486, Train Acc:=83.63000000000001, Evaluation Acc:=82.33\n",
            "Epoch=9 - Train Loss:=2372.82080078125, Evaluation Loss:=0.4990919828414917, Train Acc:=85.52499999999999, Evaluation Acc:=83.97\n",
            "Epoch=10 - Train Loss:=2243.72314453125, Evaluation Loss:=0.4924015700817108, Train Acc:=86.25, Evaluation Acc:=84.5\n",
            "Epoch=11 - Train Loss:=2138.481201171875, Evaluation Loss:=0.49452686309814453, Train Acc:=86.22166666666666, Evaluation Acc:=84.53\n",
            "Epoch=12 - Train Loss:=2057.896484375, Evaluation Loss:=0.5525606274604797, Train Acc:=84.52, Evaluation Acc:=82.54\n",
            "Epoch=13 - Train Loss:=1987.4517822265625, Evaluation Loss:=0.48655903339385986, Train Acc:=86.24833333333333, Evaluation Acc:=84.59\n",
            "Epoch=14 - Train Loss:=1939.8262939453125, Evaluation Loss:=0.5450243949890137, Train Acc:=84.52333333333333, Evaluation Acc:=82.89999999999999\n",
            "Epoch=15 - Train Loss:=1910.6131591796875, Evaluation Loss:=0.5327743291854858, Train Acc:=84.63333333333334, Evaluation Acc:=82.97\n",
            "Epoch=16 - Train Loss:=1876.2386474609375, Evaluation Loss:=0.6472849249839783, Train Acc:=83.17333333333333, Evaluation Acc:=81.74\n",
            "Epoch=17 - Train Loss:=1853.91943359375, Evaluation Loss:=0.562648355960846, Train Acc:=84.785, Evaluation Acc:=83.11\n",
            "Epoch=18 - Train Loss:=1832.5555419921875, Evaluation Loss:=0.501259446144104, Train Acc:=86.29, Evaluation Acc:=84.36\n",
            "Epoch=19 - Train Loss:=1822.0848388671875, Evaluation Loss:=0.5161204934120178, Train Acc:=85.56, Evaluation Acc:=83.61\n",
            "Epoch=0 - Train Loss:=19920.60546875, Evaluation Loss:=0.863473653793335, Train Acc:=82.55, Evaluation Acc:=81.17\n",
            "Epoch=1 - Train Loss:=10120.0078125, Evaluation Loss:=0.7719755172729492, Train Acc:=81.54666666666667, Evaluation Acc:=79.86999999999999\n",
            "Epoch=2 - Train Loss:=7234.0322265625, Evaluation Loss:=0.6240419149398804, Train Acc:=83.635, Evaluation Acc:=82.17\n",
            "Epoch=3 - Train Loss:=5631.3447265625, Evaluation Loss:=0.5393881797790527, Train Acc:=85.35666666666667, Evaluation Acc:=83.93\n",
            "Epoch=4 - Train Loss:=4593.234375, Evaluation Loss:=0.49854448437690735, Train Acc:=86.065, Evaluation Acc:=84.06\n",
            "Epoch=5 - Train Loss:=3872.011474609375, Evaluation Loss:=0.5093622803688049, Train Acc:=85.92166666666667, Evaluation Acc:=84.09\n",
            "Epoch=6 - Train Loss:=3363.251220703125, Evaluation Loss:=0.5325160026550293, Train Acc:=85.36833333333334, Evaluation Acc:=83.78\n",
            "Epoch=7 - Train Loss:=3006.418701171875, Evaluation Loss:=0.5554986596107483, Train Acc:=85.47166666666666, Evaluation Acc:=83.78999999999999\n",
            "Epoch=8 - Train Loss:=2726.160888671875, Evaluation Loss:=0.5268966555595398, Train Acc:=85.38166666666666, Evaluation Acc:=83.77\n",
            "Epoch=9 - Train Loss:=2503.504150390625, Evaluation Loss:=0.5508462190628052, Train Acc:=85.78333333333333, Evaluation Acc:=84.2\n",
            "Epoch=10 - Train Loss:=2343.498291015625, Evaluation Loss:=0.49679073691368103, Train Acc:=86.03166666666667, Evaluation Acc:=84.32\n",
            "Epoch=11 - Train Loss:=2224.187744140625, Evaluation Loss:=0.5098209977149963, Train Acc:=86.09166666666667, Evaluation Acc:=84.3\n",
            "Epoch=12 - Train Loss:=2126.548095703125, Evaluation Loss:=0.6070144772529602, Train Acc:=84.01833333333333, Evaluation Acc:=82.80999999999999\n",
            "Epoch=13 - Train Loss:=2053.009765625, Evaluation Loss:=0.49623167514801025, Train Acc:=85.495, Evaluation Acc:=83.50999999999999\n",
            "Epoch=14 - Train Loss:=2001.3641357421875, Evaluation Loss:=0.49041932821273804, Train Acc:=86.54666666666667, Evaluation Acc:=84.71\n",
            "Epoch=15 - Train Loss:=1958.5914306640625, Evaluation Loss:=0.5432340502738953, Train Acc:=85.45166666666667, Evaluation Acc:=84.39\n",
            "Epoch=16 - Train Loss:=1931.458984375, Evaluation Loss:=0.473237007856369, Train Acc:=87.11166666666666, Evaluation Acc:=85.34\n",
            "Epoch=17 - Train Loss:=1910.33056640625, Evaluation Loss:=0.527995228767395, Train Acc:=84.83166666666668, Evaluation Acc:=82.98\n",
            "Epoch=18 - Train Loss:=1895.2047119140625, Evaluation Loss:=0.6027093529701233, Train Acc:=84.24666666666667, Evaluation Acc:=82.6\n",
            "Epoch=19 - Train Loss:=1878.662353515625, Evaluation Loss:=0.47674646973609924, Train Acc:=86.095, Evaluation Acc:=84.14\n",
            "Epoch=0 - Train Loss:=19460.388671875, Evaluation Loss:=0.8855909109115601, Train Acc:=82.24166666666667, Evaluation Acc:=80.67999999999999\n",
            "Epoch=1 - Train Loss:=9841.17578125, Evaluation Loss:=0.681186854839325, Train Acc:=81.96, Evaluation Acc:=79.93\n",
            "Epoch=2 - Train Loss:=6982.66015625, Evaluation Loss:=0.6149015426635742, Train Acc:=84.02166666666666, Evaluation Acc:=82.28999999999999\n",
            "Epoch=3 - Train Loss:=5385.1015625, Evaluation Loss:=0.5281159281730652, Train Acc:=85.64, Evaluation Acc:=83.65\n",
            "Epoch=4 - Train Loss:=4410.8427734375, Evaluation Loss:=0.5705962181091309, Train Acc:=84.32666666666667, Evaluation Acc:=82.65\n",
            "Epoch=5 - Train Loss:=3758.029296875, Evaluation Loss:=0.5376511812210083, Train Acc:=85.85833333333333, Evaluation Acc:=84.04\n",
            "Epoch=6 - Train Loss:=3281.30224609375, Evaluation Loss:=0.5432060360908508, Train Acc:=85.19666666666666, Evaluation Acc:=83.59\n",
            "Epoch=7 - Train Loss:=2936.445556640625, Evaluation Loss:=0.5053348541259766, Train Acc:=85.41, Evaluation Acc:=83.25\n",
            "Epoch=8 - Train Loss:=2678.3935546875, Evaluation Loss:=0.5508505702018738, Train Acc:=84.83500000000001, Evaluation Acc:=82.99\n",
            "Epoch=9 - Train Loss:=2483.519775390625, Evaluation Loss:=0.4767876863479614, Train Acc:=86.98166666666667, Evaluation Acc:=85.02\n",
            "Epoch=10 - Train Loss:=2320.62744140625, Evaluation Loss:=0.48858603835105896, Train Acc:=86.47666666666667, Evaluation Acc:=84.61999999999999\n",
            "Epoch=11 - Train Loss:=2210.657958984375, Evaluation Loss:=0.4943329095840454, Train Acc:=86.63666666666666, Evaluation Acc:=84.52\n",
            "Epoch=12 - Train Loss:=2129.429443359375, Evaluation Loss:=0.554144024848938, Train Acc:=85.32, Evaluation Acc:=83.59\n",
            "Epoch=13 - Train Loss:=2071.099365234375, Evaluation Loss:=0.5020843148231506, Train Acc:=85.92166666666667, Evaluation Acc:=83.67\n",
            "Epoch=14 - Train Loss:=2021.63623046875, Evaluation Loss:=0.5228843092918396, Train Acc:=85.22333333333333, Evaluation Acc:=83.50999999999999\n",
            "Epoch=15 - Train Loss:=1988.23095703125, Evaluation Loss:=0.5424059629440308, Train Acc:=85.80333333333333, Evaluation Acc:=83.85000000000001\n",
            "Epoch=16 - Train Loss:=1962.8914794921875, Evaluation Loss:=0.4846663475036621, Train Acc:=86.69, Evaluation Acc:=84.76\n",
            "Epoch=17 - Train Loss:=1941.5325927734375, Evaluation Loss:=0.5751248598098755, Train Acc:=84.59666666666666, Evaluation Acc:=83.03\n",
            "Epoch=18 - Train Loss:=1920.5233154296875, Evaluation Loss:=0.5091078877449036, Train Acc:=85.685, Evaluation Acc:=84.21\n",
            "Epoch=19 - Train Loss:=1909.170654296875, Evaluation Loss:=0.5386794209480286, Train Acc:=85.57666666666667, Evaluation Acc:=83.81\n",
            "Epoch=0 - Train Loss:=19138.904296875, Evaluation Loss:=0.8392964005470276, Train Acc:=81.98166666666667, Evaluation Acc:=80.57\n",
            "Epoch=1 - Train Loss:=9398.8828125, Evaluation Loss:=0.6074235439300537, Train Acc:=83.77833333333334, Evaluation Acc:=81.95\n",
            "Epoch=2 - Train Loss:=6652.52685546875, Evaluation Loss:=0.5034569501876831, Train Acc:=86.12666666666667, Evaluation Acc:=84.41\n",
            "Epoch=3 - Train Loss:=5141.537109375, Evaluation Loss:=0.5131067633628845, Train Acc:=85.47666666666667, Evaluation Acc:=83.73\n",
            "Epoch=4 - Train Loss:=4210.70654296875, Evaluation Loss:=0.52862948179245, Train Acc:=85.62666666666667, Evaluation Acc:=83.85000000000001\n",
            "Epoch=5 - Train Loss:=3584.50390625, Evaluation Loss:=0.4899185597896576, Train Acc:=86.55333333333334, Evaluation Acc:=84.2\n",
            "Epoch=6 - Train Loss:=3124.789794921875, Evaluation Loss:=0.5253063440322876, Train Acc:=85.67, Evaluation Acc:=84.04\n",
            "Epoch=7 - Train Loss:=2783.864990234375, Evaluation Loss:=0.5395649671554565, Train Acc:=85.35000000000001, Evaluation Acc:=83.76\n",
            "Epoch=8 - Train Loss:=2546.213134765625, Evaluation Loss:=0.5839380025863647, Train Acc:=83.71, Evaluation Acc:=82.42\n",
            "Epoch=9 - Train Loss:=2363.561279296875, Evaluation Loss:=0.5014790892601013, Train Acc:=86.005, Evaluation Acc:=84.13000000000001\n",
            "Epoch=10 - Train Loss:=2236.86474609375, Evaluation Loss:=0.5783034563064575, Train Acc:=84.31833333333333, Evaluation Acc:=82.65\n",
            "Epoch=11 - Train Loss:=2134.189453125, Evaluation Loss:=0.53059983253479, Train Acc:=85.26333333333334, Evaluation Acc:=83.36\n",
            "Epoch=12 - Train Loss:=2042.788330078125, Evaluation Loss:=0.5093592405319214, Train Acc:=85.47833333333334, Evaluation Acc:=83.64\n",
            "Epoch=13 - Train Loss:=1989.1566162109375, Evaluation Loss:=0.5459969639778137, Train Acc:=85.81, Evaluation Acc:=84.23\n",
            "Epoch=14 - Train Loss:=1939.90234375, Evaluation Loss:=0.576021671295166, Train Acc:=83.435, Evaluation Acc:=81.35\n",
            "Epoch=15 - Train Loss:=1910.74267578125, Evaluation Loss:=0.523326575756073, Train Acc:=85.42166666666667, Evaluation Acc:=83.41\n",
            "Epoch=16 - Train Loss:=1893.4652099609375, Evaluation Loss:=0.5552522540092468, Train Acc:=85.65166666666667, Evaluation Acc:=83.8\n",
            "Epoch=17 - Train Loss:=1870.128662109375, Evaluation Loss:=0.5156441926956177, Train Acc:=85.44666666666667, Evaluation Acc:=83.44\n",
            "Epoch=18 - Train Loss:=1854.9976806640625, Evaluation Loss:=0.5776325464248657, Train Acc:=84.19666666666666, Evaluation Acc:=82.3\n",
            "Epoch=19 - Train Loss:=1853.702392578125, Evaluation Loss:=0.5664527416229248, Train Acc:=85.12833333333333, Evaluation Acc:=83.34\n",
            "Epoch=0 - Train Loss:=19736.12890625, Evaluation Loss:=0.7089576125144958, Train Acc:=83.77833333333334, Evaluation Acc:=82.08\n",
            "Epoch=1 - Train Loss:=9982.6650390625, Evaluation Loss:=0.6057618856430054, Train Acc:=84.93, Evaluation Acc:=83.15\n",
            "Epoch=2 - Train Loss:=7171.31201171875, Evaluation Loss:=0.48256465792655945, Train Acc:=86.345, Evaluation Acc:=84.47\n",
            "Epoch=3 - Train Loss:=5531.3154296875, Evaluation Loss:=0.5379972457885742, Train Acc:=84.84666666666666, Evaluation Acc:=82.89\n",
            "Epoch=4 - Train Loss:=4497.1181640625, Evaluation Loss:=0.49300503730773926, Train Acc:=85.93666666666667, Evaluation Acc:=84.1\n",
            "Epoch=5 - Train Loss:=3786.47802734375, Evaluation Loss:=0.5083761811256409, Train Acc:=85.97, Evaluation Acc:=84.37\n",
            "Epoch=6 - Train Loss:=3288.1201171875, Evaluation Loss:=0.5124456286430359, Train Acc:=85.36333333333333, Evaluation Acc:=83.45\n",
            "Epoch=7 - Train Loss:=2912.554443359375, Evaluation Loss:=0.5251654982566833, Train Acc:=85.2, Evaluation Acc:=83.54\n",
            "Epoch=8 - Train Loss:=2638.78369140625, Evaluation Loss:=0.5045246481895447, Train Acc:=86.30499999999999, Evaluation Acc:=84.34\n",
            "Epoch=9 - Train Loss:=2433.921630859375, Evaluation Loss:=0.5142914056777954, Train Acc:=85.16666666666667, Evaluation Acc:=83.34\n",
            "Epoch=10 - Train Loss:=2277.8251953125, Evaluation Loss:=0.5091471672058105, Train Acc:=85.94666666666667, Evaluation Acc:=84.38\n",
            "Epoch=11 - Train Loss:=2162.39208984375, Evaluation Loss:=0.47435423731803894, Train Acc:=86.50833333333333, Evaluation Acc:=84.46000000000001\n",
            "Epoch=12 - Train Loss:=2078.775390625, Evaluation Loss:=0.5323952436447144, Train Acc:=85.17166666666667, Evaluation Acc:=83.38\n",
            "Epoch=13 - Train Loss:=2016.12060546875, Evaluation Loss:=0.4925714433193207, Train Acc:=86.01833333333333, Evaluation Acc:=83.94\n",
            "Epoch=14 - Train Loss:=1965.9744873046875, Evaluation Loss:=0.5136159658432007, Train Acc:=85.82, Evaluation Acc:=84.24000000000001\n",
            "Epoch=15 - Train Loss:=1926.49609375, Evaluation Loss:=0.5000457167625427, Train Acc:=86.535, Evaluation Acc:=84.92\n",
            "Epoch=16 - Train Loss:=1900.5323486328125, Evaluation Loss:=0.5531636476516724, Train Acc:=84.80333333333333, Evaluation Acc:=83.25\n",
            "Epoch=17 - Train Loss:=1881.3204345703125, Evaluation Loss:=0.482561856508255, Train Acc:=86.015, Evaluation Acc:=84.45\n",
            "Epoch=18 - Train Loss:=1878.6873779296875, Evaluation Loss:=0.48041367530822754, Train Acc:=86.33999999999999, Evaluation Acc:=84.35000000000001\n",
            "Epoch=19 - Train Loss:=1865.5223388671875, Evaluation Loss:=0.6451396346092224, Train Acc:=82.54833333333333, Evaluation Acc:=81.04\n",
            "Epoch=0 - Train Loss:=19664.55078125, Evaluation Loss:=0.9097564220428467, Train Acc:=83.075, Evaluation Acc:=81.97\n",
            "Epoch=1 - Train Loss:=9729.537109375, Evaluation Loss:=0.6428954005241394, Train Acc:=84.05, Evaluation Acc:=82.46\n",
            "Epoch=2 - Train Loss:=6904.3623046875, Evaluation Loss:=0.5196595788002014, Train Acc:=85.98, Evaluation Acc:=84.44\n",
            "Epoch=3 - Train Loss:=5343.27783203125, Evaluation Loss:=0.6717116832733154, Train Acc:=83.33500000000001, Evaluation Acc:=82.27\n",
            "Epoch=4 - Train Loss:=4335.25732421875, Evaluation Loss:=0.5595371127128601, Train Acc:=84.81333333333333, Evaluation Acc:=82.84\n",
            "Epoch=5 - Train Loss:=3660.803955078125, Evaluation Loss:=0.4964607357978821, Train Acc:=86.50333333333333, Evaluation Acc:=84.65\n",
            "Epoch=6 - Train Loss:=3206.83056640625, Evaluation Loss:=0.6885291337966919, Train Acc:=81.24, Evaluation Acc:=79.55\n",
            "Epoch=7 - Train Loss:=2858.32763671875, Evaluation Loss:=0.6043800115585327, Train Acc:=84.05333333333334, Evaluation Acc:=82.27\n",
            "Epoch=8 - Train Loss:=2619.602783203125, Evaluation Loss:=0.4736856520175934, Train Acc:=86.67833333333334, Evaluation Acc:=84.89999999999999\n",
            "Epoch=9 - Train Loss:=2425.67529296875, Evaluation Loss:=0.4851069450378418, Train Acc:=85.98, Evaluation Acc:=84.31\n",
            "Epoch=10 - Train Loss:=2284.911376953125, Evaluation Loss:=0.5174541473388672, Train Acc:=85.84, Evaluation Acc:=84.14\n",
            "Epoch=11 - Train Loss:=2176.2216796875, Evaluation Loss:=0.5041022300720215, Train Acc:=85.98166666666667, Evaluation Acc:=84.1\n",
            "Epoch=12 - Train Loss:=2088.7412109375, Evaluation Loss:=0.5368237495422363, Train Acc:=85.70833333333333, Evaluation Acc:=84.03\n",
            "Epoch=13 - Train Loss:=2033.5606689453125, Evaluation Loss:=0.5455200672149658, Train Acc:=85.64500000000001, Evaluation Acc:=83.91999999999999\n",
            "Epoch=14 - Train Loss:=1973.380126953125, Evaluation Loss:=0.46507012844085693, Train Acc:=86.88166666666667, Evaluation Acc:=85.11\n",
            "Epoch=15 - Train Loss:=1934.2186279296875, Evaluation Loss:=0.5605345368385315, Train Acc:=84.52666666666667, Evaluation Acc:=82.98\n",
            "Epoch=16 - Train Loss:=1913.416748046875, Evaluation Loss:=0.5471858382225037, Train Acc:=84.65166666666667, Evaluation Acc:=83.19\n",
            "Epoch=17 - Train Loss:=1877.3560791015625, Evaluation Loss:=0.5543889403343201, Train Acc:=84.99, Evaluation Acc:=83.5\n",
            "Epoch=18 - Train Loss:=1856.911865234375, Evaluation Loss:=0.667423665523529, Train Acc:=82.64833333333334, Evaluation Acc:=81.37\n",
            "Epoch=19 - Train Loss:=1834.6903076171875, Evaluation Loss:=0.612084150314331, Train Acc:=84.26, Evaluation Acc:=83.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "5TAJUtVtno_H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "b2df859a-323b-4d0c-812c-80291fdeb86e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 83.451, and the variance is 1.149649000000003\n",
            "the average running time for one epoch is: 48.47898581147194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7f8dee3271d0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f8dab81a250>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dab81a790>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7f8dab821290>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f8dab81ad10>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7f8dee327790>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dee327cd0>]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model11: without regularization + custom optimizer"
      ],
      "metadata": {
        "id": "vSgvPzvD9FwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "optimizer = \"custom\"\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "learning_rate = 10e-5\n",
        "epochs = 20\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i + 11)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"fashion_mnist\")\n",
        "\n",
        "    model11 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer = optimizer)\n",
        "    train_losses, test_losses, train_accuracy, test_accuracy, _epoch_times = train(model11, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs)\n",
        "    acc = test(model11, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "1U_jPx4i9Pmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a8ff4a-b6a0-4330-a889-444ed9bc69a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=1884.739013671875, Evaluation Loss:=0.7140969634056091, Train Acc:=83.50166666666667, Evaluation Acc:=81.36\n",
            "Epoch=1 - Train Loss:=955.9222412109375, Evaluation Loss:=0.5643201470375061, Train Acc:=86.16166666666668, Evaluation Acc:=82.92\n",
            "Epoch=2 - Train Loss:=723.19580078125, Evaluation Loss:=0.48924532532691956, Train Acc:=88.43833333333333, Evaluation Acc:=84.98\n",
            "Epoch=3 - Train Loss:=618.942138671875, Evaluation Loss:=0.4266833961009979, Train Acc:=90.34666666666666, Evaluation Acc:=86.09\n",
            "Epoch=4 - Train Loss:=521.4966430664062, Evaluation Loss:=0.42026397585868835, Train Acc:=91.50500000000001, Evaluation Acc:=86.11\n",
            "Epoch=5 - Train Loss:=462.04229736328125, Evaluation Loss:=0.41882768273353577, Train Acc:=92.245, Evaluation Acc:=87.11\n",
            "Epoch=6 - Train Loss:=411.19110107421875, Evaluation Loss:=0.44027969241142273, Train Acc:=91.53999999999999, Evaluation Acc:=86.1\n",
            "Epoch=7 - Train Loss:=361.8301086425781, Evaluation Loss:=0.38612306118011475, Train Acc:=94.40166666666666, Evaluation Acc:=87.92999999999999\n",
            "Epoch=8 - Train Loss:=321.43829345703125, Evaluation Loss:=0.416637659072876, Train Acc:=94.06833333333333, Evaluation Acc:=87.17\n",
            "Epoch=9 - Train Loss:=285.5577087402344, Evaluation Loss:=0.3887535035610199, Train Acc:=95.50666666666666, Evaluation Acc:=88.24\n",
            "Epoch=10 - Train Loss:=261.81378173828125, Evaluation Loss:=0.42279282212257385, Train Acc:=95.475, Evaluation Acc:=87.64\n",
            "Epoch=11 - Train Loss:=225.77783203125, Evaluation Loss:=0.43066489696502686, Train Acc:=96.095, Evaluation Acc:=88.08\n",
            "Epoch=12 - Train Loss:=212.06016540527344, Evaluation Loss:=0.4620876908302307, Train Acc:=95.11500000000001, Evaluation Acc:=87.31\n",
            "Epoch=13 - Train Loss:=188.7758026123047, Evaluation Loss:=0.44026973843574524, Train Acc:=96.75166666666667, Evaluation Acc:=88.1\n",
            "Epoch=14 - Train Loss:=175.48275756835938, Evaluation Loss:=0.42022696137428284, Train Acc:=97.48333333333333, Evaluation Acc:=88.78\n",
            "Epoch=15 - Train Loss:=154.8367462158203, Evaluation Loss:=0.46129804849624634, Train Acc:=97.40166666666667, Evaluation Acc:=88.18\n",
            "Epoch=16 - Train Loss:=141.26925659179688, Evaluation Loss:=0.4689922332763672, Train Acc:=97.61833333333333, Evaluation Acc:=88.13\n",
            "Epoch=17 - Train Loss:=131.34141540527344, Evaluation Loss:=0.4702640771865845, Train Acc:=97.39333333333333, Evaluation Acc:=88.0\n",
            "Epoch=18 - Train Loss:=119.0248031616211, Evaluation Loss:=0.49362969398498535, Train Acc:=97.425, Evaluation Acc:=88.62\n",
            "Epoch=19 - Train Loss:=111.63709259033203, Evaluation Loss:=0.49018627405166626, Train Acc:=98.1, Evaluation Acc:=88.5\n",
            "Epoch=0 - Train Loss:=2050.268798828125, Evaluation Loss:=0.7058065533638, Train Acc:=84.83166666666668, Evaluation Acc:=81.89999999999999\n",
            "Epoch=1 - Train Loss:=1000.1254272460938, Evaluation Loss:=0.5865844488143921, Train Acc:=86.72333333333333, Evaluation Acc:=83.24000000000001\n",
            "Epoch=2 - Train Loss:=743.3540649414062, Evaluation Loss:=0.49721309542655945, Train Acc:=88.79833333333333, Evaluation Acc:=84.83000000000001\n",
            "Epoch=3 - Train Loss:=626.333740234375, Evaluation Loss:=0.46059364080429077, Train Acc:=90.07, Evaluation Acc:=85.33\n",
            "Epoch=4 - Train Loss:=544.02978515625, Evaluation Loss:=0.4866740107536316, Train Acc:=89.995, Evaluation Acc:=85.00999999999999\n",
            "Epoch=5 - Train Loss:=469.8108215332031, Evaluation Loss:=0.4736829102039337, Train Acc:=90.52833333333334, Evaluation Acc:=85.45\n",
            "Epoch=6 - Train Loss:=412.10479736328125, Evaluation Loss:=0.43512362241744995, Train Acc:=92.64333333333333, Evaluation Acc:=86.61999999999999\n",
            "Epoch=7 - Train Loss:=368.22900390625, Evaluation Loss:=0.41040560603141785, Train Acc:=93.86666666666666, Evaluation Acc:=87.18\n",
            "Epoch=8 - Train Loss:=320.47064208984375, Evaluation Loss:=0.4316394627094269, Train Acc:=93.77, Evaluation Acc:=87.72999999999999\n",
            "Epoch=9 - Train Loss:=288.35137939453125, Evaluation Loss:=0.4061424136161804, Train Acc:=95.24000000000001, Evaluation Acc:=87.82\n",
            "Epoch=10 - Train Loss:=256.76318359375, Evaluation Loss:=0.4275798201560974, Train Acc:=95.26166666666667, Evaluation Acc:=87.33\n",
            "Epoch=11 - Train Loss:=229.77426147460938, Evaluation Loss:=0.41273143887519836, Train Acc:=96.60333333333332, Evaluation Acc:=88.47\n",
            "Epoch=12 - Train Loss:=207.88729858398438, Evaluation Loss:=0.4354633688926697, Train Acc:=96.135, Evaluation Acc:=88.02\n",
            "Epoch=13 - Train Loss:=187.31796264648438, Evaluation Loss:=0.4236569404602051, Train Acc:=97.18, Evaluation Acc:=88.38000000000001\n",
            "Epoch=14 - Train Loss:=174.29139709472656, Evaluation Loss:=0.42817968130111694, Train Acc:=97.62166666666666, Evaluation Acc:=88.41\n",
            "Epoch=15 - Train Loss:=156.5273895263672, Evaluation Loss:=0.48873046040534973, Train Acc:=96.495, Evaluation Acc:=87.81\n",
            "Epoch=16 - Train Loss:=139.65260314941406, Evaluation Loss:=0.46162304282188416, Train Acc:=97.7, Evaluation Acc:=88.19\n",
            "Epoch=17 - Train Loss:=126.6081314086914, Evaluation Loss:=0.48489853739738464, Train Acc:=97.44, Evaluation Acc:=87.86\n",
            "Epoch=18 - Train Loss:=122.47334289550781, Evaluation Loss:=0.4932137131690979, Train Acc:=97.99666666666667, Evaluation Acc:=88.33\n",
            "Epoch=19 - Train Loss:=107.05423736572266, Evaluation Loss:=0.4900948703289032, Train Acc:=97.955, Evaluation Acc:=88.26\n",
            "Epoch=0 - Train Loss:=2023.1427001953125, Evaluation Loss:=0.6634216904640198, Train Acc:=85.48333333333333, Evaluation Acc:=82.85\n",
            "Epoch=1 - Train Loss:=1007.822998046875, Evaluation Loss:=0.6378875970840454, Train Acc:=83.96333333333334, Evaluation Acc:=80.62\n",
            "Epoch=2 - Train Loss:=768.42578125, Evaluation Loss:=0.5377112030982971, Train Acc:=87.09833333333333, Evaluation Acc:=83.66\n",
            "Epoch=3 - Train Loss:=628.5238037109375, Evaluation Loss:=0.4596931040287018, Train Acc:=89.715, Evaluation Acc:=85.14\n",
            "Epoch=4 - Train Loss:=535.60595703125, Evaluation Loss:=0.43206849694252014, Train Acc:=90.77, Evaluation Acc:=85.5\n",
            "Epoch=5 - Train Loss:=472.65692138671875, Evaluation Loss:=0.38932958245277405, Train Acc:=92.84666666666666, Evaluation Acc:=87.17\n",
            "Epoch=6 - Train Loss:=417.5814514160156, Evaluation Loss:=0.3828650712966919, Train Acc:=93.91666666666667, Evaluation Acc:=87.82\n",
            "Epoch=7 - Train Loss:=371.59173583984375, Evaluation Loss:=0.3820619583129883, Train Acc:=94.77499999999999, Evaluation Acc:=87.58\n",
            "Epoch=8 - Train Loss:=327.3949890136719, Evaluation Loss:=0.4439510703086853, Train Acc:=93.46666666666667, Evaluation Acc:=86.92\n",
            "Epoch=9 - Train Loss:=297.1321716308594, Evaluation Loss:=0.4016200602054596, Train Acc:=95.22500000000001, Evaluation Acc:=87.58\n",
            "Epoch=10 - Train Loss:=259.6931457519531, Evaluation Loss:=0.418819397687912, Train Acc:=95.43833333333333, Evaluation Acc:=87.44\n",
            "Epoch=11 - Train Loss:=235.9664764404297, Evaluation Loss:=0.44480225443840027, Train Acc:=95.60333333333332, Evaluation Acc:=87.39\n",
            "Epoch=12 - Train Loss:=210.779541015625, Evaluation Loss:=0.4247499406337738, Train Acc:=96.17333333333333, Evaluation Acc:=87.75\n",
            "Epoch=13 - Train Loss:=193.07916259765625, Evaluation Loss:=0.4213739335536957, Train Acc:=97.195, Evaluation Acc:=88.44\n",
            "Epoch=14 - Train Loss:=173.6681365966797, Evaluation Loss:=0.4456591308116913, Train Acc:=97.135, Evaluation Acc:=87.71\n",
            "Epoch=15 - Train Loss:=159.50892639160156, Evaluation Loss:=0.45561346411705017, Train Acc:=97.50333333333333, Evaluation Acc:=88.8\n",
            "Epoch=16 - Train Loss:=146.72898864746094, Evaluation Loss:=0.45750391483306885, Train Acc:=97.88833333333334, Evaluation Acc:=88.53\n",
            "Epoch=17 - Train Loss:=133.7111358642578, Evaluation Loss:=0.44904887676239014, Train Acc:=98.03, Evaluation Acc:=88.42999999999999\n",
            "Epoch=18 - Train Loss:=120.37499237060547, Evaluation Loss:=0.4598563015460968, Train Acc:=98.19, Evaluation Acc:=88.67\n",
            "Epoch=19 - Train Loss:=114.25657653808594, Evaluation Loss:=0.492511123418808, Train Acc:=97.95, Evaluation Acc:=88.46000000000001\n",
            "Epoch=0 - Train Loss:=1965.9324951171875, Evaluation Loss:=0.597551703453064, Train Acc:=85.67333333333333, Evaluation Acc:=83.24000000000001\n",
            "Epoch=1 - Train Loss:=930.9015502929688, Evaluation Loss:=0.5162460207939148, Train Acc:=87.77833333333334, Evaluation Acc:=84.85000000000001\n",
            "Epoch=2 - Train Loss:=727.7385864257812, Evaluation Loss:=0.5269161462783813, Train Acc:=87.23166666666667, Evaluation Acc:=83.81\n",
            "Epoch=3 - Train Loss:=615.7406005859375, Evaluation Loss:=0.41569238901138306, Train Acc:=90.69666666666667, Evaluation Acc:=86.17\n",
            "Epoch=4 - Train Loss:=532.6787109375, Evaluation Loss:=0.41280272603034973, Train Acc:=91.50333333333333, Evaluation Acc:=86.11999999999999\n",
            "Epoch=5 - Train Loss:=462.302978515625, Evaluation Loss:=0.42582520842552185, Train Acc:=91.84333333333333, Evaluation Acc:=86.61999999999999\n",
            "Epoch=6 - Train Loss:=409.8116760253906, Evaluation Loss:=0.4051177203655243, Train Acc:=93.00833333333334, Evaluation Acc:=87.22\n",
            "Epoch=7 - Train Loss:=361.3784484863281, Evaluation Loss:=0.41763320565223694, Train Acc:=93.22333333333333, Evaluation Acc:=87.06\n",
            "Epoch=8 - Train Loss:=324.4735412597656, Evaluation Loss:=0.39331743121147156, Train Acc:=94.73333333333333, Evaluation Acc:=87.92\n",
            "Epoch=9 - Train Loss:=290.6015930175781, Evaluation Loss:=0.42772039771080017, Train Acc:=94.63333333333334, Evaluation Acc:=86.8\n",
            "Epoch=10 - Train Loss:=260.2842102050781, Evaluation Loss:=0.40469130873680115, Train Acc:=95.77166666666666, Evaluation Acc:=88.11\n",
            "Epoch=11 - Train Loss:=231.3861541748047, Evaluation Loss:=0.41518720984458923, Train Acc:=96.09, Evaluation Acc:=88.42\n",
            "Epoch=12 - Train Loss:=207.0269775390625, Evaluation Loss:=0.4042081832885742, Train Acc:=97.24333333333334, Evaluation Acc:=88.6\n",
            "Epoch=13 - Train Loss:=182.5251922607422, Evaluation Loss:=0.4330608546733856, Train Acc:=96.75, Evaluation Acc:=88.14\n",
            "Epoch=14 - Train Loss:=171.60301208496094, Evaluation Loss:=0.43122953176498413, Train Acc:=97.32666666666667, Evaluation Acc:=88.39\n",
            "Epoch=15 - Train Loss:=154.4429931640625, Evaluation Loss:=0.4426456093788147, Train Acc:=97.30666666666666, Evaluation Acc:=88.92999999999999\n",
            "Epoch=16 - Train Loss:=136.76258850097656, Evaluation Loss:=0.44397979974746704, Train Acc:=98.07000000000001, Evaluation Acc:=88.88000000000001\n",
            "Epoch=17 - Train Loss:=131.37188720703125, Evaluation Loss:=0.46373188495635986, Train Acc:=97.79666666666667, Evaluation Acc:=88.25\n",
            "Epoch=18 - Train Loss:=117.47473907470703, Evaluation Loss:=0.4821968376636505, Train Acc:=98.11166666666666, Evaluation Acc:=88.44999999999999\n",
            "Epoch=19 - Train Loss:=113.41963958740234, Evaluation Loss:=0.4664779305458069, Train Acc:=98.52, Evaluation Acc:=88.77000000000001\n",
            "Epoch=0 - Train Loss:=2069.445556640625, Evaluation Loss:=0.6679586172103882, Train Acc:=85.07166666666667, Evaluation Acc:=82.78999999999999\n",
            "Epoch=1 - Train Loss:=1010.9865112304688, Evaluation Loss:=0.5368135571479797, Train Acc:=86.99166666666667, Evaluation Acc:=84.06\n",
            "Epoch=2 - Train Loss:=766.79248046875, Evaluation Loss:=0.4710695743560791, Train Acc:=89.24666666666667, Evaluation Acc:=85.21\n",
            "Epoch=3 - Train Loss:=640.4727172851562, Evaluation Loss:=0.424058735370636, Train Acc:=90.74833333333333, Evaluation Acc:=86.11999999999999\n",
            "Epoch=4 - Train Loss:=557.796875, Evaluation Loss:=0.43101152777671814, Train Acc:=90.98833333333334, Evaluation Acc:=85.67\n",
            "Epoch=5 - Train Loss:=484.6024169921875, Evaluation Loss:=0.4213935136795044, Train Acc:=92.14833333333333, Evaluation Acc:=86.63\n",
            "Epoch=6 - Train Loss:=420.0035705566406, Evaluation Loss:=0.4013795554637909, Train Acc:=93.36, Evaluation Acc:=87.3\n",
            "Epoch=7 - Train Loss:=382.805908203125, Evaluation Loss:=0.4371894598007202, Train Acc:=92.30166666666668, Evaluation Acc:=85.98\n",
            "Epoch=8 - Train Loss:=334.90264892578125, Evaluation Loss:=0.40029624104499817, Train Acc:=94.39999999999999, Evaluation Acc:=87.49\n",
            "Epoch=9 - Train Loss:=301.1769714355469, Evaluation Loss:=0.4772672951221466, Train Acc:=93.86833333333333, Evaluation Acc:=86.79\n",
            "Epoch=10 - Train Loss:=267.2870788574219, Evaluation Loss:=0.41661161184310913, Train Acc:=95.41, Evaluation Acc:=87.51\n",
            "Epoch=11 - Train Loss:=240.6255645751953, Evaluation Loss:=0.39223966002464294, Train Acc:=96.695, Evaluation Acc:=88.39\n",
            "Epoch=12 - Train Loss:=217.72372436523438, Evaluation Loss:=0.4231334924697876, Train Acc:=96.325, Evaluation Acc:=88.0\n",
            "Epoch=13 - Train Loss:=193.28004455566406, Evaluation Loss:=0.4346608519554138, Train Acc:=96.53166666666667, Evaluation Acc:=88.24\n",
            "Epoch=14 - Train Loss:=177.12863159179688, Evaluation Loss:=0.4492134749889374, Train Acc:=97.04666666666667, Evaluation Acc:=87.64999999999999\n",
            "Epoch=15 - Train Loss:=153.50851440429688, Evaluation Loss:=0.44606396555900574, Train Acc:=97.61999999999999, Evaluation Acc:=88.79\n",
            "Epoch=16 - Train Loss:=149.3325653076172, Evaluation Loss:=0.4675750136375427, Train Acc:=97.34666666666666, Evaluation Acc:=88.28\n",
            "Epoch=17 - Train Loss:=135.6341094970703, Evaluation Loss:=0.4644835889339447, Train Acc:=97.68666666666667, Evaluation Acc:=88.36\n",
            "Epoch=18 - Train Loss:=122.01500701904297, Evaluation Loss:=0.4311925768852234, Train Acc:=98.56, Evaluation Acc:=89.03\n",
            "Epoch=19 - Train Loss:=113.40729522705078, Evaluation Loss:=0.46355074644088745, Train Acc:=98.14833333333334, Evaluation Acc:=88.55\n",
            "Epoch=0 - Train Loss:=1932.822998046875, Evaluation Loss:=0.6968433856964111, Train Acc:=84.71666666666667, Evaluation Acc:=82.27\n",
            "Epoch=1 - Train Loss:=953.1770629882812, Evaluation Loss:=0.7562975287437439, Train Acc:=82.66, Evaluation Acc:=79.17999999999999\n",
            "Epoch=2 - Train Loss:=725.670654296875, Evaluation Loss:=0.5060972571372986, Train Acc:=88.63666666666667, Evaluation Acc:=84.39\n",
            "Epoch=3 - Train Loss:=608.715087890625, Evaluation Loss:=0.42349618673324585, Train Acc:=91.06333333333333, Evaluation Acc:=86.42999999999999\n",
            "Epoch=4 - Train Loss:=532.1240234375, Evaluation Loss:=0.43198320269584656, Train Acc:=91.27333333333333, Evaluation Acc:=86.18\n",
            "Epoch=5 - Train Loss:=457.9234924316406, Evaluation Loss:=0.4095408618450165, Train Acc:=93.17999999999999, Evaluation Acc:=86.79\n",
            "Epoch=6 - Train Loss:=405.54962158203125, Evaluation Loss:=0.42429155111312866, Train Acc:=93.24166666666667, Evaluation Acc:=86.8\n",
            "Epoch=7 - Train Loss:=357.0632629394531, Evaluation Loss:=0.47251400351524353, Train Acc:=92.07333333333332, Evaluation Acc:=85.16\n",
            "Epoch=8 - Train Loss:=325.14971923828125, Evaluation Loss:=0.41321781277656555, Train Acc:=94.91333333333334, Evaluation Acc:=87.6\n",
            "Epoch=9 - Train Loss:=280.8913879394531, Evaluation Loss:=0.40394505858421326, Train Acc:=95.71333333333332, Evaluation Acc:=87.28\n",
            "Epoch=10 - Train Loss:=257.8507385253906, Evaluation Loss:=0.3974236249923706, Train Acc:=96.09833333333333, Evaluation Acc:=88.21\n",
            "Epoch=11 - Train Loss:=229.4679718017578, Evaluation Loss:=0.4686287045478821, Train Acc:=95.13666666666667, Evaluation Acc:=86.95\n",
            "Epoch=12 - Train Loss:=210.06240844726562, Evaluation Loss:=0.41535431146621704, Train Acc:=96.93833333333333, Evaluation Acc:=87.94999999999999\n",
            "Epoch=13 - Train Loss:=181.6622772216797, Evaluation Loss:=0.43500879406929016, Train Acc:=97.42666666666666, Evaluation Acc:=88.0\n",
            "Epoch=14 - Train Loss:=172.12063598632812, Evaluation Loss:=0.4586242139339447, Train Acc:=96.54, Evaluation Acc:=87.72999999999999\n",
            "Epoch=15 - Train Loss:=156.2004852294922, Evaluation Loss:=0.44752246141433716, Train Acc:=97.49166666666666, Evaluation Acc:=88.2\n",
            "Epoch=16 - Train Loss:=137.3003387451172, Evaluation Loss:=0.4789932072162628, Train Acc:=97.37166666666667, Evaluation Acc:=87.81\n",
            "Epoch=17 - Train Loss:=131.5250244140625, Evaluation Loss:=0.4589924216270447, Train Acc:=98.31666666666666, Evaluation Acc:=88.53999999999999\n",
            "Epoch=18 - Train Loss:=117.00594329833984, Evaluation Loss:=0.4680551290512085, Train Acc:=98.49333333333334, Evaluation Acc:=88.74\n",
            "Epoch=19 - Train Loss:=113.45223236083984, Evaluation Loss:=0.4767571687698364, Train Acc:=98.58, Evaluation Acc:=88.52\n",
            "Epoch=0 - Train Loss:=1874.960693359375, Evaluation Loss:=0.5645560622215271, Train Acc:=86.00999999999999, Evaluation Acc:=83.24000000000001\n",
            "Epoch=1 - Train Loss:=928.5399780273438, Evaluation Loss:=0.6552206873893738, Train Acc:=85.205, Evaluation Acc:=82.72\n",
            "Epoch=2 - Train Loss:=719.4559326171875, Evaluation Loss:=0.48789283633232117, Train Acc:=88.72666666666666, Evaluation Acc:=84.25\n",
            "Epoch=3 - Train Loss:=608.929443359375, Evaluation Loss:=0.435514360666275, Train Acc:=90.46166666666666, Evaluation Acc:=85.27\n",
            "Epoch=4 - Train Loss:=519.102294921875, Evaluation Loss:=0.4180509150028229, Train Acc:=91.89666666666668, Evaluation Acc:=86.33999999999999\n",
            "Epoch=5 - Train Loss:=449.10296630859375, Evaluation Loss:=0.41730380058288574, Train Acc:=92.34666666666666, Evaluation Acc:=86.36\n",
            "Epoch=6 - Train Loss:=401.11419677734375, Evaluation Loss:=0.429964542388916, Train Acc:=92.95333333333333, Evaluation Acc:=86.76\n",
            "Epoch=7 - Train Loss:=350.722412109375, Evaluation Loss:=0.38781389594078064, Train Acc:=94.99833333333333, Evaluation Acc:=87.51\n",
            "Epoch=8 - Train Loss:=315.1504211425781, Evaluation Loss:=0.41543006896972656, Train Acc:=94.62333333333333, Evaluation Acc:=87.53999999999999\n",
            "Epoch=9 - Train Loss:=282.4190979003906, Evaluation Loss:=0.4153168797492981, Train Acc:=95.245, Evaluation Acc:=87.74\n",
            "Epoch=10 - Train Loss:=246.08969116210938, Evaluation Loss:=0.4068734645843506, Train Acc:=95.65666666666667, Evaluation Acc:=88.01\n",
            "Epoch=11 - Train Loss:=224.03330993652344, Evaluation Loss:=0.4143582582473755, Train Acc:=96.51166666666666, Evaluation Acc:=88.22\n",
            "Epoch=12 - Train Loss:=200.15345764160156, Evaluation Loss:=0.45742836594581604, Train Acc:=95.44500000000001, Evaluation Acc:=87.5\n",
            "Epoch=13 - Train Loss:=182.57196044921875, Evaluation Loss:=0.44588378071784973, Train Acc:=96.11166666666666, Evaluation Acc:=88.12\n",
            "Epoch=14 - Train Loss:=168.28907775878906, Evaluation Loss:=0.4232015609741211, Train Acc:=97.36666666666667, Evaluation Acc:=88.05\n",
            "Epoch=15 - Train Loss:=146.8489990234375, Evaluation Loss:=0.44982120394706726, Train Acc:=97.70166666666667, Evaluation Acc:=88.41\n",
            "Epoch=16 - Train Loss:=139.89407348632812, Evaluation Loss:=0.4509243071079254, Train Acc:=97.72666666666666, Evaluation Acc:=87.94999999999999\n",
            "Epoch=17 - Train Loss:=126.03204345703125, Evaluation Loss:=0.44681718945503235, Train Acc:=98.22166666666666, Evaluation Acc:=88.55\n",
            "Epoch=18 - Train Loss:=116.21466064453125, Evaluation Loss:=0.46618983149528503, Train Acc:=98.17666666666666, Evaluation Acc:=88.21\n",
            "Epoch=19 - Train Loss:=110.1856689453125, Evaluation Loss:=0.4770488142967224, Train Acc:=98.375, Evaluation Acc:=88.19\n",
            "Epoch=0 - Train Loss:=1980.6839599609375, Evaluation Loss:=0.666743278503418, Train Acc:=84.44, Evaluation Acc:=81.81\n",
            "Epoch=1 - Train Loss:=960.5513305664062, Evaluation Loss:=0.5413141846656799, Train Acc:=87.19166666666666, Evaluation Acc:=84.25\n",
            "Epoch=2 - Train Loss:=756.3125610351562, Evaluation Loss:=0.4958738684654236, Train Acc:=88.47, Evaluation Acc:=84.88\n",
            "Epoch=3 - Train Loss:=626.228515625, Evaluation Loss:=0.4509764611721039, Train Acc:=89.57333333333334, Evaluation Acc:=85.07000000000001\n",
            "Epoch=4 - Train Loss:=537.7330932617188, Evaluation Loss:=0.42747989296913147, Train Acc:=91.17333333333333, Evaluation Acc:=85.95\n",
            "Epoch=5 - Train Loss:=462.1123046875, Evaluation Loss:=0.39330482482910156, Train Acc:=92.455, Evaluation Acc:=87.58\n",
            "Epoch=6 - Train Loss:=416.7381896972656, Evaluation Loss:=0.4470697343349457, Train Acc:=91.69, Evaluation Acc:=86.1\n",
            "Epoch=7 - Train Loss:=369.3800048828125, Evaluation Loss:=0.38979029655456543, Train Acc:=94.69166666666666, Evaluation Acc:=88.18\n",
            "Epoch=8 - Train Loss:=333.5605773925781, Evaluation Loss:=0.4062938392162323, Train Acc:=94.19833333333332, Evaluation Acc:=87.21\n",
            "Epoch=9 - Train Loss:=292.35235595703125, Evaluation Loss:=0.4272648096084595, Train Acc:=94.69, Evaluation Acc:=87.69\n",
            "Epoch=10 - Train Loss:=265.3254699707031, Evaluation Loss:=0.4389956593513489, Train Acc:=94.305, Evaluation Acc:=86.88\n",
            "Epoch=11 - Train Loss:=234.77354431152344, Evaluation Loss:=0.39557063579559326, Train Acc:=96.39, Evaluation Acc:=87.83\n",
            "Epoch=12 - Train Loss:=217.52618408203125, Evaluation Loss:=0.3887699246406555, Train Acc:=97.14666666666668, Evaluation Acc:=88.66000000000001\n",
            "Epoch=13 - Train Loss:=194.9824981689453, Evaluation Loss:=0.42725273966789246, Train Acc:=96.40666666666667, Evaluation Acc:=88.26\n",
            "Epoch=14 - Train Loss:=178.54238891601562, Evaluation Loss:=0.40649956464767456, Train Acc:=97.45333333333333, Evaluation Acc:=88.67\n",
            "Epoch=15 - Train Loss:=162.89923095703125, Evaluation Loss:=0.43142640590667725, Train Acc:=97.43333333333334, Evaluation Acc:=88.31\n",
            "Epoch=16 - Train Loss:=143.55979919433594, Evaluation Loss:=0.4325953722000122, Train Acc:=97.89, Evaluation Acc:=88.47\n",
            "Epoch=17 - Train Loss:=133.05453491210938, Evaluation Loss:=0.46122080087661743, Train Acc:=97.58333333333333, Evaluation Acc:=88.44999999999999\n",
            "Epoch=18 - Train Loss:=127.15890502929688, Evaluation Loss:=0.4732765555381775, Train Acc:=98.15, Evaluation Acc:=88.39\n",
            "Epoch=19 - Train Loss:=106.05168151855469, Evaluation Loss:=0.48308631777763367, Train Acc:=98.275, Evaluation Acc:=88.77000000000001\n",
            "Epoch=0 - Train Loss:=2020.819580078125, Evaluation Loss:=0.7490240335464478, Train Acc:=84.00666666666666, Evaluation Acc:=82.02000000000001\n",
            "Epoch=1 - Train Loss:=963.957763671875, Evaluation Loss:=0.5636107325553894, Train Acc:=86.55333333333334, Evaluation Acc:=83.47\n",
            "Epoch=2 - Train Loss:=749.4058837890625, Evaluation Loss:=0.45224741101264954, Train Acc:=89.60166666666667, Evaluation Acc:=85.77\n",
            "Epoch=3 - Train Loss:=623.02978515625, Evaluation Loss:=0.46720224618911743, Train Acc:=89.79833333333333, Evaluation Acc:=86.09\n",
            "Epoch=4 - Train Loss:=537.14794921875, Evaluation Loss:=0.43658125400543213, Train Acc:=90.945, Evaluation Acc:=85.58\n",
            "Epoch=5 - Train Loss:=470.8250427246094, Evaluation Loss:=0.39814049005508423, Train Acc:=92.99, Evaluation Acc:=87.42999999999999\n",
            "Epoch=6 - Train Loss:=415.7611083984375, Evaluation Loss:=0.42009237408638, Train Acc:=93.195, Evaluation Acc:=86.71\n",
            "Epoch=7 - Train Loss:=374.1799621582031, Evaluation Loss:=0.4496079981327057, Train Acc:=92.71833333333333, Evaluation Acc:=86.64\n",
            "Epoch=8 - Train Loss:=326.0844421386719, Evaluation Loss:=0.43450790643692017, Train Acc:=93.67, Evaluation Acc:=86.77\n",
            "Epoch=9 - Train Loss:=298.68328857421875, Evaluation Loss:=0.4338911175727844, Train Acc:=94.48833333333333, Evaluation Acc:=87.14\n",
            "Epoch=10 - Train Loss:=257.8303527832031, Evaluation Loss:=0.42202699184417725, Train Acc:=95.62166666666667, Evaluation Acc:=87.56\n",
            "Epoch=11 - Train Loss:=241.10324096679688, Evaluation Loss:=0.42398446798324585, Train Acc:=96.37333333333333, Evaluation Acc:=87.71\n",
            "Epoch=12 - Train Loss:=208.15130615234375, Evaluation Loss:=0.43257397413253784, Train Acc:=96.75333333333333, Evaluation Acc:=87.94\n",
            "Epoch=13 - Train Loss:=196.865234375, Evaluation Loss:=0.4557316303253174, Train Acc:=96.72500000000001, Evaluation Acc:=87.8\n",
            "Epoch=14 - Train Loss:=167.65524291992188, Evaluation Loss:=0.4328094720840454, Train Acc:=97.17, Evaluation Acc:=88.22\n",
            "Epoch=15 - Train Loss:=160.71026611328125, Evaluation Loss:=0.4709484279155731, Train Acc:=96.86833333333334, Evaluation Acc:=88.14\n",
            "Epoch=16 - Train Loss:=145.80661010742188, Evaluation Loss:=0.4721854627132416, Train Acc:=97.29333333333334, Evaluation Acc:=88.23\n",
            "Epoch=17 - Train Loss:=131.21157836914062, Evaluation Loss:=0.47412949800491333, Train Acc:=97.51833333333333, Evaluation Acc:=88.27000000000001\n",
            "Epoch=18 - Train Loss:=123.2499771118164, Evaluation Loss:=0.48016592860221863, Train Acc:=98.07833333333333, Evaluation Acc:=88.17\n",
            "Epoch=19 - Train Loss:=110.000244140625, Evaluation Loss:=0.5152864456176758, Train Acc:=97.37333333333333, Evaluation Acc:=88.6\n",
            "Epoch=0 - Train Loss:=1921.740966796875, Evaluation Loss:=0.6858258843421936, Train Acc:=84.42166666666667, Evaluation Acc:=81.2\n",
            "Epoch=1 - Train Loss:=945.1922607421875, Evaluation Loss:=0.5586074590682983, Train Acc:=87.59833333333333, Evaluation Acc:=84.41\n",
            "Epoch=2 - Train Loss:=721.11962890625, Evaluation Loss:=0.493252694606781, Train Acc:=88.895, Evaluation Acc:=84.77\n",
            "Epoch=3 - Train Loss:=604.5852661132812, Evaluation Loss:=0.47383642196655273, Train Acc:=89.67166666666667, Evaluation Acc:=85.06\n",
            "Epoch=4 - Train Loss:=519.0863647460938, Evaluation Loss:=0.42294809222221375, Train Acc:=91.825, Evaluation Acc:=86.6\n",
            "Epoch=5 - Train Loss:=458.3587341308594, Evaluation Loss:=0.41180527210235596, Train Acc:=92.23333333333333, Evaluation Acc:=86.77\n",
            "Epoch=6 - Train Loss:=406.9112243652344, Evaluation Loss:=0.4111470580101013, Train Acc:=93.00500000000001, Evaluation Acc:=86.7\n",
            "Epoch=7 - Train Loss:=358.0158996582031, Evaluation Loss:=0.398334801197052, Train Acc:=94.54666666666667, Evaluation Acc:=87.61\n",
            "Epoch=8 - Train Loss:=319.83453369140625, Evaluation Loss:=0.4231616258621216, Train Acc:=94.16333333333333, Evaluation Acc:=87.09\n",
            "Epoch=9 - Train Loss:=288.962890625, Evaluation Loss:=0.43198150396347046, Train Acc:=94.865, Evaluation Acc:=87.01\n",
            "Epoch=10 - Train Loss:=259.5915222167969, Evaluation Loss:=0.4183516502380371, Train Acc:=95.84666666666666, Evaluation Acc:=87.5\n",
            "Epoch=11 - Train Loss:=228.10357666015625, Evaluation Loss:=0.450443834066391, Train Acc:=95.09, Evaluation Acc:=86.89\n",
            "Epoch=12 - Train Loss:=211.70326232910156, Evaluation Loss:=0.4177553653717041, Train Acc:=96.905, Evaluation Acc:=88.59\n",
            "Epoch=13 - Train Loss:=189.19064331054688, Evaluation Loss:=0.4293443262577057, Train Acc:=97.03500000000001, Evaluation Acc:=88.3\n",
            "Epoch=14 - Train Loss:=174.02191162109375, Evaluation Loss:=0.4224605858325958, Train Acc:=97.995, Evaluation Acc:=88.73\n",
            "Epoch=15 - Train Loss:=156.1099853515625, Evaluation Loss:=0.4481566548347473, Train Acc:=97.69, Evaluation Acc:=88.35\n",
            "Epoch=16 - Train Loss:=146.22332763671875, Evaluation Loss:=0.4689769446849823, Train Acc:=97.30833333333334, Evaluation Acc:=88.03\n",
            "Epoch=17 - Train Loss:=130.1145477294922, Evaluation Loss:=0.43792998790740967, Train Acc:=98.48166666666667, Evaluation Acc:=88.75\n",
            "Epoch=18 - Train Loss:=124.35545349121094, Evaluation Loss:=0.4798574149608612, Train Acc:=98.39833333333333, Evaluation Acc:=88.64999999999999\n",
            "Epoch=19 - Train Loss:=114.38066864013672, Evaluation Loss:=0.48255282640457153, Train Acc:=98.4, Evaluation Acc:=88.16000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "UUYIX6F4nqBl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "b1fb7fb4-310a-4a5b-85d0-311130d7a2c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 88.478, and the variance is 0.04247600000000018\n",
            "the average running time for one epoch is: 48.32909945487976\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7f8dab4801d0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f8dab487250>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dab487550>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7f8dab487fd0>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f8dab487ad0>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7f8dab480790>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dab480cd0>]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model12: with dropout and L2 regularization + custom optimizer"
      ],
      "metadata": {
        "id": "FbRJIC5r9Pym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "input_size = image_size * image_size\n",
        "output_size = 10\n",
        "\n",
        "optimizer = \"custom\"\n",
        "hidden_size = 1024\n",
        "batch_size = 32\n",
        "learning_rate = 10e-5\n",
        "dropout = 0.1\n",
        "beta = 0.001\n",
        "epochs = 20\n",
        "\n",
        "test_accs = []\n",
        "epoch_times = []\n",
        "\n",
        "for i in range(10):\n",
        "    tf.random.set_seed(i+21)\n",
        "    x_train, y_train, x_test, y_test = load_data(dataset=\"fashion_mnist\")\n",
        "\n",
        "    model12 = MLP(input_size, output_size, hidden_size = hidden_size, optimizer = optimizer)\n",
        "    train_losses, test_losses, train_accuracy, test_accuracy, _epoch_times = train(model12, x_train, y_train, x_test, y_test,\n",
        "                                                                 batch_size=batch_size,\n",
        "                                                                 learning_rate=learning_rate,\n",
        "                                                                 epochs=epochs,\n",
        "                                                                 dropout_rate=dropout,\n",
        "                                                                 beta=beta)\n",
        "    acc = test(model12, x_test, y_test)\n",
        "    test_accs.append(acc)\n",
        "    epoch_times.append(np.mean(_epoch_times))"
      ],
      "metadata": {
        "id": "4MU9q2S09WCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab73d12-0705-4460-dbd9-2c6e130e6b18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 - Train Loss:=25847.658203125, Evaluation Loss:=0.701223611831665, Train Acc:=84.32666666666667, Evaluation Acc:=82.08\n",
            "Epoch=1 - Train Loss:=20097.58984375, Evaluation Loss:=0.5288601517677307, Train Acc:=86.07666666666667, Evaluation Acc:=83.46000000000001\n",
            "Epoch=2 - Train Loss:=17040.9296875, Evaluation Loss:=0.42841699719429016, Train Acc:=88.63333333333333, Evaluation Acc:=85.63\n",
            "Epoch=3 - Train Loss:=14296.21484375, Evaluation Loss:=0.426090806722641, Train Acc:=88.03333333333333, Evaluation Acc:=85.0\n",
            "Epoch=4 - Train Loss:=11882.58984375, Evaluation Loss:=0.3875444829463959, Train Acc:=89.51666666666667, Evaluation Acc:=86.44\n",
            "Epoch=5 - Train Loss:=9881.3955078125, Evaluation Loss:=0.39705365896224976, Train Acc:=89.46499999999999, Evaluation Acc:=85.65\n",
            "Epoch=6 - Train Loss:=8272.27734375, Evaluation Loss:=0.3630055785179138, Train Acc:=91.12, Evaluation Acc:=87.28\n",
            "Epoch=7 - Train Loss:=6995.63525390625, Evaluation Loss:=0.3434748649597168, Train Acc:=91.81833333333333, Evaluation Acc:=87.53\n",
            "Epoch=8 - Train Loss:=5979.76025390625, Evaluation Loss:=0.3670901358127594, Train Acc:=90.93166666666667, Evaluation Acc:=87.03999999999999\n",
            "Epoch=9 - Train Loss:=5150.56640625, Evaluation Loss:=0.34317880868911743, Train Acc:=91.61166666666666, Evaluation Acc:=87.7\n",
            "Epoch=10 - Train Loss:=4484.63427734375, Evaluation Loss:=0.36469346284866333, Train Acc:=90.76833333333333, Evaluation Acc:=86.74\n",
            "Epoch=11 - Train Loss:=3931.903076171875, Evaluation Loss:=0.3255673944950104, Train Acc:=92.28166666666667, Evaluation Acc:=88.31\n",
            "Epoch=12 - Train Loss:=3474.346435546875, Evaluation Loss:=0.33710208535194397, Train Acc:=91.92666666666666, Evaluation Acc:=87.92\n",
            "Epoch=13 - Train Loss:=3100.121337890625, Evaluation Loss:=0.34159862995147705, Train Acc:=91.56, Evaluation Acc:=87.75\n",
            "Epoch=14 - Train Loss:=2783.5693359375, Evaluation Loss:=0.3228660225868225, Train Acc:=92.52333333333334, Evaluation Acc:=88.59\n",
            "Epoch=15 - Train Loss:=2518.072998046875, Evaluation Loss:=0.3520370125770569, Train Acc:=91.39666666666668, Evaluation Acc:=87.83\n",
            "Epoch=16 - Train Loss:=2291.359130859375, Evaluation Loss:=0.334703654050827, Train Acc:=91.98166666666665, Evaluation Acc:=88.28\n",
            "Epoch=17 - Train Loss:=2100.530029296875, Evaluation Loss:=0.3179219365119934, Train Acc:=92.735, Evaluation Acc:=88.57000000000001\n",
            "Epoch=18 - Train Loss:=1933.7939453125, Evaluation Loss:=0.31921297311782837, Train Acc:=92.565, Evaluation Acc:=88.31\n",
            "Epoch=19 - Train Loss:=1796.322021484375, Evaluation Loss:=0.3296476900577545, Train Acc:=91.92833333333333, Evaluation Acc:=87.94999999999999\n",
            "Epoch=0 - Train Loss:=25986.291015625, Evaluation Loss:=0.7681939601898193, Train Acc:=83.30666666666666, Evaluation Acc:=81.37\n",
            "Epoch=1 - Train Loss:=20228.951171875, Evaluation Loss:=0.49793896079063416, Train Acc:=86.86166666666666, Evaluation Acc:=84.24000000000001\n",
            "Epoch=2 - Train Loss:=17049.83984375, Evaluation Loss:=0.430194616317749, Train Acc:=88.495, Evaluation Acc:=85.42\n",
            "Epoch=3 - Train Loss:=14256.490234375, Evaluation Loss:=0.38554880023002625, Train Acc:=89.875, Evaluation Acc:=86.61999999999999\n",
            "Epoch=4 - Train Loss:=11823.93359375, Evaluation Loss:=0.36945292353630066, Train Acc:=90.45166666666667, Evaluation Acc:=87.26\n",
            "Epoch=5 - Train Loss:=9819.791015625, Evaluation Loss:=0.3569014072418213, Train Acc:=90.59833333333333, Evaluation Acc:=86.9\n",
            "Epoch=6 - Train Loss:=8214.9853515625, Evaluation Loss:=0.360774427652359, Train Acc:=91.10666666666667, Evaluation Acc:=87.13\n",
            "Epoch=7 - Train Loss:=6954.2919921875, Evaluation Loss:=0.33643805980682373, Train Acc:=91.94166666666666, Evaluation Acc:=87.78\n",
            "Epoch=8 - Train Loss:=5945.66064453125, Evaluation Loss:=0.34431323409080505, Train Acc:=91.67666666666666, Evaluation Acc:=87.64999999999999\n",
            "Epoch=9 - Train Loss:=5124.94287109375, Evaluation Loss:=0.34434354305267334, Train Acc:=91.32000000000001, Evaluation Acc:=87.66000000000001\n",
            "Epoch=10 - Train Loss:=4457.66845703125, Evaluation Loss:=0.33136507868766785, Train Acc:=92.025, Evaluation Acc:=88.05\n",
            "Epoch=11 - Train Loss:=3913.31494140625, Evaluation Loss:=0.33473584055900574, Train Acc:=91.82333333333334, Evaluation Acc:=88.17\n",
            "Epoch=12 - Train Loss:=3457.384765625, Evaluation Loss:=0.33551669120788574, Train Acc:=92.05499999999999, Evaluation Acc:=87.92999999999999\n",
            "Epoch=13 - Train Loss:=3071.556884765625, Evaluation Loss:=0.3338962495326996, Train Acc:=91.81666666666666, Evaluation Acc:=88.08\n",
            "Epoch=14 - Train Loss:=2753.333984375, Evaluation Loss:=0.33603665232658386, Train Acc:=91.16166666666666, Evaluation Acc:=87.9\n",
            "Epoch=15 - Train Loss:=2485.869384765625, Evaluation Loss:=0.33327606320381165, Train Acc:=91.99666666666667, Evaluation Acc:=88.23\n",
            "Epoch=16 - Train Loss:=2260.472900390625, Evaluation Loss:=0.3198871612548828, Train Acc:=92.21833333333333, Evaluation Acc:=88.55\n",
            "Epoch=17 - Train Loss:=2070.59619140625, Evaluation Loss:=0.31603994965553284, Train Acc:=92.405, Evaluation Acc:=88.53999999999999\n",
            "Epoch=18 - Train Loss:=1903.5277099609375, Evaluation Loss:=0.3369380533695221, Train Acc:=91.38, Evaluation Acc:=87.78\n",
            "Epoch=19 - Train Loss:=1767.66943359375, Evaluation Loss:=0.3201999366283417, Train Acc:=92.34666666666666, Evaluation Acc:=88.67\n",
            "Epoch=0 - Train Loss:=25668.291015625, Evaluation Loss:=0.7796086072921753, Train Acc:=83.12, Evaluation Acc:=81.6\n",
            "Epoch=1 - Train Loss:=19982.3828125, Evaluation Loss:=0.5432098507881165, Train Acc:=85.71166666666666, Evaluation Acc:=83.19\n",
            "Epoch=2 - Train Loss:=16753.337890625, Evaluation Loss:=0.4235076308250427, Train Acc:=88.89666666666668, Evaluation Acc:=85.68\n",
            "Epoch=3 - Train Loss:=13916.8505859375, Evaluation Loss:=0.42469629645347595, Train Acc:=88.445, Evaluation Acc:=84.99\n",
            "Epoch=4 - Train Loss:=11458.0947265625, Evaluation Loss:=0.3949442505836487, Train Acc:=89.32666666666667, Evaluation Acc:=86.46000000000001\n",
            "Epoch=5 - Train Loss:=9458.771484375, Evaluation Loss:=0.36410367488861084, Train Acc:=90.86833333333333, Evaluation Acc:=87.1\n",
            "Epoch=6 - Train Loss:=7894.60546875, Evaluation Loss:=0.35635173320770264, Train Acc:=91.40666666666667, Evaluation Acc:=87.45\n",
            "Epoch=7 - Train Loss:=6672.34814453125, Evaluation Loss:=0.3513164818286896, Train Acc:=91.37333333333333, Evaluation Acc:=87.57000000000001\n",
            "Epoch=8 - Train Loss:=5704.4326171875, Evaluation Loss:=0.34932002425193787, Train Acc:=91.35, Evaluation Acc:=87.92999999999999\n",
            "Epoch=9 - Train Loss:=4924.8115234375, Evaluation Loss:=0.3528464436531067, Train Acc:=91.36333333333333, Evaluation Acc:=87.01\n",
            "Epoch=10 - Train Loss:=4288.61279296875, Evaluation Loss:=0.33803611993789673, Train Acc:=91.97166666666666, Evaluation Acc:=87.68\n",
            "Epoch=11 - Train Loss:=3766.71826171875, Evaluation Loss:=0.33960360288619995, Train Acc:=91.915, Evaluation Acc:=87.42\n",
            "Epoch=12 - Train Loss:=3341.226806640625, Evaluation Loss:=0.33074048161506653, Train Acc:=92.29, Evaluation Acc:=88.2\n",
            "Epoch=13 - Train Loss:=2980.455810546875, Evaluation Loss:=0.33690181374549866, Train Acc:=91.88666666666667, Evaluation Acc:=87.83999999999999\n",
            "Epoch=14 - Train Loss:=2684.296875, Evaluation Loss:=0.3352859318256378, Train Acc:=91.62666666666667, Evaluation Acc:=87.74\n",
            "Epoch=15 - Train Loss:=2429.334228515625, Evaluation Loss:=0.3118729591369629, Train Acc:=92.92333333333333, Evaluation Acc:=88.64999999999999\n",
            "Epoch=16 - Train Loss:=2219.33154296875, Evaluation Loss:=0.3522755801677704, Train Acc:=91.05833333333334, Evaluation Acc:=87.0\n",
            "Epoch=17 - Train Loss:=2035.81982421875, Evaluation Loss:=0.32793155312538147, Train Acc:=91.79666666666667, Evaluation Acc:=87.74\n",
            "Epoch=18 - Train Loss:=1877.1787109375, Evaluation Loss:=0.3371210992336273, Train Acc:=92.05666666666666, Evaluation Acc:=88.17\n",
            "Epoch=19 - Train Loss:=1746.34521484375, Evaluation Loss:=0.3225559592247009, Train Acc:=92.38333333333333, Evaluation Acc:=88.49000000000001\n",
            "Epoch=0 - Train Loss:=25583.27734375, Evaluation Loss:=0.6461607813835144, Train Acc:=84.76833333333333, Evaluation Acc:=82.91\n",
            "Epoch=1 - Train Loss:=19922.83203125, Evaluation Loss:=0.4514775276184082, Train Acc:=88.38333333333334, Evaluation Acc:=85.53\n",
            "Epoch=2 - Train Loss:=16694.044921875, Evaluation Loss:=0.4290623962879181, Train Acc:=88.47833333333334, Evaluation Acc:=85.39\n",
            "Epoch=3 - Train Loss:=13850.8876953125, Evaluation Loss:=0.40102851390838623, Train Acc:=89.005, Evaluation Acc:=85.92\n",
            "Epoch=4 - Train Loss:=11390.25390625, Evaluation Loss:=0.3916412591934204, Train Acc:=89.66333333333333, Evaluation Acc:=85.76\n",
            "Epoch=5 - Train Loss:=9390.3486328125, Evaluation Loss:=0.379891961812973, Train Acc:=90.39166666666667, Evaluation Acc:=86.00999999999999\n",
            "Epoch=6 - Train Loss:=7829.87451171875, Evaluation Loss:=0.33392080664634705, Train Acc:=92.025, Evaluation Acc:=87.86\n",
            "Epoch=7 - Train Loss:=6621.26123046875, Evaluation Loss:=0.35019218921661377, Train Acc:=91.46833333333333, Evaluation Acc:=87.14\n",
            "Epoch=8 - Train Loss:=5676.12353515625, Evaluation Loss:=0.3286701738834381, Train Acc:=92.26333333333334, Evaluation Acc:=88.03\n",
            "Epoch=9 - Train Loss:=4912.1640625, Evaluation Loss:=0.32782259583473206, Train Acc:=92.52333333333334, Evaluation Acc:=88.03999999999999\n",
            "Epoch=10 - Train Loss:=4288.8857421875, Evaluation Loss:=0.3435445725917816, Train Acc:=91.63666666666667, Evaluation Acc:=87.64999999999999\n",
            "Epoch=11 - Train Loss:=3776.89013671875, Evaluation Loss:=0.3399273455142975, Train Acc:=91.965, Evaluation Acc:=87.97\n",
            "Epoch=12 - Train Loss:=3349.643798828125, Evaluation Loss:=0.3443661630153656, Train Acc:=91.495, Evaluation Acc:=87.8\n",
            "Epoch=13 - Train Loss:=2993.771484375, Evaluation Loss:=0.3238041400909424, Train Acc:=92.60666666666667, Evaluation Acc:=88.49000000000001\n",
            "Epoch=14 - Train Loss:=2699.269287109375, Evaluation Loss:=0.31779390573501587, Train Acc:=92.66333333333333, Evaluation Acc:=88.47\n",
            "Epoch=15 - Train Loss:=2441.44970703125, Evaluation Loss:=0.32828521728515625, Train Acc:=92.135, Evaluation Acc:=88.21\n",
            "Epoch=16 - Train Loss:=2232.10595703125, Evaluation Loss:=0.33774349093437195, Train Acc:=91.73, Evaluation Acc:=87.99\n",
            "Epoch=17 - Train Loss:=2049.348876953125, Evaluation Loss:=0.32382476329803467, Train Acc:=92.21333333333334, Evaluation Acc:=88.06\n",
            "Epoch=18 - Train Loss:=1887.01806640625, Evaluation Loss:=0.3330533802509308, Train Acc:=91.82000000000001, Evaluation Acc:=87.72999999999999\n",
            "Epoch=19 - Train Loss:=1758.1248779296875, Evaluation Loss:=0.3309417963027954, Train Acc:=91.915, Evaluation Acc:=88.26\n",
            "Epoch=0 - Train Loss:=25629.4296875, Evaluation Loss:=0.6870026588439941, Train Acc:=85.05833333333334, Evaluation Acc:=83.16\n",
            "Epoch=1 - Train Loss:=20016.38671875, Evaluation Loss:=0.5006885528564453, Train Acc:=87.34666666666666, Evaluation Acc:=84.58\n",
            "Epoch=2 - Train Loss:=16836.404296875, Evaluation Loss:=0.449739545583725, Train Acc:=88.275, Evaluation Acc:=85.41\n",
            "Epoch=3 - Train Loss:=14091.681640625, Evaluation Loss:=0.40367329120635986, Train Acc:=89.34666666666666, Evaluation Acc:=85.84\n",
            "Epoch=4 - Train Loss:=11698.4794921875, Evaluation Loss:=0.3654078245162964, Train Acc:=90.64333333333333, Evaluation Acc:=86.98\n",
            "Epoch=5 - Train Loss:=9716.173828125, Evaluation Loss:=0.3794128894805908, Train Acc:=90.73, Evaluation Acc:=87.35000000000001\n",
            "Epoch=6 - Train Loss:=8148.34326171875, Evaluation Loss:=0.3681897222995758, Train Acc:=90.77666666666667, Evaluation Acc:=87.0\n",
            "Epoch=7 - Train Loss:=6910.47998046875, Evaluation Loss:=0.3273167908191681, Train Acc:=92.13, Evaluation Acc:=87.97\n",
            "Epoch=8 - Train Loss:=5919.09375, Evaluation Loss:=0.34502923488616943, Train Acc:=91.59166666666667, Evaluation Acc:=87.91\n",
            "Epoch=9 - Train Loss:=5115.6015625, Evaluation Loss:=0.3281702995300293, Train Acc:=92.35, Evaluation Acc:=88.47\n",
            "Epoch=10 - Train Loss:=4455.74853515625, Evaluation Loss:=0.3514440059661865, Train Acc:=91.42, Evaluation Acc:=87.38\n",
            "Epoch=11 - Train Loss:=3907.713623046875, Evaluation Loss:=0.3223041892051697, Train Acc:=92.38, Evaluation Acc:=88.41\n",
            "Epoch=12 - Train Loss:=3462.208251953125, Evaluation Loss:=0.3192042112350464, Train Acc:=92.70833333333334, Evaluation Acc:=88.39\n",
            "Epoch=13 - Train Loss:=3087.61279296875, Evaluation Loss:=0.3377138078212738, Train Acc:=91.89666666666668, Evaluation Acc:=87.72\n",
            "Epoch=14 - Train Loss:=2769.093994140625, Evaluation Loss:=0.33151519298553467, Train Acc:=92.09166666666667, Evaluation Acc:=88.14\n",
            "Epoch=15 - Train Loss:=2513.87939453125, Evaluation Loss:=0.3124290406703949, Train Acc:=92.97, Evaluation Acc:=88.87\n",
            "Epoch=16 - Train Loss:=2289.559326171875, Evaluation Loss:=0.3440496623516083, Train Acc:=91.43, Evaluation Acc:=87.27000000000001\n",
            "Epoch=17 - Train Loss:=2099.6865234375, Evaluation Loss:=0.336198627948761, Train Acc:=91.54666666666667, Evaluation Acc:=87.64\n",
            "Epoch=18 - Train Loss:=1936.881103515625, Evaluation Loss:=0.33258432149887085, Train Acc:=91.93333333333334, Evaluation Acc:=87.98\n",
            "Epoch=19 - Train Loss:=1798.2373046875, Evaluation Loss:=0.3282645046710968, Train Acc:=92.34833333333333, Evaluation Acc:=88.59\n",
            "Epoch=0 - Train Loss:=25593.51171875, Evaluation Loss:=0.6061902642250061, Train Acc:=85.095, Evaluation Acc:=83.19\n",
            "Epoch=1 - Train Loss:=19986.08984375, Evaluation Loss:=0.5619950294494629, Train Acc:=85.13333333333334, Evaluation Acc:=82.94\n",
            "Epoch=2 - Train Loss:=16749.05078125, Evaluation Loss:=0.4694923460483551, Train Acc:=87.02, Evaluation Acc:=84.15\n",
            "Epoch=3 - Train Loss:=13927.3427734375, Evaluation Loss:=0.39848119020462036, Train Acc:=89.01166666666667, Evaluation Acc:=86.11\n",
            "Epoch=4 - Train Loss:=11480.2822265625, Evaluation Loss:=0.380691796541214, Train Acc:=89.755, Evaluation Acc:=86.59\n",
            "Epoch=5 - Train Loss:=9489.259765625, Evaluation Loss:=0.3865159749984741, Train Acc:=89.84666666666666, Evaluation Acc:=86.65\n",
            "Epoch=6 - Train Loss:=7916.61083984375, Evaluation Loss:=0.3559573292732239, Train Acc:=90.97666666666666, Evaluation Acc:=87.12\n",
            "Epoch=7 - Train Loss:=6685.34375, Evaluation Loss:=0.3533923327922821, Train Acc:=91.50666666666667, Evaluation Acc:=87.53\n",
            "Epoch=8 - Train Loss:=5710.56396484375, Evaluation Loss:=0.33799538016319275, Train Acc:=92.02666666666667, Evaluation Acc:=88.12\n",
            "Epoch=9 - Train Loss:=4932.21826171875, Evaluation Loss:=0.34325194358825684, Train Acc:=91.825, Evaluation Acc:=87.42999999999999\n",
            "Epoch=10 - Train Loss:=4294.037109375, Evaluation Loss:=0.33651942014694214, Train Acc:=91.96166666666666, Evaluation Acc:=87.55\n",
            "Epoch=11 - Train Loss:=3766.688232421875, Evaluation Loss:=0.32511600852012634, Train Acc:=92.40833333333333, Evaluation Acc:=88.17\n",
            "Epoch=12 - Train Loss:=3331.656005859375, Evaluation Loss:=0.3233508765697479, Train Acc:=92.34833333333333, Evaluation Acc:=88.31\n",
            "Epoch=13 - Train Loss:=2968.810302734375, Evaluation Loss:=0.3471490442752838, Train Acc:=91.41666666666667, Evaluation Acc:=87.72999999999999\n",
            "Epoch=14 - Train Loss:=2674.6484375, Evaluation Loss:=0.3426594138145447, Train Acc:=91.63, Evaluation Acc:=87.81\n",
            "Epoch=15 - Train Loss:=2417.198486328125, Evaluation Loss:=0.3373980224132538, Train Acc:=91.83833333333334, Evaluation Acc:=87.79\n",
            "Epoch=16 - Train Loss:=2203.18212890625, Evaluation Loss:=0.33477354049682617, Train Acc:=91.77833333333332, Evaluation Acc:=87.57000000000001\n",
            "Epoch=17 - Train Loss:=2021.5567626953125, Evaluation Loss:=0.322541743516922, Train Acc:=92.16666666666666, Evaluation Acc:=88.16000000000001\n",
            "Epoch=18 - Train Loss:=1867.82958984375, Evaluation Loss:=0.3111594617366791, Train Acc:=92.82666666666667, Evaluation Acc:=88.75999999999999\n",
            "Epoch=19 - Train Loss:=1731.27099609375, Evaluation Loss:=0.32555505633354187, Train Acc:=92.2, Evaluation Acc:=88.39\n",
            "Epoch=0 - Train Loss:=25484.3046875, Evaluation Loss:=0.6842358708381653, Train Acc:=84.14333333333333, Evaluation Acc:=81.56\n",
            "Epoch=1 - Train Loss:=19532.876953125, Evaluation Loss:=0.4969259798526764, Train Acc:=87.115, Evaluation Acc:=84.44\n",
            "Epoch=2 - Train Loss:=16384.681640625, Evaluation Loss:=0.4002809524536133, Train Acc:=89.305, Evaluation Acc:=86.39\n",
            "Epoch=3 - Train Loss:=13688.40234375, Evaluation Loss:=0.4173133373260498, Train Acc:=88.89666666666668, Evaluation Acc:=86.24000000000001\n",
            "Epoch=4 - Train Loss:=11367.6220703125, Evaluation Loss:=0.36454272270202637, Train Acc:=90.52666666666667, Evaluation Acc:=86.98\n",
            "Epoch=5 - Train Loss:=9451.1962890625, Evaluation Loss:=0.3663276731967926, Train Acc:=90.685, Evaluation Acc:=86.83999999999999\n",
            "Epoch=6 - Train Loss:=7945.404296875, Evaluation Loss:=0.35633596777915955, Train Acc:=90.91833333333334, Evaluation Acc:=87.19\n",
            "Epoch=7 - Train Loss:=6753.912109375, Evaluation Loss:=0.34048160910606384, Train Acc:=91.25333333333333, Evaluation Acc:=87.37\n",
            "Epoch=8 - Train Loss:=5798.8447265625, Evaluation Loss:=0.3564046025276184, Train Acc:=90.97, Evaluation Acc:=87.16000000000001\n",
            "Epoch=9 - Train Loss:=5021.63134765625, Evaluation Loss:=0.354336142539978, Train Acc:=90.94, Evaluation Acc:=87.31\n",
            "Epoch=10 - Train Loss:=4383.609375, Evaluation Loss:=0.3377309739589691, Train Acc:=91.89333333333335, Evaluation Acc:=87.72\n",
            "Epoch=11 - Train Loss:=3859.8994140625, Evaluation Loss:=0.321683794260025, Train Acc:=92.56166666666667, Evaluation Acc:=88.44\n",
            "Epoch=12 - Train Loss:=3426.221923828125, Evaluation Loss:=0.33403027057647705, Train Acc:=91.905, Evaluation Acc:=88.03999999999999\n",
            "Epoch=13 - Train Loss:=3061.96533203125, Evaluation Loss:=0.34561166167259216, Train Acc:=91.23166666666667, Evaluation Acc:=87.48\n",
            "Epoch=14 - Train Loss:=2748.856201171875, Evaluation Loss:=0.3184162974357605, Train Acc:=92.61333333333333, Evaluation Acc:=88.29\n",
            "Epoch=15 - Train Loss:=2493.095947265625, Evaluation Loss:=0.3115794062614441, Train Acc:=92.805, Evaluation Acc:=88.64999999999999\n",
            "Epoch=16 - Train Loss:=2273.2802734375, Evaluation Loss:=0.32143867015838623, Train Acc:=92.18499999999999, Evaluation Acc:=88.49000000000001\n",
            "Epoch=17 - Train Loss:=2085.0859375, Evaluation Loss:=0.3142809569835663, Train Acc:=92.51666666666667, Evaluation Acc:=88.92\n",
            "Epoch=18 - Train Loss:=1922.8648681640625, Evaluation Loss:=0.34796327352523804, Train Acc:=91.31833333333333, Evaluation Acc:=87.71\n",
            "Epoch=19 - Train Loss:=1784.6514892578125, Evaluation Loss:=0.3239142596721649, Train Acc:=92.345, Evaluation Acc:=88.3\n",
            "Epoch=0 - Train Loss:=25691.041015625, Evaluation Loss:=0.7133193016052246, Train Acc:=84.80499999999999, Evaluation Acc:=82.95\n",
            "Epoch=1 - Train Loss:=20073.671875, Evaluation Loss:=0.5546838641166687, Train Acc:=85.61, Evaluation Acc:=83.02000000000001\n",
            "Epoch=2 - Train Loss:=16953.609375, Evaluation Loss:=0.448333203792572, Train Acc:=88.675, Evaluation Acc:=85.16\n",
            "Epoch=3 - Train Loss:=14204.669921875, Evaluation Loss:=0.4197736382484436, Train Acc:=89.185, Evaluation Acc:=85.52\n",
            "Epoch=4 - Train Loss:=11792.5771484375, Evaluation Loss:=0.4158424735069275, Train Acc:=88.82666666666667, Evaluation Acc:=85.34\n",
            "Epoch=5 - Train Loss:=9783.0224609375, Evaluation Loss:=0.36721551418304443, Train Acc:=90.87666666666667, Evaluation Acc:=86.83\n",
            "Epoch=6 - Train Loss:=8176.0166015625, Evaluation Loss:=0.38502779603004456, Train Acc:=90.255, Evaluation Acc:=86.42999999999999\n",
            "Epoch=7 - Train Loss:=6914.54345703125, Evaluation Loss:=0.356594979763031, Train Acc:=91.18666666666667, Evaluation Acc:=87.06\n",
            "Epoch=8 - Train Loss:=5922.76123046875, Evaluation Loss:=0.34509891271591187, Train Acc:=91.70833333333334, Evaluation Acc:=87.67\n",
            "Epoch=9 - Train Loss:=5122.39453125, Evaluation Loss:=0.34084826707839966, Train Acc:=92.05833333333334, Evaluation Acc:=87.57000000000001\n",
            "Epoch=10 - Train Loss:=4462.939453125, Evaluation Loss:=0.33989834785461426, Train Acc:=92.11500000000001, Evaluation Acc:=87.71\n",
            "Epoch=11 - Train Loss:=3918.234375, Evaluation Loss:=0.331205815076828, Train Acc:=92.50166666666667, Evaluation Acc:=88.4\n",
            "Epoch=12 - Train Loss:=3474.01611328125, Evaluation Loss:=0.33277976512908936, Train Acc:=92.265, Evaluation Acc:=88.17\n",
            "Epoch=13 - Train Loss:=3094.02880859375, Evaluation Loss:=0.33218085765838623, Train Acc:=92.255, Evaluation Acc:=88.18\n",
            "Epoch=14 - Train Loss:=2777.381103515625, Evaluation Loss:=0.3228526711463928, Train Acc:=92.54333333333334, Evaluation Acc:=88.53\n",
            "Epoch=15 - Train Loss:=2508.674072265625, Evaluation Loss:=0.324020117521286, Train Acc:=92.29666666666667, Evaluation Acc:=88.24\n",
            "Epoch=16 - Train Loss:=2278.723388671875, Evaluation Loss:=0.3499663174152374, Train Acc:=91.38, Evaluation Acc:=87.13\n",
            "Epoch=17 - Train Loss:=2089.88525390625, Evaluation Loss:=0.3377225995063782, Train Acc:=91.91333333333334, Evaluation Acc:=88.02\n",
            "Epoch=18 - Train Loss:=1924.95263671875, Evaluation Loss:=0.3300233781337738, Train Acc:=92.31666666666666, Evaluation Acc:=88.25\n",
            "Epoch=19 - Train Loss:=1782.8126220703125, Evaluation Loss:=0.3365813195705414, Train Acc:=91.75833333333333, Evaluation Acc:=87.94999999999999\n",
            "Epoch=0 - Train Loss:=26145.115234375, Evaluation Loss:=0.7741080522537231, Train Acc:=82.69833333333332, Evaluation Acc:=80.27\n",
            "Epoch=1 - Train Loss:=20467.189453125, Evaluation Loss:=0.5110634565353394, Train Acc:=87.285, Evaluation Acc:=84.96000000000001\n",
            "Epoch=2 - Train Loss:=17324.927734375, Evaluation Loss:=0.4545343816280365, Train Acc:=88.07000000000001, Evaluation Acc:=85.36\n",
            "Epoch=3 - Train Loss:=14519.0751953125, Evaluation Loss:=0.3984525799751282, Train Acc:=89.59333333333333, Evaluation Acc:=86.36\n",
            "Epoch=4 - Train Loss:=12043.224609375, Evaluation Loss:=0.3658452033996582, Train Acc:=90.53166666666667, Evaluation Acc:=86.83999999999999\n",
            "Epoch=5 - Train Loss:=9980.46484375, Evaluation Loss:=0.36313194036483765, Train Acc:=90.47166666666666, Evaluation Acc:=86.83\n",
            "Epoch=6 - Train Loss:=8349.8310546875, Evaluation Loss:=0.349916934967041, Train Acc:=91.31666666666666, Evaluation Acc:=87.21\n",
            "Epoch=7 - Train Loss:=7062.212890625, Evaluation Loss:=0.3418281376361847, Train Acc:=91.47333333333333, Evaluation Acc:=87.58\n",
            "Epoch=8 - Train Loss:=6030.69580078125, Evaluation Loss:=0.35386839509010315, Train Acc:=90.68666666666667, Evaluation Acc:=86.91\n",
            "Epoch=9 - Train Loss:=5195.501953125, Evaluation Loss:=0.34736424684524536, Train Acc:=91.35833333333333, Evaluation Acc:=86.99\n",
            "Epoch=10 - Train Loss:=4514.53125, Evaluation Loss:=0.3260655701160431, Train Acc:=92.17666666666666, Evaluation Acc:=88.07000000000001\n",
            "Epoch=11 - Train Loss:=3953.70068359375, Evaluation Loss:=0.3363349735736847, Train Acc:=91.80833333333334, Evaluation Acc:=87.91\n",
            "Epoch=12 - Train Loss:=3491.999755859375, Evaluation Loss:=0.32521581649780273, Train Acc:=92.18499999999999, Evaluation Acc:=88.14999999999999\n",
            "Epoch=13 - Train Loss:=3108.857177734375, Evaluation Loss:=0.3437730371952057, Train Acc:=91.40166666666667, Evaluation Acc:=87.27000000000001\n",
            "Epoch=14 - Train Loss:=2792.0361328125, Evaluation Loss:=0.31946587562561035, Train Acc:=92.345, Evaluation Acc:=88.64\n",
            "Epoch=15 - Train Loss:=2520.90576171875, Evaluation Loss:=0.34185999631881714, Train Acc:=91.31833333333333, Evaluation Acc:=87.92\n",
            "Epoch=16 - Train Loss:=2290.122802734375, Evaluation Loss:=0.33915287256240845, Train Acc:=91.2, Evaluation Acc:=87.68\n",
            "Epoch=17 - Train Loss:=2096.593994140625, Evaluation Loss:=0.3179495632648468, Train Acc:=92.44833333333334, Evaluation Acc:=88.48\n",
            "Epoch=18 - Train Loss:=1932.4329833984375, Evaluation Loss:=0.3205243647098541, Train Acc:=92.38166666666666, Evaluation Acc:=88.31\n",
            "Epoch=19 - Train Loss:=1790.94384765625, Evaluation Loss:=0.32730016112327576, Train Acc:=92.07499999999999, Evaluation Acc:=88.21\n",
            "Epoch=0 - Train Loss:=26041.56640625, Evaluation Loss:=0.6638607978820801, Train Acc:=85.09833333333333, Evaluation Acc:=82.95\n",
            "Epoch=1 - Train Loss:=20329.396484375, Evaluation Loss:=0.472507119178772, Train Acc:=87.2, Evaluation Acc:=84.31\n",
            "Epoch=2 - Train Loss:=17217.791015625, Evaluation Loss:=0.4823671281337738, Train Acc:=86.93166666666666, Evaluation Acc:=83.77\n",
            "Epoch=3 - Train Loss:=14472.5205078125, Evaluation Loss:=0.38496124744415283, Train Acc:=89.705, Evaluation Acc:=86.65\n",
            "Epoch=4 - Train Loss:=12025.001953125, Evaluation Loss:=0.4089629352092743, Train Acc:=88.74333333333333, Evaluation Acc:=85.39999999999999\n",
            "Epoch=5 - Train Loss:=9998.6728515625, Evaluation Loss:=0.37302419543266296, Train Acc:=90.545, Evaluation Acc:=86.92999999999999\n",
            "Epoch=6 - Train Loss:=8380.845703125, Evaluation Loss:=0.3638485074043274, Train Acc:=90.685, Evaluation Acc:=87.42999999999999\n",
            "Epoch=7 - Train Loss:=7100.74951171875, Evaluation Loss:=0.35648277401924133, Train Acc:=91.065, Evaluation Acc:=87.2\n",
            "Epoch=8 - Train Loss:=6076.77392578125, Evaluation Loss:=0.3374224603176117, Train Acc:=91.82666666666667, Evaluation Acc:=87.55\n",
            "Epoch=9 - Train Loss:=5239.41064453125, Evaluation Loss:=0.34168368577957153, Train Acc:=91.84666666666666, Evaluation Acc:=87.69\n",
            "Epoch=10 - Train Loss:=4562.5673828125, Evaluation Loss:=0.3470853269100189, Train Acc:=91.51166666666667, Evaluation Acc:=87.64\n",
            "Epoch=11 - Train Loss:=4002.0625, Evaluation Loss:=0.32043778896331787, Train Acc:=92.32000000000001, Evaluation Acc:=88.68\n",
            "Epoch=12 - Train Loss:=3533.689453125, Evaluation Loss:=0.3426044285297394, Train Acc:=91.56333333333333, Evaluation Acc:=87.56\n",
            "Epoch=13 - Train Loss:=3144.800537109375, Evaluation Loss:=0.3234594166278839, Train Acc:=92.49166666666667, Evaluation Acc:=88.44999999999999\n",
            "Epoch=14 - Train Loss:=2820.8720703125, Evaluation Loss:=0.32212650775909424, Train Acc:=92.37666666666667, Evaluation Acc:=88.06\n",
            "Epoch=15 - Train Loss:=2546.72998046875, Evaluation Loss:=0.35777825117111206, Train Acc:=91.07499999999999, Evaluation Acc:=87.17\n",
            "Epoch=16 - Train Loss:=2318.45947265625, Evaluation Loss:=0.36565926671028137, Train Acc:=90.39166666666667, Evaluation Acc:=87.25\n",
            "Epoch=17 - Train Loss:=2122.499267578125, Evaluation Loss:=0.3151184618473053, Train Acc:=92.56166666666667, Evaluation Acc:=88.7\n",
            "Epoch=18 - Train Loss:=1955.087158203125, Evaluation Loss:=0.32952681183815, Train Acc:=92.09, Evaluation Acc:=88.25\n",
            "Epoch=19 - Train Loss:=1811.1529541015625, Evaluation Loss:=0.32934287190437317, Train Acc:=91.84166666666667, Evaluation Acc:=88.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(test_accs)\n",
        "var = np.var(test_accs)\n",
        "print(f\"the mean of 10 runs is {mean}, and the variance is {var}\")\n",
        "\n",
        "avg_epoch_time = np.mean(epoch_times)\n",
        "print(f\"the average running time for one epoch is: {avg_epoch_time}\")\n",
        "\n",
        "plt.boxplot(test_accs)"
      ],
      "metadata": {
        "id": "15UW1vpbnq4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "4b1651e2-2dc2-4ae1-aaf6-c49d11ab0b2c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the mean of 10 runs is 88.299, and the variance is 0.05342900000000218\n",
            "the average running time for one epoch is: 51.47434956431389\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7f8dab214350>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f8dab2193d0>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dab219910>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7f8dab220410>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f8dab219e90>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7f8dab214910>,\n",
              "  <matplotlib.lines.Line2D at 0x7f8dab214e50>]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NjK-BxnDRVOp",
        "o3Ge-GyQZG7E",
        "G4Qak0QzWbuv",
        "-3QX0gR-8hNS"
      ],
      "name": "IST597_Assignment3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}